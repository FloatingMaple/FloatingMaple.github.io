{"meta":{"title":"Maple's Blog","subtitle":"The future commences now","description":"A blog of Maple W","author":"Maple W","url":"http://yoursite.com","root":"/"},"pages":[{"title":"about","date":"2019-04-04T01:06:32.000Z","updated":"2019-04-04T01:06:33.000Z","comments":true,"path":"about/index-1.html","permalink":"http://yoursite.com/about/index-1.html","excerpt":"","text":""},{"title":"about","date":"2019-03-30T06:42:44.000Z","updated":"2019-03-30T06:42:44.258Z","comments":true,"path":"about/index.html","permalink":"http://yoursite.com/about/index.html","excerpt":"","text":""},{"title":"categories","date":"2019-04-04T01:06:11.000Z","updated":"2019-04-04T01:07:06.307Z","comments":true,"path":"categories/index.html","permalink":"http://yoursite.com/categories/index.html","excerpt":"","text":""},{"title":"tags","date":"2019-04-04T01:06:21.000Z","updated":"2019-04-04T01:07:22.146Z","comments":true,"path":"tags/index.html","permalink":"http://yoursite.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"Paper-TextTruth","slug":"Paper-TextTruth","date":"2019-04-15T13:41:31.000Z","updated":"2019-04-15T14:14:09.802Z","comments":true,"path":"2019/04/15/Paper-TextTruth/","link":"","permalink":"http://yoursite.com/2019/04/15/Paper-TextTruth/","excerpt":"","text":"TextTruth:An Unsupervised Approach to Discover Trustworthy Information from Multi-Sourced Text Data 一种无监督方法，从多源文本数据中发现可信信息 AbstractTruth discovery 真实信息发现/真值发现 含义： ​ 能够在不受任何监督的情况下从嘈杂的多源数据中提取可信的信息。 问题： ​ 现有方法多针对结构化数据； ​ 文本数据有其独特的特点，无法满足从原始文本数据中提取可信信息的强烈需求 主要挑战： 文本答案的多因素属性(答案可能包含多个关键要素) 单词用法的多样性（不同的单词可能有相同的语义） 本文方法： 将从特定问题的答案中提取的关键词组合成多个可解释的因素； 从而推断出答案因素和答案提供者的可信度 之后，每个问题的答案都可以根据因素的可信度进行排序 非监督方法，能够应用于涉及文本数据的各种应用场景 能够准确选择可信答案，即使答案涉及多因素 Introduction海量数据在线平台： ​ Amazon Mechanical Turk ； Stack Exchange ； Yahoo Answers 问题： ​ 此类多源数据通常由非专家在线用户提供，因此数据中可能存在错误甚至冲突 两个基本原则：（真值发现的） 如果用户提供了很多值得信赖的信息或真实答案，其可靠性就很高 如果一个答案得到了很多可靠用户的支持，该答案更可能是正确的 文本数据的两个特性： 答案可能是多要素的，给定的文本答案很难覆盖所有要素 单词用法的多样性，用户可能通过不同的关键词传达非常相似的意思 ​ 左图：用户、答案、问题之间的关系 ​ 中图：例证展示关键词、答案要素【特性2】 ​ 右图：用户的答案中包含的要素【特性1】 TextTruth： ​ 将每个答案中的关键词作为输入，输出其候选答案可信度的排名 步骤： 首先将文本答案中的关键词转换为经过训练的可计算向量表示（细粒度） 通过对语义相似的关键词进行聚类，对答案的多样性进行建模（从每个答案要素的角度分析可信性） 优点： 通过对每个答案要素的可信度进行评估，可以处理文本答案部分正确现象 通过将答案关键词建模为向量表示形式，使得答案中的要素可计算，从而解决文本数据中用词多样性问题 Related WorkTruth Discovery 概念 从相互冲突的多源数据中识别出可信信息 处理场景 不同数据类型 数据源依赖关系 细粒度数据源可靠性 实体/对象依赖关系 长尾数据 Community Question Answering 概念 CQA，社区问答系统 现有工作分类 从众包答案中提取特征，将答案质量评估任务转化为分类或排序问题 通常需要高质量训练集和各种有用的特性来训练模型，实际应用较为困难 转化为专家寻找问题（expert finding problem），基于答案提供者来推断答案的质量 需要外部信息， 例如询问者-回答者交互、投票信息等 Answer Selection 概念 答案选择，从一组候选句子中选择最合适的答案 不同方法 基于词汇特征 基于神经网络的模型表示一个句子在向量空间中的意义，并借此比问题和候选答案 引入注意力机制，增强句子表征学习 Problem Formulation术语： Question 问题 一个问题 q 包含 N~q~ 个单词，并且能够被用户回答 Answer 回答 用户 u 对于问题 q 给出的一个回答表示为 a~qu~ Answer Keyword 答案关键词 答案关键词是答案中的领域转由内容单词/短语 用户 u 对问题 q 给出的答案中的第m个答案关键词表示为 x~qum~ Answer Factor 答案要素 答案要素是答案的关键点，用答案关键词的簇表示 问题 q 的答案中第 k 个答案要素表示为 c~qk~ Problem Definition 问题定义 对每个问题，不同用户可能提供不同答案；答案可能由包含多个要素的复合句组成；答案可能部分正确 用户集 $ {u}_1^U$ 问题集 ${q}1^Q$ 答案集${ a{qu} }_{q,u=1,1}^{Q,U}$ U 用户数量 Q 问题数量 目的：提取出每个问题的高可信度答案和答案中的高可信度关键要素 MethodologyOverview 问题 在将真值发现方法应用于复杂自然语言问题寻找可信答案时，应该考虑到答案之间的语义相关性，从而准确估计用户的可信度。但是学习整个答案的准确表示向量很困难，特别是上下文语料库不丰富时 自然语言复杂性，答案的意义过于复杂，无法用一个向量表示 处理方式 使用更加细粒度的语义单元（semantic units），例如答案要素（answer factors）来确定答案的可信度 处理步骤 对每个问题，首先提取每个答案中的关键词，并学习其向量表示 然后将这些词/短语级关键词聚类到语义聚类中，即要素(factors) 这些要素代表了问题答案中所有可能的关键点，可以用来确定答案的可信度； 对于每个簇中的关键字，由于他们具有非常相似的语义含义，他们的可信度应该几乎相同。 用户可能有不同的可信度，能够通过他们提供的答案反映出来 提出一种两步方法估计每个答案的可信度 指定一个考虑用户可靠性的概率模型来建模关键词的生成 首先生成答案要素和其语义参数的混合 然后生成双重用户可信变量，对某个用户提供的答案要素的准确性和全面性进行建模 最后根据语义、答案要素的可信度、提供答案的用户的可信度来选择答案要素；并通过VMF分布生成关键词嵌入向量 优点 答案要素和用户可信度的设计考虑了答案的多因素特性 关键词嵌入向量的生成也捕捉到了词汇用法的多样性 因此能够捕获文本数据的独特特征 Generative Model​ 建立一个概率模型共同学习每个问题的答案要素和每个答案要素的真值标签（truth labels） ​ 对一个答案 $a_{qu}$ ，提取领域特定的答案关键词（keywords），并得到它们的归一化（normalized）向量表示 ​ 所有向量表示的集合记作 ${v_{qum}}$ ，也作为概率模型的观察（observation） ​ 提出模型的盘子表示法(Plate notation)；白色圆圈表示潜在变量，灰色圆圈表示观察，其他表示超参数 模型组成： Answer Factor Modeling答案要素建模 ​ 模型首先根据狄利克雷分布(Dirichlet distribution)生成要素的混合。 ​ 形式上，混合分布 $\\pi_q$ 生成：(服从概率分布)$$\\pi \\sim Dirichlet(\\beta)$$​ 其中，$\\beta$ 是 $K_q$ 维向量，$K_q$ 表示问题 q 的答案中所含的要素个数 ​ 对于问题 q 下的第 k 个答案要素，通过一个二元真值标签 $t_{qk}$ 对其可信度进行建模。 ​ 模型首先生成先验事实概率 $ \\gamma _{qk}$ ，它决定了每个要素为真值的先验分布，服从超参数为 $\\alpha_1^{(a)}$ 和 $\\alpha 0^{(a)}$ 的Beta分 布$$\\gamma{qk} \\sim Beta(\\alpha_1^{(a)},\\alpha_0^{(a)})$$​ 然后真值标签（truth label） $t_{qk} $ 由参数为 $\\gamma_{qk} $ 的伯努利分布（Bernoulli distribution）生成$$t_{qk} \\sim Bernoulli(\\gamma_{qk})$$​ 最后，为了对每个答案要素的语义特征进行建模，通过其共轭先验分布 $\\Phi (\\mu_{qk},\\kappa_{qk};m_0,R_0,c) $ 定义vMF分布 的质心参数（centroid parameter） $\\mu_{qk}$ 、集中参数（concentrate parameter） $\\kappa_{qk}$ 。$$\\mu_{qk},\\kappa_{qk} \\sim \\Phi (\\mu_{qk},\\kappa_{qk};m_0,R_0,c)$$​ 其中，$\\Phi (\\mu_{qk},\\kappa_{qk};m_0,R_0,c)$ 定义： ​ $\\Phi (\\mu_{qk},\\kappa_{qk};m_0,R_0,c) \\propto {C_D(\\kappa_{qk})}^cexp(\\kappa_{kq}R_0m_0^T\\mu_{qk})$ ​ $C_D(\\kappa) = \\frac{\\kappa^{D/2-1}}{I_{D/2-1(\\kappa)}}$ ; $I_{D/2-1}(·)$ 是第一类修正贝塞尔函数（modified Bessel function of the first kind） User Reliability Modeling用户可信度建模 ​ 每个用户的可信度是由其提供的答案推断出来的。 ​ 由于用户提供答案的策略不同（有些人可能只提供很有信心的答案，一些人的答案中可能含有不同可信度的 答案要素），需要使用二重评分（two-fold score）来为用户可信度建模 ​ 假设我们事先知道所有的答案要素（answer factors）和它们的真值标签（truth labels），对所有问题和答 案： ​ $TP_u $ 用户 u 的答案所覆盖的可信要素 $FP_u $ 用户 u 的答案所覆盖的不可信要素 ​ $FN_u$ 用户 u 的答案未覆盖的可信要素 $FN_u $ 用户 u 的答案未覆盖的不可信要素 真实情况 预测结果 预测结果 正例 反例 正例 TP【真正例】 FN【假反例】 反例 FP【假正例】 TN【真反例】 ​ $false\\space positive\\space rate = \\frac{FP_u}{FP_u+TN_u} $ $true\\space positive\\space rate = \\frac{TP_u}{TP_u+FN_u}$ ​ 即可利用假正例率 FPR 、真正例率 TPR 来充分表征用户 u 的可信度 ​ 但是在生成过程中，答案要素及其真值标签是事先不知道的 ​ 定义二重用户可信度变量 $\\phi_u^0$ 和 $\\phi_u^1$ 对用户 u 的答案所覆盖的 FPR 、 TPR 进行建模： ​ 对于每个用户 u ，我们通过两个Beta分布生成 $\\phi_u^0$ 和 $\\phi_u^1$$$\\phi_u^0 \\sim Beta(\\alpha_{0,1},\\alpha_{0,0}) \\qquad\\qquad\\qquad (False\\space Positive\\space Rate)\\ \\phi_u^1 \\sim Beta(\\alpha_{1,1},\\alpha_{1,0}) \\qquad\\qquad\\qquad (True\\space Positive\\space Rate)$$​ 其中 $\\alpha_{0,1}$ 假正例计数 $\\alpha_{0,0}$ 真反例计数 $\\alpha_{1,1}$ 真正例计数 $\\alpha_{1,0}$ 假反例计数 Observation Modeling观察建模 ​ 使用关键词的向量表示作为观察（observations） ​ 对用户 u 对问题 q 的第 m 个单词表示，指定了如下的生成过程： 首先，定义一个二元指标（binary indicator） $y_{u,qk}$ ，表示基于用户 u 的可信度时，问题 q 的答案中第 k 个答案要素是否应该被用户 u 所覆盖。 对于问题 q ，如果其真值标签 $t_{qk} = 1$ ，用户 u 的回答覆盖第 k 个答案要素的概率遵循可信度参数为 $\\phi_u^0 $ 的伯努利分布（Bernoulli distribution）$$y_{u,qk} \\sim Bernoulli(\\phi_u^0) \\qquad\\qquad\\qquad If t_{qk}=0,\\y_{u,qk} \\sim Bernoulli(\\phi_u^1) \\qquad\\qquad\\qquad If t_{qk}=1,$$至此，已经确定了答案 $a_{qu}$ 应该覆盖的答案要素集，并且考虑了用户 u 的可信度 然后，对于答案 $a_{qu}$ 中的第 m 个关键词，其要素标签（factor label） $z_{qum} $ 由概率密度函数表示为： $$P(z_{qum}=k|\\pi_q,y_{u,qk}) \\propto\\begin{cases}\\pi_{qk} \\qquad &amp; if\\space y_{u,qk}=1, \\0 &amp; if\\space y_{u,qk}=0.\\end{cases}$$ ​ 概率密度函数综合考虑了答案要素混合分布（answer factor mixture distribution）和二元指标 $y_{u,q·}$ ，这意 味着在确定一个答案关键词时考虑到了语义和用户可信度 确定要素标签后，模型对描述其对应的要素的语义含义的关键词向量进行采样 该过程中不应该涉及用户可信度 关键词的向量表示（$v_{qum}$）是从参数为 $\\mu_{qk}$ , $\\kappa_{qk}$ 的vMF分布中随机抽样：$$v_{qum} \\sim vMF(\\mu_{qk},\\kappa_{qk}).$$对于一个遵循vMF分布的 D 维单元语义向量（unit semantic vector） v ，其概率密度函数为：$$p(v_{qum}|\\mu_{qk},\\kappa_{qk})=C_D(\\kappa_{qk})exp(\\kappa_{qk}\\mu_{qk}^Tv_{qum})$$对vMF分布，有两个参数： ​ 平均方向（mean direction） $\\mu_{qk}$ , 集中参数（concentration parameter） $\\kappa_{qk} $ ($\\kappa_{qk} &gt;0$) $v_{qum}$ 在单位球（the unit sphere）上的分布集中在平均方向 $\\mu_{qk}$ 的方向上，当 $\\kappa_{qk}$ 越大时越集中 在本文场景中，均值向量 $\\mu$ 作为一个单位球上的语义关注（semantic focus on the unit sphere），并围绕其产生相关的语义映射（produces relevant semantic embeddings around it） Overall generative process​ Trustworthy-Aware Answer Scoring信任感知的答案评分 ​ 答案的可信度应该由其提供的正确信息的量来评估 ​ 给定问题 q 的每个答案要素的推断真值标签（inferred truth labels），根据与真值标签（truth label）$t_{qk}$ ​ $t_{qk}=1$ 的答案要素相关的答案 $a_{qu}$ 中答案关键词的数量对答案进行评分：$$score_{qu} = \\sum_{k=1}^{K_q}N_{u,qk}\\Bbb I(t_{qk}=1),$$​ 其中 ​ $K_q $ 是问题 q 的答案要素的数量 $N_{u,qk}$ 表示用户 u 提供的，且被聚簇到要素 k 中的关键词数量 ​ $\\Bbb I(t_{qk}=1) =1 \\quad if\\space t_{qk}=1 $ $\\Bbb I(t_{qk}=1) =0 \\quad if\\space t_{qk}=0$ Model Fitting模型拟合 ​ 估计潜在变量和用户可信度参数的方法 ExperimentsDatasets数据集 SuperUser Dataset &amp; ServerFault Dataset 介绍： 来自社区问答CQA网站 SuperUser.com 和 ServerFault.com ,主要分别关注日常电脑使用和服务器管理方面的问题。 任务： 为每个问题提取最可信的答案 使用来自网站的答案投票作为评估的基本事实（groundtruths） Student Exam Dataset 介绍： 北得克萨斯大学一个本科生班级提供的计算机科学导论作业的答案，由30名学生提交 任务： 为每个问题提取Top-K可信的学生答案 groundtruth由教师给出，所有的答案由两个评委独立评分，使用从0(完全错误)-5(完美答案)的整数范围 Data Statistics. Item SuperUser ServerFault Student Exam of Questions 3379 7621 80 of Users 1036 1920 30 of Answers 16014 40373 2273 Pre-Processing 预处理 ​ 对所有的数据集，抛弃所有代码块、HTML标记、文本中的停止符； ​ 使用实体字典（entity dictionary）和Stanford POS-Tagger 提取答案关键词 ​ 为训练词向量表示，使用所有抓取的文本作为语料库 ​ 使用 gensim 包中的 Skip-gram 结构学习每个答案关键词的向量表示 ​ 词向量维度为100，上下文窗口大小为5，最小出现次数为20 Comparison Methods Bag-of-Word（BOW） Similarity 利用词袋模型，问题向量与其对应的答案向量之间的相似性对答案进行排序 Topic Similarity 利用LDA模型对每个问题及对应的答案提取一个100维的主题表示，类似BOW，根据问题的余弦相似度进行排序 CRH + Topic Dist. CRH + Word Vec. CATD + Topic Dist. CATD + Word Vec. Evaluation Metrics 评价指标 在CQA数据集上，使用每个问题返回最佳答案的精确度（precision） 在学生测试数据集上，使用每个问题返回的Top-K的答案的平均得分 Performance and Analysis Method ServerFault SuperUser BOW Similarity 0.2077 0.1944 Topic Similarity 0.2462 0.2462 CATD + Topic Dist. 0.2311 0.2308 CATD + Word Vec. 0.1821 0.2234 CRH + Topic Dist. 0.2453 0.2453 CRH + Word Vec. 0.1847 0.2231 TextTruth 0.3985 0.4019 Analysis 基于检索的方法（BOW Similarity、Topic Similarity） 仅根据问题和答案之间的语义相似性对答案进行排序。 问题本身不一定会包含理想答案中应该包含的所有语义。 因此只能发现相关的答案，不能发现可信的答案 现有的真值发现方法能够捕获答案排序的用户可信度，但并不理想 因为将答案视为一个完整的语义单元，忽略了每个答案的语义可能是复杂的，单个向量无法捕获这些答案之间的内在关联 CRH和CATD将这些单个向量表示的加权聚合作为“真实”语义表示来评估用户可靠性，只会产生不准确的表示，进一步导致不正确的汇总结果 Case Study ​ 蓝色的是估计的可信的关键词，红色的是估计不可信的或不相关的关键词 表现分析： ​ 可以看到能够自动选择对问题有意义的关键词，例如 node 、tree 、root ​ 可以看到排名最高的答案中可信关键词比排名靠后的不可信答案中的多 ​ 如果是使用现有的方法，问题本身只包括一个关键词 tree ，基于检索的方法会将不可信答案排在Top2之前， 因为包含的关键词与问题中的完全相同；其次看到正确的关键词涉及多个要素，这在自然语言问答中也十分 常见，现有的方法也无法成功处理。 User Reliability Validation​ CQA数据集中没有直接的用户可信度值，研究学生考试数据集中估计的用户可信度。 ​ 蓝点表示用户，y轴为用户可信度的groundtruth，x轴为估计的用户可信度 ​ 可以看到，groundtruth用户可信度Y增加时，估计的用户可靠性得分X通常会增加 Conclusion由于自然语言的语义模糊性和文本答案的复杂性，现有的真值发现方法都存在对非结构化文本数据的处理问题。 本文提出了一个概率模型 TextTruth，该模型将从答案中提取的关键要素向量表示作为输入，并基于每个答案中关键要素的可信度进行排序输出。 具体来说，模型通过对答案要素嵌入表示的生成过程建模，共同学习每个答案要素聚类的聚类标签和真值标签。","categories":[{"name":"论文阅读","slug":"论文阅读","permalink":"http://yoursite.com/categories/论文阅读/"}],"tags":[{"name":"Data Mining","slug":"Data-Mining","permalink":"http://yoursite.com/tags/Data-Mining/"}]},{"title":"Python-NLP-Chapter3","slug":"Python-NLP-Chapter3","date":"2019-04-02T08:35:42.000Z","updated":"2019-04-15T14:32:16.617Z","comments":true,"path":"2019/04/02/Python-NLP-Chapter3/","link":"","permalink":"http://yoursite.com/2019/04/02/Python-NLP-Chapter3/","excerpt":"","text":"Chapter3 中文分词技术List 中文分词的概念与种类 常用分词（规则分词、统计分词、混合分词等）的技术介绍 开源中文分词工具——Jieba简介 实战分词之高频词提取 中文分词简介 主要流派 规则分词 人工设立词库，按照一定方式进行匹配切分 难以处理新词 统计分词 过于依赖语料质量 混合分词（规则+统计） 主要困难：分词歧义 规则分词基于规则的分词是一种机械分词方法，通过维护词典，在切分语句时，将语句的每个字符串与词表中的词进行逐一匹配，找到则切分，否则不予切分。 按照匹配切分的方式划分： 正向最大匹配法 MM 逆向最大匹配法 RMM 双向最大匹配法 正向最大匹配法Maximum Match Method ，MM法。 基本思想： 已知分词词典中最长词有i个汉子字符，则用被处理文档的当前字串中的前i个字作为匹配字段，查找字典。 如果字典中存在这样的i字的词，匹配成功，将其作为一个词切分出去。 如果不存在，将字段中最后一个字去掉，重新进行匹配；如此进行，直到匹配成功。然后取下一个i字字串进行匹配处理，直到文档被扫描完 12345678910111213141516171819202122232425262728293031323334353637class MM(object): def __init__(self): #设置词典中最长字串为3 self.window_size = 3 def cut(self,text): ''' 对文本进行切片并匹配 :param text:要进行匹配的文本 :return: 匹配到的结果 ''' result = [] index = 0 text_length = len(text) #定义词典 dic = ['研究','研究生','生命','命','的','起源'] while text_length &gt; index: piece = '' #range(start,stop,step) =&gt; range(4,0,-1) =&gt; size in (4,3,2,1) for size in range(self.window_size + index,index,-1): #4,0,-1 #对text进行切片，切size大小 piece = text[index:size] if piece in dic: #在词典中匹配到之后，后移继续切片 index = size - 1 break #没有匹配到，则往后移动 index = index + 1 result.append(piece+'----') print(result) return resultif __name__ == '__main__': text = '研究生命的起源' # text = '研究你生命的起源' tokenizer = MM() tokenizer.cut(text) 输出结果： 1['研究生----', '命----', '的----', '起源----'] 可以看出，当词典中有长词汇覆盖短词汇时表现较差。 逆向最大匹配法Reverse Maximum Match Method RMM法 基本原理与MM法相同，不同的是分词切分的方向与MM法相反 基本思想 从文档末端开始匹配扫描，每次取末端i个字符做匹配 相应的，使用的分词词典是逆序词典 实际操作 先将文档进行倒排处理，生成逆序文档。然后根据逆序词典，对逆序文档使用正向最大匹配法处理即可 由于汉语中偏正结构较多，逆向最大匹配法比正向匹配的误差要小。 1234567891011121314151617181920212223242526class RMM(object): def __init__(self): self.window_size = 3 def cut(self,text): result = [] index = len(text) dic = ['研究','研究生','生命','命','的','起源'] while index &gt; 0: piece = '' for size in range(index - self.window_size,index): piece = text[size:index] if piece in dic: index = size + 1 break index = index - 1 result.append(piece + '----') #倒置result里面元素的位置 result.reverse() print(result) return resultif __name__ == '__main__': text = '研究生命的起源' tokenizer = RMM() tokenizer.cut(text) 输出结果： 1['研究----', '生命----', '的----', '起源----'] 双向最大匹配Bi-directction Matching Method; 将正向最大匹配得到的分词结果和逆向最大匹配得到的结果进行比较，然后按照最大匹配原则，选取词数切分最少的作为结果。 规则 如果正反向分词结果词数不同，取分词数量较少的那个 如果分词结果词数相同： 分词结果相同，没有歧义，可以返回任一个 分词结果不同，返回其中单字较少的那个 123456789101112131415161718192021222324252627class MM(object): ... class RMM(object): ... if __name__ == '__main__': text = '研究生命的起源' tokenizer1 = MM() tokenizer2 = RMM() MMresult = tokenizer1.cut(text) RMMresult = tokenizer2.cut(text) if len(MMresult) == len(RMMresult): print(\"分词结果词数相同\") #判定是否结果相同，否则返回单字较少的 if MMresult == RMMresult: print(\"分词结果相同\") else: #判断单字数量 print(\"返回单字数量少的\") elif len(MMresult) &lt; len(RMMresult): print(\"MMresult:\") print(MMresult) elif len(MMresult) &gt; len(RMMresult): print(\"RMMresult:\") print(RMMresult) 输出结果： 1234['研究生----', '命----', '的----', '起源----']['研究----', '生命----', '的----', '起源----']分词结果词数相同返回单字数量少的 统计分词 主要思想 把每个词看做是由词的最小单位的各个字组成的，如果相连的字在不同的文本中出现的词数越多，就证明这相连的字很可能就是一个词。 因此利用字与字相邻出现的频率来反应成词的可靠度。 操作步骤 建立统计语言模型 对句子进行单词划分，然后对划分结果进行概率计算，获得概率最大的分词方式。 其中使用统计学习算法，如隐马尔科夫HMM，条件随机场CRF等 语言模型为长度为 m 的字符串确定其概率分布P(ω~1~ ，ω~2~，…，ω~m~) ，其中 ω~1~ 到 ω~m~ 依次表示文本中的各个词语。 一般采用链式法则计算其概率值： P(\\omega_1,\\omega_2,···,\\omega_m) = P(\\omega_1)P(\\omega_2|\\omega_1)P(\\omega_3|\\omega_1,\\omega_2)\\\\···P(\\omega_i|\\omega_1,\\omega2,···,\\omega_{i-1})···P(\\omega_m|\\omega_1,\\omega_2,···,\\omega_{m-1})观察可知，当文本过长时，公式右侧从第三项起每一项计算难度都很大。 为解决该问题，提出n元模型（n-gram model）降低该计算难度： ​ 即在估算条件概率时，忽略距离大于等于n的上文词的影响，因此$P(\\omegai|\\omega_1,\\omega2,…,\\omega{i-1})$ 的计算可以简化： P(\\omega_i|\\omega_1,\\omega2,···,\\omega_{i-1}) \\approx P(\\omega_i|\\omega_{i-(n-1)},···,\\omega_{i-1})可以看出，当n=1时称为一元模型 unigram model ，效果并不理想。当 $ n\\geqslant 2 $ 时，该模型可以保留一定的词序信息，且 n 越大，保留的信息越丰富，但计算成本也呈指数级增加。 一般使用频率计数的比例来计算 n 元条件概率： P(\\omega_i|\\omega_{i-(n-1)},···,\\omega_{i-1},\\omega_i) = \\frac{count(\\omega_{i-(n-1)},···,\\omega_{i-1},\\omega_i)}{count(\\omega_{i-(n-1)},···,\\omega_{i-1})}​ 式中 count(\\omega_{i-(n-1)},···,\\omega_{i-1}) 表示词语 \\omega_{i-(n-1)},···,\\omega_{i-1} 在语料库中出现的总次数 ​ 可见，当n越大，模型包含的词序信息越丰富，同时计算量随之增大。与此同时，长度越长的文本序列出现的 词数也会减少，可能出现分子分母为0的情况。 ​ 因此，一般需要配合相应的平滑算法解决该问题，如拉普拉斯平滑算法等 HMM模型隐含马尔可夫模型（HMM），将分词作为字在字串中的序列标注任务来实现的。 基本思路 每个字在构造一个特定的词语时都占据着一个确定的构词位置（即词位），现规定每个字最多只有四个构词位置，即 B（词首）、M（词中）、E（词尾）、S（单独成词）。 1中文 / 分词 / 是 / 文本处理 / 不可或缺 / 的 / 一步！ 1中/B 文/E 分/B 词/E 是/S 文/B 本/M 处/M 理/E 不/B 可/M 或/M 缺/E 的/S 一/B 步/E !/S 用数学抽象表示： ​ 用\\lambda = \\lambda_1\\lambda_2···\\lambda_n代表输入的句子，n为句子长度，\\lambda_i表示字，o=o_1o_2···o_n代表输出的标签，那么理想的输出为： max = maxP(o_1o_2···o_n|\\lambda_1\\lambda_2···\\lambda_n)在分词任务上，o即为B、M、E、S这四种标记，λ为注入“中” “文”等句子中的每一个字（包括标点等非中文字符） 引入观测独立性假设：每个字的输出仅仅与当前字有关 可以得到： P(o_1o_2···o_n|\\lambda_1\\lambda_2···\\lambda_n) = P(o_1|\\lambda_1)P(o_2|\\lambda_2)···P(o_n|\\lambda_n)但是该方法完全没有考虑上下文，会出现不合理的情况： ​ 例如B后面只能是M或者E，基于观测独立假设，可能得到BBB、BEM等输出。 HMM即用来解决该问题的一种方法。通过贝叶斯公式 P(A|B)=\\frac{P(B|A)P(A)}{P(B)}： P(o|\\lambda)=\\frac{P(o,\\lambda)}{P(\\lambda)}=\\frac{P(\\lambda|o)P(o)}{P(\\lambda)}λ为给定的输入，因此P(λ)计算为常数，可以忽略。即最大化P(o|λ)等价于最大化P(λ|o)P(o) 针对 P(λ|o)P(o) 作 马尔科夫假设，得到： P(\\lambda|o) = P(\\lambda_1|o_1)P(\\lambda_2|o_2)···P(\\lambda_n|o_n)同时，对 P(o) 有： P(o) = P(o_1)P(o_2|o_1)P(o_3|o_1,O_2)···P(o_n|o_1,o_2,···,o_{n-1})此时再做齐次马尔科夫假设 ,那么有： P(o) = P(o_1)P(o_2|o_1)P(o_3|O_2)···P(o_n|o_{n-1})于是有： P(\\lambda|o)P(o)\\sim P(\\lambda_1|o_1)P(o_2|o_1)P(\\lambda_2|o_2)P(o_3|o_2)···P(o_n|o_{n-1})P(\\lambda_n|o_n)在HMM中，将P(\\lambda_k|o_k)称为发射概率，P(o_k|o_{k-1})称为转移概率。通过设置某些P(o_k|o_{k-1})=0，可以排除类似BBB、EM等不合理的组合。 在HMM中，求解maxP(λ|o)P(o)的常用方法是Veterbi算法，它是一种动态规划方法，核心思想是： ​ 如果最终的最优路径经过某个o_i ,那么从初始结点到o_{i-1}点的路径必然也是一个最优路径，因为每个节点o_i只 会影响前后两个P(o_{i-1}|o_i) 和 P(o_i|o_{i+1}) 根据该思想，可通过地推的方法，在考虑每个o_i时只需要求出所有经过各o_{i-1}的候选点的最优路径，再与当前的o_i结合比较。每步只需要算不超过l^2次。Viterbi算法的效率是O(n·l^2)，l 是候选数目最多的结点o_i的候选数目。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193class HMM(object): def __init__(self): pass def try_load_model(self,trained): pass def train(self,path): pass def viterbi(self,text,states,strat_p,trans_p,emit_p): pass def cut(self,text): passclass HMM(object): def __init__(self): import os #存取算法中间结果，不用每次都训练模型 self.model_file = './data/hmm_model.pkl' #状态值集合 self.state_list = ['B','M','E','S'] #参数加载，用于判断是否需要重新加载model_file self.load_para = False #加载已计算的中间结果，当需要重新训练时，需要初始化清空结果 def try_load_model(self,trained): if trained: import pickle with open(self.model_file,'rb') as f: self.A_dic = pickle.load(f) self.B_dic = pickle.load(f) self.Pi_dic = pickle.load(f) self.load_para = True else: #状态转移概率 (状态-&gt;状态的条件概率) self.A_dic = &#123;&#125; #发射概率 (状态-&gt;词语的条件概率) self.B_dic = &#123;&#125; #状态的初始概率 (一句话第一个字被标记成S B E M的概率) self.Pi_dic = &#123;&#125; self.load_para = False #计算转移概率、发射概率、初始概率 def train(self,path): #重置几个概率矩阵 self.try_load_model(False) #统计状态出现次数，求P(o) Count_dic = &#123;&#125; #初始化参数 def init_parameters(): for state in self.state_list: self.A_dic[state] = &#123;s:0.0 for s in self.state_list&#125; self.Pi_dic[state] = 0.0 self.B_dic[state] = &#123;&#125; Count_dic[state] = 0 def makeLabel(text): out_text = [] if len(text) == 1: #单独成词 out_text.append('S') else: #词首、词中、词尾 out_text += ['B']+['M']*(len(text)-2)+['E'] return out_text init_parameters() line_num = -1 #观察者集合，主要是字以及标点等 words = set() with open(path,encoding='utf8') as f: for line in f: line_num += 1 #去除首位空格 line = line.strip() if not line: continue word_list = [i for i in line if i != ' '] words |= set(word_list) #更新字的集合 | 位或运算符，按照二进制做或操作 linelist = line.split() line_state = [] for w in linelist: line_state.extend(makeLabel(w)) #assert断言，声明布尔值必须为真的判定，如果发生异常表明表达式为假 assert len(word_list) == len(line_state) for k,v in enumerate(line_state): Count_dic[v] += 1 if k==0: self.Pi_dic[v] += 1 #每个句子的第一个字的状态，用于计算初始状态概率 else: self.A_dic[line_state[k-1]][v] += 1 #计算转移概率 self.B_dic[line_state[k]][word_list[k]] = \\ self.B_dic[line_state[k]].get(word_list[k],0) + 1.0 #计算发射概率 self.Pi_dic = &#123;k:v *1.0/line_num for k,v in self.Pi_dic.items()&#125; self.A_dic = &#123;k:&#123;k1:v1 / Count_dic[k] for k1,v1 in v.items()&#125; for k,v in self.A_dic.items()&#125; #加1平滑 self.B_dic = &#123;k:&#123;k1:(v1+1)/Count_dic[k] for k1,v1 in v.items()&#125; for k,v in self.B_dic.items()&#125; #序列化 import pickle with open(self.model_file,'wb') as f : #使用pickle.dump序列化对象，并将结果数据流写到文件对象中 pickle.dump(self.A_dic,f) pickle.dump(self.B_dic,f) pickle.dump(self.Pi_dic,f) return self def viterbi(self,text,states,start_p,trans_p,emit_p): ''' Veterbi算法的实现，求最大概率的路径 :param text: :param states: :param start_p: 初始概率 :param trans_p: 转移概率 :param emit_p: 发射概率 :return: ''' V = [&#123;&#125;] path = &#123;&#125; for y in states: V[0][y] = start_p[y] * emit_p[y].get(text[0],0) path[y] = [y] for t in range(1,len(text)): V.append(&#123;&#125;) newpath = &#123;&#125; #检验训练的发射概率矩阵中是否有该字 #python在一行后加 \\ 作为换行标志符 neverSeen = text[t] not in emit_p['S'].keys() and \\ text[t] not in emit_p['M'].keys() and \\ text[t] not in emit_p['E'].keys() and \\ text[t] not in emit_p['B'].keys() for y in states: emitP = emit_p[y].get(text[t],0) if not neverSeen else 1.0 #设置未知字单独成词 (prob, state) = max( [(V[t - 1][y0] * trans_p[y0].get(y, 0) * emitP, y0) for y0 in states if V[t - 1][y0] &gt; 0]) V[t][y] = prob newpath[y] = path[state] + [y] path = newpath if emit_p['M'].get(text[-1],0)&gt;emit_p['S'].get(text[-1],0): (prob,state) = max([(V[len(text) - 1][y],y) for y in ('E','M')]) else: (prob,state) = max([(V[len(text) - 1][y],y) for y in states]) return (prob,path[state]) def cut(self,text): import os if not self.load_para: self.try_load_model(os.path.exists(self.model_file)) prob,pos_list = self.viterbi(text,self.state_list,self.Pi_dic,self.A_dic,self.B_dic) begin,next = 0,0 for i,char in enumerate(text): pos = pos_list[i] if pos == 'B': begin = i elif pos == 'E': #python yield 生成器 yield text[begin:i+1] next = i+1 elif pos == 'S': yield char next = i+1 if next&lt;len(text): yield text[next:]hmm = HMM()hmm.train('./data/trainCorpus.txt_utf8')text='这是一个非常棒的方案!'res = hmm.cut(text)print(text)print(str(list(res))) 输出结果： 12这是一个非常棒的方案!['这是', '一个', '非常', '棒', '的', '方案', '!'] 其他统计分词算法条件随机场 CRF 也是一种基于马尔科夫思想的统计模型 ​ CRF使得每个状态不止与它前面的状态有关，还与它后面的状态有关 神经网络分词算法，将深度学习方法应用 ​ 通常采用CNN、LSTM等自动发现一些模式和特征，然后结合CRF、softmax等分类算法进行分词预测 混合分词​ 在实际工程应用中，多是基于一种分词算法，然后用其他分词算法加以辅助。 中文分词工具——JiebaJieba官方地址 Jieba分词集合了基于规则和基于统计两类方法 首先基于前缀词典进行词图扫描；对于未登陆词，Jieba使用了基于汉字成词的HMM模型，采用Viterbi算法推导 前缀词典： ​ 词典中的词按照前缀包含的顺序排列；如词典中出现“上”，之后以“上“开头的词都会出现在这一部分，如”上 海“，进而出现”上海市“，从而形成一种层级包含结构。 ​ 因此可以快速构建包含全部可能分词结果的有向无环图； ​ 有向：指全部的路径都始于第一个字，止于最后一个字 ​ 无环：节点之间不构成闭环 Jieba的三种分词模式 精确模式：将句子最精确的切开，适合文本分析 全模式：把句子中所有可以成词的词语都扫描出来，速度快；但是不能解决歧义 搜索引擎模式：在精确模式的基础上，对长词再次切分，提高召回率；适用于搜索引擎分词 12345678910import jiebasent = '中文分词是文本处理不可或缺的一步！'seg_list1 = jieba.cut(sent,cut_all=True)print('全模式：','/'.join(seg_list1))seg_list2 = jieba.cut(sent,cut_all=False)print('精确模式：','/'.join(seg_list2))seg_list3 = jieba.cut(sent)print('默认精确模式：','/'.join(seg_list3))seg_list4 = jieba.cut_for_search(sent)print('搜索引擎模式：','/ '.join(seg_list4)) 输出结果： 1234全模式： 中文/分词/是/文本/文本处理/本处/处理/不可/不可或缺/或缺/的/一步//精确模式： 中文/分词/是/文本处理/不可或缺/的/一步/！默认精确模式： 中文/分词/是/文本处理/不可或缺/的/一步/！搜索引擎模式： 中文/ 分词/ 是/ 文本/ 本处/ 处理/ 文本处理/ 不可/ 或缺/ 不可或缺/ 的/ 一步/ ！ 实战：高频词提取高频词：指文档中出现频率较高且非无用的词语，在一定程度上代表了文档的焦点所在。 高频词提取：自然语言处理中的TF（Term Frequency）策略。 ​ 干扰项：标点符号、停用词（“的”，“是”，“了”等无意义的常用词） 12345678910111213141516171819202122232425262728293031323334353637383940414243444546def get_content(path): ''' 加载指定路径下的数据 :param path: :return: ''' with open(path,'r',encoding='gbk',errors='ignore') as f: content = '' for l in f : l = l.strip() content += l return contentdef get_TF (words,topK=10): ''' 高频词统计 :param words: :param topK: 前N个高频词 :return: 高频词数组 ''' tf_dic = &#123;&#125; for w in words: tf_dic[w] = tf_dic.get(w,0) + 1 #lambda 快速构建一个匿名函数 #python sorted函数进行排序，返回一个新的list #key参数传入一个自定义lambda函数，x:x[1]即根据列表中每个元组的第二个元素进行排序 #reverse设置为True进行倒序排列，从大到小 return sorted(tf_dic.items(),key=lambda x:x[1],reverse=True)[:topK]def main(): import glob import random import jieba #glob 通配符模块，对目录内容进行匹配 files = glob.glob('./data/news/C000013/*.txt') corpus = [get_content(x) for x in files] #randint生成一个范围内的指定随机整数 sample_inx = random.randint(0,len(corpus)) split_words = list(jieba.cut(corpus[sample_inx])) print('样本之一：'+corpus[sample_inx]) print('样本分词效果：'+'/'.join(split_words)) print('样本的TopK(10)词：'+str(get_TF(split_words)))main() 展示结果： 123样本之一：风油精含有薄荷脑、樟脑、桉叶油、丁香酚、水杨酸甲酯等成分，有消炎止痛、清凉止痒、杀菌、抗真菌等功效，所以常用于蚊虫叮咬及伤风感冒引起的头痛、头晕、晕车等症状。其实，它还可以用于轻度烫伤的治疗。烫伤最危险的是损伤部位由于细菌侵入而引起感染，而风油精中的薄荷脑、樟脑、桉叶油成分恰好对细菌有较好的杀灭作用，所以当受到小范围烫伤时，不妨试试风油精。不过，使用风油精前必须先分清烫伤的轻重。一般而言，只要是由火焰、热水、蒸汽、电流、放射线、激光、强酸、强碱等作用于人体所引起的损伤都可统称为烧烫伤。在临床上，烫伤被分为三级。Ⅰ度烫伤：为表皮烫伤，局部皮肤发红疼痛；Ⅱ度烫伤：浅Ⅱ度烫伤是真皮浅层烫伤，局部可出现水泡，泡壁薄，极度水肿，有剧痛；深Ⅱ度烫伤可达真皮深层，疼痛较轻，水泡较小，泡壁较厚，治愈后可留下瘢痕；Ⅲ度烫伤：烫伤深度达皮肤全层或更深，皮色苍白或形成焦痂，无痛感，经治疗后焦痂可脱落，但会形成肉芽创面，或引起局部畸形。可使用风油精治疗的烫伤主要指小范围浅Ⅱ度烫伤或Ⅰ度烫伤。其余类型烫伤必须送医院救治。使用方法：将风油精直接滴敷在烫伤部位，每隔3—4小时滴敷一次，不仅止痛效果明显，且不易发生感染，无结痂，愈后一般不会留下瘢痕。样本分词效果：风油精/含有/薄荷脑/、/樟脑/、/桉叶油/、/丁香酚/、/水杨酸甲酯/等/成分/，/有/消炎/止痛/、/清凉/止痒/、/杀菌/、/抗真菌/等/功效/，/所以/常用/于/蚊虫/叮咬/及/伤风感冒/引起/的/头痛/、/头晕/、/晕车/等/症状/。/其实/，/它/还/可以/用于/轻度/烫伤/的/治疗/。/烫伤/最/危险/的/是/损伤/部位/由于/细菌/侵入/而/引起/感染/，/而/风油精/中/的/薄荷脑/、/樟脑/、/桉叶油/成分/恰好/对/细菌/有/较/好/的/杀灭/作用/，/所以/当/受到/小/范围/烫伤/时/，/不妨/试试/风油精/。/不过/，/使用/风油精/前/必须/先/分清/烫伤/的/轻重/。/一般而言/，/只要/是/由/火焰/、/热水/、/蒸汽/、/电流/、/放射线/、/激光/、/强酸/、/强碱/等/作用/于/人体/所/引起/的/损伤/都/可/统称/为/烧烫伤/。/在/临床/上/，/烫伤/被/分为/三级/。/Ⅰ/度/烫伤/：/为/表皮/烫伤/，/局部/皮肤/发红/疼痛/；/Ⅱ/度/烫伤/：/浅/Ⅱ/度/烫伤/是/真皮/浅层/烫伤/，/局部/可/出现/水泡/，/泡壁/薄/，/极度/水肿/，/有/剧痛/；/深/Ⅱ/度/烫伤/可/达/真皮/深层/，/疼痛/较/轻/，/水泡/较/小/，/泡壁/较厚/，/治愈/后/可/留下/瘢痕/；/Ⅲ/度/烫伤/：/烫伤/深度/达/皮肤/全层/或/更深/，/皮色/苍白/或/形成/焦痂/，/无/痛感/，/经/治疗/后/焦痂/可/脱落/，/但会/形成/肉芽/创面/，/或/引起/局部/畸形/。/可/使用/风油精/治疗/的/烫伤/主要/指小/范围/浅/Ⅱ/度/烫伤/或/Ⅰ/度/烫伤/。/其余/类型/烫伤/必须/送/医院/救治/。/使用/方法/：/将/风油精/直接/滴/敷/在/烫伤/部位/，/每隔/3/—/4/小时/滴/敷/一次/，/不仅/止痛/效果/明显/，/且/不易/发生/感染/，/无/结痂/，/愈后/一般/不会/留下/瘢痕/。样本的TopK(10)词：[('，', 28), ('、', 18), ('烫伤', 18), ('。', 10), ('的', 8), ('度', 7), ('风油精', 6), ('可', 6), ('等', 4), ('引起', 4)] 自定义词典优化： ​ 将常用的停用词（包括标点符号）写入到文件中，并进行过滤 123456def stop_words(path): with open(path，encoding='utf8') as f : return [l.strip() for l in f]#过滤停用词split_words = [x for x in jieba.cut(corpus[sample_inx]) if x not in stop_words('./data/stop_words.utf8')] ​ 输出结果： 123样本之一：肺燥型咳嗽，干咳无痰或少痰者，宜吃以下之品。百合能润肺止咳。《上海常用中草药》中说：“百合治肺热咳嗽，干咳久咳。”可用鲜百合100~150克，加冰糖适量煎汤喝。也可用新百合120克，蜂蜜50克，拌和蒸熟后食用。甘蔗有生津、润肺、止咳的功效。肺燥咳嗽之人尤为适宜。可单饮甘蔗汁，或用蔗浆和粳米或糯米煮成稀薄粥食用。豆腐浆《随息居饮食谱》云：“豆浆清肺补胃，润燥化痰。”所以，燥热型干咳少痰者宜食。蜂蜜善于润燥，可治肺燥咳嗽。凡干咳无痰、燥咳不愈者，单用蜂蜜15~30克，开水冲服，早晚各1次。也可用蜂蜜50克，熟猪油50克，同熬匀后备用，每日早晚开水冲服1调羹。饴糖据《日华子本草》记载：“饴糖消痰止嗽，并润五脏。”凡燥咳无痰或少痰之人，宜用饴糖30克，同白萝卜汁100克，搅匀后蒸熟，分2次服食，连吃3~5天，有很好的润肺止咳效果。白木耳能滋阴、润肺、生津、止咳，适宜肺燥干咳无痰之人食用。《增订伪药条辨》就有记载：“白木耳治肺热肺燥，干咳痰嗽。”柿霜有润燥、化痰、止咳的作用，能治肺热燥咳。《医学衷中参西录》中说得好：“柿霜入肺，而甘凉滑润。其甘也，能益肺气；其凉也，能清肺热；其滑也，能利肺痰；其润也，能滋肺燥。”可以每次用柿霜10克，开水冲服，每日2~3次。北沙参能养阴清肺、祛痰止咳，适宜肺热燥咳者煎水喝。若用北沙参、麦门冬各10克，川贝母6克，同煎服则更妙。海松子能润肺燥，故肺燥于咳者宜食。《玄感传尸方》中有“风髓汤”一法：“治肺燥咳嗽：松子仁一两(30克)，胡桃仁二两(100克)。研膏，和熟蜜半两(15克)收之。每服二钱(6克)，食后沸汤点服。”或采用《士材三书》中的办法，用海松子仁30克，同粳米50克煮成稀薄粥食用。花生有润肺止咳作用，适宜燥咳之人服食。《药性考》中记载：“落花生，干咳者宜餐，滋燥润火。”民间常用花生仁、大枣、蜂蜜或冰糖各30克，水煎，吃花生和枣并饮汤，日服2次，对干咳无痰或少痰者颇宜。白砂糖能润肺生津，肺燥咳嗽之人宜服食。《本草纲目》云：“白砂糖润心肺燥热，治嗽消痰。”由白砂糖煎炼而成的冰块状结晶，也同样有润肺止咳作用，或含化，或煎水饮，对燥热者均宜。橄榄有清肺、生津、止咳的作用。《本草再新》中指出：“润肺滋阴，消痰理气，止咳嗽。”中国药科大学叶橘泉教授也认为橄榄“有清肺解毒化痰之功”。由于橄榄能润肺滋阴生津液，故燥咳之人宜食之，橄榄又有清肺消痰作用，所以肺热咳嗽者亦宜。榧子有润肺燥，止咳嗽的作用。《本草备要》记载：“榧子润肺杀虫”。《生生编》言其“治咳嗽”。《本草再新》中还说：“治肺火，止咳嗽”。所以，对燥热型咳嗽者，食之最宜。燕窝性味甘平，有养阴润燥作用。《本草从新》中曾说：“大养肺阴，化痰止嗽，补而能清”。《食物宜忌》也认为它能“润肺，消痰涎”。《本草再新》还指出：“大补元气，润肺滋阴，治虚劳咳嗽。”所以，对燥热咳嗽，久咳无痰或少痰之人，食之尤宜。芝麻性味甘辛，有润五脏的作用，故肺燥干咳之人宜食之。民间有用黑芝麻125克，冰糖30克，共捣烂，每次开水冲服15~30克，早晚各服一次，专治干咳症。黄精性子味甘，能润肺养阴生津，故肺燥型咳嗽久咳无痰者宜食之。《四川中药志》中就曾指出：“补肾润肺，益气滋阴，治肺虚咳嗽”，福建省民间常以此同冰糖炖食，以治肺燥干咳。石斛有生津养阴的作用。《药品化义》中说得好：“石斛气味轻清，合肺之性，性凉而清，得肺之宜。肺为娇脏，独此最为相配。主治肺气久虚，咳嗽不止。”凡肺虚肺燥久咳少痰之人，食之最宜。柿饼性味甘涩而凉。《本草通玄》中认为它能“润心肺，消痰”，古代医家常用以治疗肺燥干咳无痰或咳痰带血，民间也有用柿饼2个，川贝母末9克，柿饼挖开去核，纳入川贝末，饭上蒸熟，一次食尽，日服2次。对肺燥型咳嗽者颇宜。猪肉有滋阴、润燥作用，尤其是肥猪肉，干咳燥咳者宜食之。正如《本草备要》所言：“猪肉生痰，惟风痰、湿痰、寒痰忌之，如老人燥痰干咳，更须肥浓以滋润之，不可拘泥于猪肉生痰之说也。”《随息居饮食谱》中还介绍：“治干嗽!猪肉煮汤，吹去油饮。”阿胶性子味甘，能滋阴，对肺燥干咳和肺阴虚所致的久咳不止者，食之尤宜。正如明·李时珍所言：“阿胶，大要只是补血与液，故能清肺益阴而治诸证。”《圣济总录》还有“阿胶饮”，就是以阿胶为主，“治久咳嗽”。甜杏仁性平味甘，能润肺止咳平喘，对燥咳虚喘者尤宜。《食物中药与便方》介绍：“肺病虚弱，老年咳嗽，干咳无痰：甜杏仁炒熟，每日早、晚嚼食7～10粒。或加砂糖一同捣烂细研，开水冲服，1日2次。”鸭肉性平，味甘咸，能滋阴止咳。《本草汇》中曾说它“滋阴除蒸，化虚痰，止咳嗽”。《随息居饮食谱》亦认为：“鸭肉滋五脏之阴，清虚劳之热，……止嗽。”故对肺燥型咳嗽，干咳无痰或少痰者，宜食之。鸭蛋亦有滋阴止咳作用，故肺燥咳嗽者亦宜。此外，风寒型咳嗽之人还宜吃些花生、赤砂糖、南瓜、大蒜、薤白、砂仁、桂皮、香醋、咖啡等。风热型咳嗽还宜服食苹果、草莓、菠萝、橄榄、椰子浆、菊花脑、瓠子、节瓜、苦瓜、地瓜、黄瓜、菜瓜、榧子、莴苣、茭白、薤菜、芹菜、绿豆芽等。肺燥型咳嗽者还宜吃牛奶、胖大海、青菜、无花果、梨子、桑椹、枇杷等。样本分词效果：肺燥/型/咳嗽/干咳/无痰/少/痰/宜吃/以下/之品/百合/润肺/止咳/上海/常用/中草药/中说/百合/治肺/热/咳嗽/干咳/久/可用/鲜/百合/100/150/克/加/冰糖/适量/煎/汤/喝/可用/新/百合/120/克/蜂蜜/50/克/拌和/蒸熟/食用/甘蔗/有生/津/润肺/止咳/功效/肺燥/咳嗽/尤为/适宜/可单/饮/甘蔗汁/或用/蔗浆/粳米/糯米/煮成/稀薄/粥/食用/豆腐/浆/随息/居/饮食/谱/云/豆浆/清肺/补胃/润燥/化痰/燥热/型/干咳/少/痰/宜食/蜂蜜/善于/润燥/可治/肺燥/咳嗽/干咳/无痰/燥/愈者/单/蜂蜜/15/30/克/开水/冲服/早晚/次/可用/蜂蜜/50/克/熟猪油/50/克/同熬/匀/后备/每日/早晚/开水/冲服/调羹/饴糖/日华子/本草/记载/饴糖/消痰/止嗽/并润/五脏/凡燥/痰/少/痰/宜用/饴糖/30/克/白萝卜/汁/100/克/搅匀/蒸熟/分/次/服食/吃/天/润肺/止咳/效果/白木耳/滋阴/润肺/生津/止咳/适宜/肺燥/干咳/无痰/食用/增订/伪药/条辨/记载/白木耳/治肺/热/肺燥/干/咳痰/嗽/柿霜/润燥/化痰/止咳/作用/能治肺/热燥/医学/衷中/参西录/中/说得好/柿霜/入/肺/甘凉/滑润/其甘/能益/肺气/其凉/清肺/热/其滑/能利/肺/痰/其润/能滋/肺燥/每次/柿霜/10/克/开水/冲服/每日/次/北沙参/养阴清/肺/祛痰/止咳/适宜/肺热燥/咳者/煎/水/喝/若用/北沙参/麦门冬/10/克/川贝母/克/煎/服则/更妙/海/松子/润肺/燥/肺燥/咳者/宜食/玄感传/尸方/中有/风髓/汤/一法/治/肺燥/咳嗽/松子/仁/一两/30/克/胡桃/仁/二两/100/克/研膏/熟蜜/半两/15/克/收之/每服/二钱/克/食后/沸汤/点服/采用/士材/三书/中/办法/用海/松子/仁/30/克/粳米/50/克/煮成/稀薄/粥/食用/花生/润肺/止咳/作用/适宜/燥/咳之人/服食/药性/考/中/记载/落花生/干咳/宜餐/滋燥润火/民间/常用/花生仁/大枣/蜂蜜/冰糖/30/克/水/煎/吃/花生/枣/饮汤/日服/次/干咳/无痰/少/痰/颇/宜/白砂糖/润肺/生津/肺燥/咳嗽/之人宜/服食/本草纲目/云/白砂糖/润/心肺/燥热/治嗽/消痰/白砂糖/煎/炼而成/冰块/状/结晶/同样/润肺/止咳/作用/含化/煎/水/饮/燥热/均/宜/橄榄/清肺/生津/止咳/作用/本草/新/中/指出/润肺/滋阴/消痰/理气/止/咳嗽/中国/药科/大学/叶橘/泉/教授/认为/橄榄/清肺/解毒/化痰/之功/橄榄/润肺/滋阴/生/津液/故燥/咳之人/宜食/橄榄/清肺/消痰/作用/肺热/咳嗽/宜/榧子/润肺/燥/止/咳嗽/作用/本草/备要/记载/榧子/润肺/杀虫/生/生编/言/治咳嗽/本草/新/中/治肺/火/止/咳嗽/燥热/型/咳嗽/食/最宜/燕窝/性味/甘平/养阴润/燥/作用/本草/新/中曾/大养/肺/阴/化痰/止嗽/补而能/清/食物/宜忌/认为/润肺/消痰/涎/本草/新/指出/大补/元气/润肺/滋阴/治/虚劳/咳嗽/燥热/咳嗽/久/痰/少/痰/食/尤宜/芝麻/性味/甘辛/有润/五脏/作用/肺燥/干咳/之人/宜食/民间/有用/黑芝麻/125/克/冰糖/30/克/共/捣烂/每次/开水/冲服/15/30/克/早晚/各服/一次/专治/干咳/症/黄精/性子/味甘/润肺/养阴生/津/肺燥/型/咳嗽/久/痰/宜食/四川/中药/志/中/指出/补肾/润肺/益气/滋阴/治肺/虚/咳嗽/福建省/民间/常/以此/冰糖/炖/食/以治/肺燥/干咳/石斛/有生/津养/阴/作用/药品/化义/中/说得好/石斛/气味/轻清/合肺/之性/性凉/清/肺之宜/肺为/娇脏/独此/最为/相配/主治/肺气久/虚/咳嗽/不止/肺/虚/肺燥/久/咳少/痰/食/最宜/柿饼/性味/甘涩/凉/本草/通玄/中/认为/润/心肺/消痰/古代/医家/常用/治疗/肺燥/干咳/无痰/咳痰/带血/民间/有用/柿饼/川贝母/末/克/柿饼/挖/开去/核/纳入/川贝/末/饭上/蒸熟/一次/食尽/日服/次/肺燥/型/咳嗽/颇/宜/猪肉/滋阴/润燥/作用/尤其/肥/猪肉/干咳/燥/咳者/宜食/本草/备要/言/猪肉/生痰/惟风/痰/湿痰/寒痰/忌/老人/燥/痰/干咳/更须/肥/浓以/滋润/不可/拘泥于/猪肉/生痰/随息/居/饮食/谱/中/介绍/治/干嗽/猪肉/煮汤/吹/油/饮/阿胶/性子/味甘/滋阴/肺燥/干咳/肺/阴虚/所致/久/不止/食/尤宜/明/·/李时珍/言/阿胶/补血/液/故能/清肺/益阴而治/诸证/圣济总录/阿胶/饮/阿胶/为主/治久/咳嗽/甜/杏仁/性/平味甘/润肺/止咳/平喘/对燥/咳虚喘者/尤宜/食物/中药/便方/介绍/肺病/虚弱/老年/咳嗽/干咳/无痰/甜/杏仁/炒熟/每日/早/晚嚼食/～/10/粒/或加/砂糖/一同/捣烂/细研/开水/冲服/日/次/鸭肉/性平/味甘咸/滋阴/止咳/本草/汇/中曾/滋阴/除蒸/化虚/痰/止/咳嗽/随息/居/饮食/谱/认为/鸭肉/滋/五脏/之阴/清/虚劳/之热/止嗽/故对/肺燥/型/咳嗽/干咳/无痰/少/痰/宜食/鸭蛋/滋阴/止咳/作用/肺燥/咳嗽/宜/风寒/型/咳嗽/之人/宜吃些/花生/赤/砂糖/南瓜/大蒜/薤白/砂仁/桂皮/香醋/咖啡/风热型/咳嗽/还宜/服食/苹果/草莓/菠萝/橄榄/椰子/浆/菊花/脑/瓠子/节瓜/苦瓜/地瓜/黄瓜/菜瓜/榧子/莴苣/茭白/薤菜/芹菜/绿豆芽/肺燥/型/咳嗽/宜/吃/牛奶/胖大海/青菜/无花果/梨子/桑椹/枇杷样本的TopK(10)词：[('咳嗽', 25), ('克', 22), ('肺燥', 19), ('润肺', 17), ('干咳', 16), ('痰', 14), ('止咳', 12), ('作用', 11), ('滋阴', 10), ('本草', 9)] 用户可以加载自己的自定义词典： 12#Jieba加载用户自定义词典jieba.load_userdict('./data/user_dict.utf8') 要求用户词典格式一般如下： 1234朝三暮四 3 i大数据 5汤姆 nz公主坟 每一行分为三个部分：词语、词频（可省略）、词性（可省略）；用空格隔开，顺序不可以颠倒； 词典文件需要是 utf8 编码","categories":[{"name":"Python自然语言处理实战","slug":"Python自然语言处理实战","permalink":"http://yoursite.com/categories/Python自然语言处理实战/"}],"tags":[{"name":"NLP","slug":"NLP","permalink":"http://yoursite.com/tags/NLP/"},{"name":"Python","slug":"Python","permalink":"http://yoursite.com/tags/Python/"},{"name":"Jieba","slug":"Jieba","permalink":"http://yoursite.com/tags/Jieba/"},{"name":"中文分词","slug":"中文分词","permalink":"http://yoursite.com/tags/中文分词/"}]},{"title":"Python-NLP-Chapter2","slug":"Python-NLP-Chapter2","date":"2019-03-31T09:54:59.000Z","updated":"2019-04-02T08:38:03.843Z","comments":true,"path":"2019/03/31/Python-NLP-Chapter2/","link":"","permalink":"http://yoursite.com/2019/03/31/Python-NLP-Chapter2/","excerpt":"","text":"Chapter 2 NLP前置技术解析list 选择Python作为NLP开发语言 安装与使用Anaconda 正则表达式 Numpy Python语言 官方中文文档 正则表达式在NLP的基本应用作用： ​ 将非结构化、半结构化文本转为结构化以方便后续的文本挖掘；去除“噪声” 正则表达式中一些特殊符号可以处理常用逻辑 符号 含义 . 匹配任意一个字符“.” 代替任何单个字符，换行除外 ^ 匹配开始的字符串 $ 匹配结尾的字符串 [] 匹配多个字符串 匹配字符串在Python中使用re模块实现正则表达式。 通过使用 123456789101112**例 获取包含“爬虫”关键字的句子**```pythonimport retext_string = &apos;文本最重要的来源无疑是网络。我们要把网络中的文本获取形成一个文本数据库。利用一个爬虫抓取到网络中的信息。爬取的策略有广度爬取和深度爬取。根据用户的需求，爬虫可以有主题爬虫和通用爬虫之分。&apos;regex = &apos;爬虫&apos;#以句号为分隔符通过split切分段落为句子p_string = text_string.split(&apos;。&apos;) for line in p_string: if re.search(regex,line) is not None: print(line) 输出结果： 12利用一个爬虫抓取到网络中的信息根据用户的需求，爬虫可以有主题爬虫和通用爬虫之分 例 匹配任意一个字符 正则表达式 可以匹配的例子 不能匹配的例子 a.c abc , branch add , crash ..t bat , oat it , table 123456789import retext_string = '文本最重要的来源无疑是网络。我们要把网络中的文本获取形成一个文本数据库。利用一个爬虫抓取到网络中的信息。爬取的策略有广度爬取和深度爬取。根据用户的需求，爬虫可以有主题爬虫和通用爬虫之分。'#查找包含“爬”+任意一个字的句子regex = '爬.'#以句号为分隔符通过split切分段落为句子p_string = text_string.split('。')for line in p_string: if re.search(regex,line) is not None: print(line) 输出结果： 123利用一个爬虫抓取到网络中的信息爬取的策略有广度爬取和深度爬取根据用户的需求，爬虫可以有主题爬虫和通用爬虫之分 例 匹配起始和结尾字符串 “^a” 表示的是匹配所有以字母a开头的字符串 “a$” 表示的是所有以字母a结尾的字符串 123456789import retext_string = '文本最重要的来源无疑是网络。我们要把网络中的文本获取形成一个文本数据库。利用一个爬虫抓取到网络中的信息。爬取的策略有广度爬取和深度爬取。根据用户的需求，爬虫可以有主题爬虫和通用爬虫之分。'#查找以“文本”为起始的句子regex = '^文本'#以句号为分隔符通过split切分段落为句子p_string = text_string.split('。')for line in p_string: if re.search(regex,line) is not None: print(line) 输出结果： 1文本最重要的来源无疑是网络 123456789import retext_string = '文本最重要的来源无疑是网络。我们要把网络中的文本获取形成一个文本数据库。利用一个爬虫抓取到网络中的信息。爬取的策略有广度爬取和深度爬取。根据用户的需求，爬虫可以有主题爬虫和通用爬虫之分。'#查找以“信息”为结尾的句子regex = '信息$'#以句号为分隔符通过split切分段落为句子p_string = text_string.split('。')for line in p_string: if re.search(regex,line) is not None: print(line) 输出结果： 1利用一个爬虫抓取到网络中的信息 例 匹配多个字符 [bcr]at代表的是匹配 bat 、cat 、 rat 1234567891011import retext_string = ['[重要的]今年第七号台风23日登录广州东部沿海地区','上海发布车库销售管理通知：违规者暂停网签资格','[紧要的]中国对印连发强硬信息，印度急切需要结束对峙']#使用^表示起始#存在“重要”或“紧要”，使用[]匹配多个字符#以“..”代表之后的两个字符regex = '^\\[[重紧]..\\]'for line in text_string: if re.search(regex,line) is not None: print(line) else: print(\"not match\") 输出结果： 123[重要的]今年第七号台风23日登录广州东部沿海地区not match[紧要的]中国对印连发强硬信息，印度急切需要结束对峙 转义字符”\\”匹配正则表达式中使用”\\”作为转义字符，因此如果要匹配文本中的字符”\\”,需要四个反斜杠：123456789前两个和后两个分别在编程语言里转义成反斜杠，然后再在正则表达式里转义成一个反斜杠```pythonimport reif re.search(&quot;\\\\\\\\&quot;,&quot;I have one nee\\dle&quot;) is not None: print(&quot;match it&quot;)else: print(&quot;not match&quot;) 输出结果： 1match it Python中可以通过12345678````pythonimport reif re.search(r&quot;\\\\&quot;,&quot;I have one nee\\dle&quot;) is not None: print(&quot;match it&quot;)else: print(&quot;not match&quot;)` 输出结果： 1match it 抽取文本中的数字 通过正则表达式匹配年份 [0-9]代表从0-9的所有数字，[a-z]代表从a到z的所有小写字母 12345678910import restrings = [&apos;War of 1812&apos;,&apos;There are 5280 feet to a mile&apos;,&apos;Happy New Year 2019!&apos;]#先声明year_strings为list变量，不然下面直接用会报错year_strings=[]for string in strings: #匹配含有1000~2999数字的字符串 #[0-9]&#123;3&#125;表示重复[0-9]三次，相当于[0-9][0-9][0-9] if re.search(&apos;[1-2][0-9]&#123;3&#125;&apos;,string): year_strings.append(string)print(year_strings) 输出结果： 1[&apos;War of 1812&apos;, &apos;Happy New Year 2019!&apos;] 抽取所有的年份 使用re模块的另一个方法findall()来返回匹配带正则表达式的那部分字符串 re.findall(&quot;[a-z]&quot;,&#39;abc1234&#39;)得到的结果是[&quot;a&quot;,&quot;b&quot;,&quot;c&quot;] 12345import reyears_string = '2016 was a good year,but 2017 will be better!'#抽取所有2000~2999的年份years = re.findall('[2][0-9]&#123;3&#125;',years_string)print(years) 输出结果: 1['2016', '2017'] Numpy使用详解Numpy(Numerical Python) 是高性能科学计算和数据分析的基础包，提供矩阵运算的功能。主要功能： ndarray 一个具有向量算术运算和复杂广播能力的多维数组对象 用于对数组数据进行快速运算的标准数学函数 用于读写磁盘数据的工具以及用于操作内存映射文件的工具 非常有用的线性代数，傅里叶变换和随机数操作 用于集成C/C++ 和 Fortran 代码的工具 也可以用作通用数据的高效多维容器，可以定义任意的数据类型。 广播 ： 当有两个维度不同的数组(array)运算的时候，可以用低维的数组复制成高维数组参与运算(Numpy运算时需要结构相同) 创建Numpy数组在Numpy中，最核心的数据结构是 ndarray ，代表的是多维数组。 借用线性代数的说法，一维数组通常称为向量(vector)，二维数组通常称为矩阵(matrix) 1234567import numpy as np#通过array可以将向量直接导入vector = np.array([1,2,3,4])#通过array也可以将矩阵导入matrix = np.array([[1,'Tim'],[2,'Joey'],[3,'Johnny'],[4,'Frank']])print(vector)print(matrix) 输出结果： 12345[1 2 3 4][['1' 'Tim'] ['2' 'Joey'] ['3' 'Johnny'] ['4' 'Frank']] 获取Numpy数组的维度 通过arange(n) 方法生成0到n-1的数组 通过reshape(row,column)自动架构一个多行多列的array对象 通过shape属性获取Numpy数组的维度 12345678import numpy as nparray = np.arange(12)print(array)#将array构造成三行4列的array对象matrix = array.reshape(3,4)print(matrix)#shape返回一个元组tuple，第一个代表行，第二个代表列print(matrix.shape) 输出结果： 12345[ 0 1 2 3 4 5 6 7 8 9 10 11][[ 0 1 2 3] [ 4 5 6 7] [ 8 9 10 11]](3, 4) 获取本地数据 通过genfromtxt()读取本地的数据集，delimiter参数定义数据由什么分割 12345import numpy as np#genfromtxt默认comments为#，即数据中由#标注的数据将被注释掉#由于我的数据文件中使用#开头作为表头，设定注释用字符为##nfl = np.genfromtxt(\"D:/Code/Python-NLP-Code/source/price.csv\",delimiter=\"\\t\",comments=\"##\")print(nfl) 输出结果： 12345678910111213141516[[ nan nan nan nan nan nan] [1.00e+00 2.10e+03 3.00e+00 1.00e+00 nan nan] [2.00e+00 3.20e+03 4.00e+00 2.00e+00 nan nan] [3.00e+00 2.20e+03 2.00e+00 2.00e+00 nan nan] [4.00e+00 1.90e+03 4.00e+00 1.00e+00 nan nan] [5.00e+00 3.40e+03 5.00e+00 1.00e+00 nan nan] [6.00e+00 6.70e+03 3.00e+00 1.00e+00 nan nan] [7.00e+00 1.20e+03 2.00e+00 1.00e+00 nan nan] [8.00e+00 3.50e+03 4.00e+00 1.00e+00 nan nan] [9.00e+00 6.50e+03 1.00e+00 2.00e+00 nan nan] [1.00e+01 9.80e+03 2.00e+00 1.00e+00 nan nan] [1.10e+01 7.80e+03 3.00e+00 1.00e+00 nan nan] [1.20e+01 6.70e+03 4.00e+00 3.00e+00 nan nan] [1.30e+01 5.50e+03 6.00e+00 3.00e+00 nan nan] [1.40e+01 6.60e+03 3.00e+00 1.00e+00 nan nan] [1.50e+01 3.34e+03 2.00e+00 2.00e+00 nan nan]] 自己手动编写的数据文件price.csv： 12345678910111213141516id price #bedroom #bathroom #isBrick local1 2100 3 1 t bos2 3200 4 2 r bos3 2200 2 2 t bos4 1900 4 1 t bos5 3400 5 1 t bos6 6700 3 1 t bos7 1200 2 1 r bos8 3500 4 1 r bos9 6500 1 2 r link10 9800 2 1 t link11 7800 3 1 r link12 6700 4 3 r link13 5500 6 3 t link14 6600 3 1 t new york15 3340 2 2 t new york 正确读取数据 genfromtxt函数默认其数据类型dtype为float，因此不是该种类型的数据会读出 nan(not a number) 数据类型转换出错 或 na(not available) 数值为空、不存在。 将dtype关键字设置为&#39;U75&#39;，即表示每个值都是75byte的unicode。 skip_header关键字可以设置为整数，该参数可以跳过文件开头的对应的行数，然后再执行任何其他操作 123456import numpy as np# genfromtxt默认comments为#，即数据中由#标注的数据将被注释掉# dtype设置为U75,即每个值都是75byte的Unicode。# skip_header设置为整数，意思是跳过文件开头的X行。nfl = np.genfromtxt(\"D:/Code/Python-NLP-Code/source/price.csv\",delimiter=\"\\t\",comments=\"##\",dtype='U75',skip_header=1)print(nfl) 输出结果： 12345678910111213141516[['id' 'price' '#bedroom' '#bathroom' '#isBrick' 'local'] #添加skip_header=1后本行消失 ['1' '2100' '3' '1' 't' 'bos'] ['2' '3200' '4' '2' 'r' 'bos'] ['3' '2200' '2' '2' 't' 'bos'] ['4' '1900' '4' '1' 't' 'bos'] ['5' '3400' '5' '1' 't' 'bos'] ['6' '6700' '3' '1' 't' 'bos'] ['7' '1200' '2' '1' 'r' 'bos'] ['8' '3500' '4' '1' 'r' 'bos'] ['9' '6500' '1' '2' 'r' 'link'] ['10' '9800' '2' '1' 't' 'link'] ['11' '7800' '3' '1' 'r' 'link'] ['12' '6700' '4' '3' 'r' 'link'] ['13' '5500' '6' '3' 't' 'link'] ['14' '6600' '3' '1' 't' 'new york'] ['15' '3340' '2' '2' 't' 'new york']] Numpy数组索引 Numpy支持list一样的定位操作 使用[x,y]来提取第x行，第y列的数据 123import numpy as npmatrix = np.array([[1,2,3],[20,30,40]])print(matrix[0,1]) 输出结果： 12 切片 numpy支持list一样的切片操作 1234567891011121314151617import numpy as npmatrix = np.array([ [5,10,15], [20,25,30], [35,40,45]])#选择所有行且列的索引是1的数据print(matrix[:,1])print()#选择所有行且列的索引是0和1的数据print(matrix[:,0:2])print()#选择行的索引是1和2且所有列的数据print(matrix[1:3,:])print()#选择行的所以是1和2且列的索引是0和1的数据print(matrix[1:3,0:2]) 输出结果： 1234567891011[10 25 40][[ 5 10] [20 25] [35 40]][[20 25 30] [35 40 45]][[20 25] [35 40]] 数组比较 numpy可以进行数组或矩阵的比较，比较之后会产生boolean值 允许使用条件符来拼接条件，&amp;代表且，|代表或 123456789import numpy as npmatrix = np.array([ [5,10,15], [20,25,30], [35,40,45]])#判断matrix矩阵中的每个值是否等于25m = (matrix == 25)print(m) 输出结果： 123[[False False False] [False True False] [False False False]] 较为复杂的一个例子： 12345678910111213import numpy as npmatrix = np.array([ [5,10,15], [20,25,30], [35,40,45]])#matrix[:,1]选取所有行切索引为1的列的数据，然后判断是否等于25second_column_25 = (matrix[:,1] == 25)print(second_column_25)#展示返回true值的那一行数据print(matrix[second_column_25,:])#更清晰的展示上面的作用，即选择True行的数据展示print(matrix[[True,True,False],:]) 输出结果： 123456[False True False][[20 25 30]][[ 5 10 15] [20 25 30]] 替代值 numpy可以运用布尔值来替换值 123456789101112131415161718import numpy as npvector = np.array([5,10,15,20])#在数组中利用#判断等于5或10，得到一个布尔值数组equal_to_ten_or_five = (vector == 10) | (vector == 5)#利用布尔值数组将值替换为50vector[equal_to_ten_or_five] = 50print(vector)#在矩阵中利用matrix = np.array([ [5,10,15], [20,25,30], [35,40,45]])#将第二列中为25的值替换为10second_column_25 = matrix[:,1] == 25matrix[second_column_25,1] = 10print(matrix) 输出结果： 12345[50 50 15 20][[ 5 10 15] [20 10 30] [35 40 45]] 利用替换处理空值 12345678910import numpy as npmatrix = np.array([ ['','10','15'], ['20','25','30'], ['35','40','']])#将第3列中的空值替换为'0'second_column_25 = (matrix[:,2] == '')matrix[second_column_25,2] = '0'print(matrix) 输出结果： 123[['' '10' '15'] ['20' '25' '30'] ['35' '40' '0']] 数据类型转换 Numpy ndarray 数据类型可以通过参数 dtype 设定 可以使用 astype 转换类型，调用时会返回一个新的数组，也就是原始数据的一份复制 123456import numpy as npvector = np.array([\"1\",\"2\",\"3\"])#从string转为float类#如果含非数字类型，会报错vector = vector.astype(float)print(vector) 输出结果： 1[1. 2. 3.] Numpy的统计计算方法 sum() 计算数组元素的和； mean() 计算数组元素的平均值 max() 计算数组元素的最大值 对矩阵计算结果为一个一维数组，需要指定行或列；注意数值类型必须是int或float 1234567891011121314import numpy as np#数组例子vector = np.array([5,10,15,20])print(vector.sum())#矩阵例子matrix = np.array([ [5,10,15], [20,10,30], [35,40,45]])#axis=1计算行的和print(matrix.sum(axis=1))#axis=0计算列的和print(matrix.sum(axis=0)) 输出结果： 1234550[ 30 60 120][60 60 90]","categories":[{"name":"Python自然语言处理实战","slug":"Python自然语言处理实战","permalink":"http://yoursite.com/categories/Python自然语言处理实战/"}],"tags":[{"name":"NLP","slug":"NLP","permalink":"http://yoursite.com/tags/NLP/"},{"name":"Python","slug":"Python","permalink":"http://yoursite.com/tags/Python/"},{"name":"Numpy","slug":"Numpy","permalink":"http://yoursite.com/tags/Numpy/"}]},{"title":"Python-NLP-Chapter1","slug":"Python-NLP-Chapter1","date":"2019-03-31T09:54:53.000Z","updated":"2019-04-02T08:38:07.075Z","comments":true,"path":"2019/03/31/Python-NLP-Chapter1/","link":"","permalink":"http://yoursite.com/2019/03/31/Python-NLP-Chapter1/","excerpt":"","text":"Chapter 1 NLP基础List NLP基础概念 NLP的发展与应用 NLP常用术语以及扩展介绍 NLP的概念NLP（Natural Language Processing ，自然语言处理），研究用计算机来 处理、理解以及运用人类语言 ，达到人与计算机之间进行有效通讯。 NLP的基本分类 自然语言理解 音系学 词态学 句法学 语义句法学 语用学 自然语言生成 自然语言文本 NLP的研究任务 机器翻译 计算机具备将一种语言翻译成另一种语言的能力 情感分析 计算机能够判断用户评论是否积极 智能问答 计算机能够正确回答输入的问题 文摘生成 计算机能够准确归纳、总结并产生文本摘要 文本分类 计算机能够菜鸡各种文章，进行主题分析，进而进行自动分类 舆论分析 计算机能够判断目前舆论的导向 知识图谱 知识点相互连接而成的语义网络 NLP相关知识构成基本术语 分词 segment 词是最小的能够独立活动的有意义的语言成分，中文分词问题比较重要 常用手段：基于字典的最长串匹配 问题：歧义分词 词性标注 part-of-speech tagging 词性一般指动词、名词、形容词等。 目的：表征词的一种隐藏状态，隐藏状态构成的转移就构成了状态转移序列 命名实体识别 NER Named Entity Recognition 从文本中识别具有特定类别的实体，通常是名词 句法分析 syntax parsing 往往是一种基于规则的专家系统 指代消解 anaphora resolution 中文中代词出现的频率很高，指代消解即是辨明代词所代表的意思 情感识别 emotion recognition 本质上是分类问题，一般分为正面、负面，或加上中性。 方法：词袋模型+分类器、词向量模型+RNN等 纠错 correction 自动纠错多应用在搜索技术及输入法中 方法：基于N-Gram进行纠错、通过字典树、有限状态机 问答系统 QA system 较为复杂，需要语音识别、合成，自然语言理解、知识图谱等多项技术配合 知识结构 句法语义分析 针对目标句子，进行各种句法分析，如分词、词性标记、命名实体识别、链接，句法分析、语义角色识别、多义词消歧等 关键词抽取 抽取目标文本中的主要信息，如从一条新闻中抽取。 涉及实体识别、时间抽取、因果关系抽取等 文本挖掘 对文本的聚类、分类、信息抽取、摘要、情感分析以及对挖掘信息和知识的可视化、交互式的呈现界面 机器翻译 可分为文本翻译、语音翻译、手语翻译、图形翻译等 信息检索 对大规模的文档进行索引。 问答系统 对自然语言查询语句进行语义分析，包括实体链接、关系识别，形成逻辑表达式，然后到知识库中查找可能的候选答案并根据排序机制找出最佳答案 对话系统 语料库(中文) 中文维基百科 搜狗新闻语料库 提供URL和正文信息 IMDB情感分析语料库 包括影片的众多信息、演员、片长、内容介绍、分级、评论等 探讨NLP的几个层面 词法分析 分词、词性标注 句法分析 语义分析 语义角色标注(semantic role labeling)","categories":[{"name":"Python自然语言处理实战","slug":"Python自然语言处理实战","permalink":"http://yoursite.com/categories/Python自然语言处理实战/"}],"tags":[{"name":"NLP","slug":"NLP","permalink":"http://yoursite.com/tags/NLP/"}]},{"title":"Python-NLP-Abstract","slug":"Python-NLP-Abstract","date":"2019-03-31T09:54:18.000Z","updated":"2019-04-02T08:38:00.491Z","comments":true,"path":"2019/03/31/Python-NLP-Abstract/","link":"","permalink":"http://yoursite.com/2019/03/31/Python-NLP-Abstract/","excerpt":"","text":"Abstract 内容简介 Part 1介绍NLP所需要了解的Python科学包、正则表达式、Solr检索 Section 1Section 2Section 11 Part2介绍NLP相关的各个知识点 Section3~5词法分析 层面的一些技术，是NLP技术的基础 Section6句法分析 技术 Section7常用的 向量化 方法，常用于各种NLP任务的输入 Section8情感分析 相关概念、场景以及一般做情感分析的流程 Section9机器学习 的一些基本概念，重点突出NLP常用的 分类算法 、聚类算法 Section10介绍NLP中常用的 深度学习算法","categories":[{"name":"Python自然语言处理实战","slug":"Python自然语言处理实战","permalink":"http://yoursite.com/categories/Python自然语言处理实战/"}],"tags":[{"name":"NLP","slug":"NLP","permalink":"http://yoursite.com/tags/NLP/"}]},{"title":"AHU HPC培训","slug":"AHU HPC培训","date":"2019-03-25T09:00:10.000Z","updated":"2019-04-02T08:39:03.935Z","comments":true,"path":"2019/03/25/AHU HPC培训/","link":"","permalink":"http://yoursite.com/2019/03/25/AHU HPC培训/","excerpt":"","text":"介绍 域名hpcc.ahu.edu.cn 配置情况 CPU 26 * gpu1 1 * gpu2 1 2卡 gpu8 3 8卡 Linux下操作命令 登录到结点 ssh [gpu01] 登陆后查看 top free 查看历史命令 .bash_history 环境变量 .bashrc .bash_profile 常用环境变量 设置可执行程序的查找路径 export PATH=/:$PATH e.g. export PATH=/Share/apps/matlab//R2016a/bin:$PATH # 直接在窗口输入时只在当前窗口有效 # 可以写入到 ~/.bashrc文件中 # 或写入到自定义.sh文件中，然后通过source加载 slurm作业管理器 sinfo查看当前各节点资源 timelimit 时间限制 State 状态 mix 有人用但是没有满 alloc 被占满 idle 空闲 squeue查看任务队列 -u 账号名 srun交互式提交 -p 节点名 -N 节点数 -n 核心数 -c 指定每个进程使用的cpu核数 -gres=gpu:1 指定使用GPU卡 ※必须 程序命令 sbatch后台提交作业 shell脚本 固定格式#!/bin/bash 参数同srun 默认输出 slurm-作业号.out文件 scancel取消作业 参数：作业号 -u 用户名 -n 作业名NAME scontrol show job查看正在运行的作业信息 参数：作业号 sacct查看历史任务信息 -u 用户名 salloc申请结点资源 Q&amp;A 安大集群软件 /Share/apps 常见问题 节点、核数不匹配 缺库 .so.x 提交AI作业 容器Singularity 主程序路径 /Share/apps/singularity/bin/singularity 镜像文件 /Share/imgs/ahu_ai.img GPU型号 V100,显存16G，共28张卡 实例 PyTorch脚本 #SBATCH –job-name=pytorch 作业名称 #SBATCH -N 1 节点数 #SBATCH –gres=gpu:1 GPU卡数 :star: #SBATCH -mem=20G 内存大小 #SBATCH -p GPU8 队列名（分区） 查看GPU使用情况 nvidia-smi Tensorflow使用 安装位置 /usr/local/lib/python3.6/dist-packages/tensorlfow 脚本 类似PyTorch 安装软件 ./install configure -&gt; make -&gt; make install cmake -&gt; ccmake -&gt; make –&gt; make install pip install 容器安装 没有root权限但是有安装常用软件的权限 :star:","categories":[{"name":"其他技术","slug":"其他技术","permalink":"http://yoursite.com/categories/其他技术/"}],"tags":[{"name":"linux","slug":"linux","permalink":"http://yoursite.com/tags/linux/"}]},{"title":"Hello World","slug":"hello-world","date":"2019-03-25T08:41:51.318Z","updated":"2019-03-25T08:41:51.318Z","comments":true,"path":"2019/03/25/hello-world/","link":"","permalink":"http://yoursite.com/2019/03/25/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[],"tags":[]}]}