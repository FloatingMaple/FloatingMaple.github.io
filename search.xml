<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>paper-ComplEx-NNE+AER</title>
      <link href="/2019/05/12/paper-ComplEx-NNE-AER/"/>
      <url>/2019/05/12/paper-ComplEx-NNE-AER/</url>
      
        <content type="html"><![CDATA[<h1 id="Improving-Knowledge-Graph-Embedding-Using-Simple-Constraints"><a href="#Improving-Knowledge-Graph-Embedding-Using-Simple-Constraints" class="headerlink" title="Improving Knowledge Graph Embedding Using Simple Constraints"></a>Improving Knowledge Graph Embedding Using Simple Constraints</h1><p>通过使用简单约束提升知识图谱嵌入</p><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><ul><li><p>Work: </p><p>工作内容：</p><ul><li>Embedding Knowledge graphs into continuous vector spaces.</li></ul><p>​    将知识图谱嵌入到连续向量空间中</p></li><li><p>Early works:</p><p>早期工作：</p><ul><li>Via simple models developed over KG triples</li></ul><p>​    通过在知识图谱中的三元组上开发的简单模型</p></li><li><p>Recent works：</p><p>最近工作：</p><ul><li>designing more complicated triple scoring models、incorporating extra information beyond triples</li></ul><p>​    设计更加复杂的元组得分模型、利用元组之外的其他信息</p></li><li><p>This paper：</p><p>该论文：</p><ul><li>using very simple constraints</li></ul><p>​    利用简单的约束</p><ul><li><em>non-negativity constraints</em> on <strong>entity representations</strong></li></ul><p>​    实体表示上：非负性约束        </p><p>​    help to learn compact and interpretable representations for entities</p><p>​    帮助学习紧凑的、可解释的实体表示 </p><ul><li><em>approximate entailment constraints</em> on <strong>relation representations</strong></li></ul><p>​    关系表示上：近似包含约束</p><p>​    further encode regularities of logical entailment between relations into their distributed  representations</p><p>​    将关系之间逻辑包含约束规则进一步编码到它们的分布表示中</p><ul><li><p>These constrains impose prior beliefs upon the structure of the embedding space,without negative impacts on efficiency or scalability.</p><p>这些约束将先验信念强加于嵌入空间的结构上，而不会对效率或可扩展性产生负面影响。</p></li><li><p>The constraints imposed indeed improve model interpretability, leading to a substantially increased structuring of the embedding space.</p><p>施加的约束确实提高了模型的可解释性，从而大大增加了嵌入空间的结构。</p></li></ul></li><li><p>Code and data</p><p>代码和数据</p><ul><li><a href="https://github.com/iieir-km/ComplEx-NNE_AER" target="_blank" rel="noopener">https://github.com/iieir-km/ComplEx-NNE_AER</a></li></ul></li></ul><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>Web-scale knowledge graphs (KGs):</p><p>网络规模的知识图谱:</p><p>​    Freebase 、 DBpedia、Google’s Knowledge Vault</p><p>Typical KG ：</p><p>典型的知识图谱：</p><p>​    a multirelational graph composed of entities as nodes and relations as different types of edges, where  each edge is represented as a triple of the form (head entity, relation, tail entity).</p><p>​    由作为节点的实体和作为不同类型的边的关系组成的多关系图，其中每条边都表示为三元组，形如(头实体、关系、尾实体)。</p><p>Knowledge graph embedding：</p><p>知识图谱嵌入：</p><p>​    embed components of a KG (i.e., entities and relations) into a continuous vector space,so as to simplify manipulation while preserving the inherent structure of the KG.</p><p>​    嵌入知识图谱的组件（实体、关系）到连续向量空间中，从而能够在保留知识图谱的固有结构的同时简化操作</p><p>Early Work：</p><p>早期工作：</p><p>​    via just simple models developed over KG triples e.g. TransE、HolE</p><p>​    通过在三元组上开发的简单模型</p><p>Recent Work：</p><p>最近的工作：</p><p>​    designing more complicated triple scoring models e.g. NTN 、incorporating extra information beyond KG triples</p><p>​    设计更复杂的三元组得分模型、使用三元组之外的额外信息</p><p>This paper：</p><p>本论文：</p><p>​    investigates the potential of using very simple constrains to improve the KG embedding task</p><p>​    研究了使用非常简单的约束改进KG嵌入任务的潜力</p><p>​    <strong>Two types of constraints：</strong></p><p>​    <strong>两种约束：</strong></p><ol><li><p>non-negativity constraints on entity representations</p><p>=&gt; learn compact representations for entities,which would naturally induce sparsity and interprtability</p></li><li><p>approximate entailment constraints over relation representations</p><p>=&gt; further encode regularities of logical entailment between relations into their distributed representations,which might be advantageous to downstream tasks like link prediction and relation extraction</p><p>实体表示上的非负性约束 =&gt; 学习实体的紧凑表示，这自然会导致稀疏性和可解释性</p></li></ol><ol><li>关系表示上的近似蕴涵约束 =&gt; 进一步将关系之间逻辑蕴涵的规则编码到它们的分布式表示中，这可能有利于后续的任务，如链接预测和关系提取</li></ol><p>​    These constraints impose prior beliefs upon the  structure of the embedding space,and will help us to learn more predictive embeddings,without significantly increasing the space or time complexity.</p><p>​    这些约束将先验信念强加于嵌入空间的结构，并将帮助我们学习更多的预测性嵌入，而不会显著增加空间或时间复杂性。</p><p>​    <strong>Difference</strong>：</p><p>​    与其他方法的区别：</p><ol><li><p>it imposes constraints directly on entity and relation representations without grounding, and can easily scale up to large KGs;</p><p>它直接对实体和关系表示施加约束，没有基础(工作)，并且可以很容易地扩展到大型KG</p></li><li><p>the constraints, i.e., non-negativity and approximate entailment derived automatically from statistical properties, are quite universal, requiring no manual effort and applicable to almost all KGs;</p><p>使用的约束，例如非负性和近似蕴含是由统计特性自动导出的，是相当普遍的，不需要人工操作，几乎适用于所有KG</p></li><li><p>it learns an individual representation for each entity, and can successfully make predictions between unpaired entities.</p><p>它学习每个实体的单独表示，并能够成功地在未配对的实体之间进行预测。</p></li></ol><h2 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h2><h3 id="Early-works"><a href="#Early-works" class="headerlink" title="Early works"></a>Early works</h3><p>Early Works on this topic devised very simple models on the basis of triples observed in a given KG:</p><p>早期工作：基于在给定KG中观察到的三元组设计非常简单的模型</p><p>​    TransE：takes relations as translating operations between head and tail entities</p><p>​    将关系作为头和尾实体之间的转换操作</p><p>​    RESCAL: models triples through bilinear opeartions over entity and relation representations</p><p>​    通过实体和关系表示上的双线性操作对三元组建模</p><h3 id="Later-attempts"><a href="#Later-attempts" class="headerlink" title="Later attempts"></a>Later attempts</h3><p>Later attempts in two groups:</p><p>后来的尝试多在两个方面：</p><ol><li><p>tried to design more complicated scoring models </p><p>尝试设计更加复杂的得分模型</p><p>e.g. TransE extensions 、 RESCAL extensions 、 (deep)neural network models</p></li><li><p>tried to integrate extra information beyond triples</p><p>尝试整合三元组以外的其他信息</p><p>e.g. entity types、relation paths、textual descriptions</p></li></ol><h3 id="In-this-paper"><a href="#In-this-paper" class="headerlink" title="In this paper"></a>In this paper</h3><p>​    using very simple constraints to improve KG embedding</p><p>​    利用简单约束提升KG嵌入表现</p><p>​    e.g. non-negativity constraints 、approximate entailment constraints 非负性约束、近似蕴含约束</p><p>​    We investigate the ability of non-negativity constraints to learn more accurate KG  embeddings with good interpretability </p><p>​    研究了非负性约束的能力，已学习更加准确的、具有良好解释性的KG嵌入</p><p>​    </p><h2 id="Our-Approach"><a href="#Our-Approach" class="headerlink" title="Our Approach"></a>Our Approach</h2><h3 id="A-Basic-Embedding-Model"><a href="#A-Basic-Embedding-Model" class="headerlink" title="A Basic Embedding Model"></a>A Basic Embedding Model</h3><p>​    ComplEx represents each entity/relation as a complex-valued vector in a same d dimensionality embedding space</p><p>​    ComplEx将每个实体和关系表示为同一个d维嵌入空间中的复值向量</p><p>​    Ecah <strong>x</strong> in this space consists of a real vector component Re(x) and an imaginary vector component IM(x)</p><p>​    该空间中的每个x都由一个实值向量部分和一个虚值向量部分组成 <strong>x</strong>=Re(<strong>x</strong>)+<em>i</em>IM(<strong>x</strong>)</p><p>​    Score: a multi-linear dot product </p><p>​    使用一个多重线性点积函数作为评分函数</p><script type="math/tex; mode=display">\phi(e_i,r_k,e_j)\overset{\Delta}{=}Re(<\vec{e_i},\vec{r_k},\bar{e_j}>) \\ \overset{\Delta}{=}Re(\sum_{l}{[\vec{e_i}}]_l{[\vec{r_k}}]_l{[\bar{e_j}}]_l)</script><p>​    其中$ \vec{e}、\vec{r} $ 是实体、关系对应的向量表示，$ \bar{e} $ 是 $\vec{e}$ 的共轭向量，$[·]_l$ 是一个向量的第l个条目(entry)，Re(·) 表示取复值的实部</p><h3 id="Non-negativity-of-Entity-Representations"><a href="#Non-negativity-of-Entity-Representations" class="headerlink" title="Non-negativity of Entity Representations"></a>Non-negativity of Entity Representations</h3><p>​    Entities have non-negative (and bounded) vectorial representations</p><p>​    实体有非负（且有限界的）向量表示</p><p>​    In fact，these distributed representations can be taken as feature vectors for entities，with latent semantics encoded in different dimensions</p><p>​    这些分散的表示能够作为实体的特征向量，拥有编码在不同维度的潜在语义</p><p>​    <strong>Previous approaches：</strong></p><p>​    no limit the range of such feature values，both positive and negative properties of an entity can be encoded in its representation</p><p>​    没有约束这些特征值的范围，意味着一个实体的正负属性都能编码到其表示中</p><p>​    <strong>Point：</strong></p><p>​    uneconomical to store all negative properties of an entity or a concept</p><p>​    存储一个实体或概念的所有负属性是浪费的</p><p>​    e.g.</p><figure class="hljs highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">在描述 “猫 cat”时，</span><br><span class="line">人们通常使用正属性，例如 猫是哺乳动物、猫吃鱼、猫有四条腿 ；</span><br><span class="line">但是几乎不会使用负属性，例如 猫不是交通工具、猫没有轮子、猫不能用来通信</span><br></pre></td></tr></table></figure><p>​    <strong>This paper:</strong></p><p>​    Impose non-negativity constraints on entity representations,by using only positive properties will be stored in these representations.</p><p>​    对实体表示施加非负性约束，即在实体表示中只存储正属性</p><p>​    To better compare different entities on the same scale ,we further require entities representations to stay within the hypercube of $[0,1]^d$ ,as approximately Boolean embeddings , i.e. </p><p>​    为了更好的在相同尺度上比较不同实体，要求实体表示在d维的[0,1]超立方内，近似布尔嵌入</p><script type="math/tex; mode=display">\vec{0} \leq Re(\vec{e}),Im(\vec{e}) \leq \vec{1} ,\forall e\in \mathcal{E}</script><p>​    0,1是d维向量，他们的所有项entry都是0或1，≥、≤、= denote the entry-wise comparisons throughout the paper whenever applicable.</p><p>​    non-negativity , in most cases , will further induce sparsity and interpretability</p><p>​    在大多数情况下，非负性将进一步导致稀疏性和可解释性</p><h3 id="Approximate-Entailment-for-Relations"><a href="#Approximate-Entailment-for-Relations" class="headerlink" title="Approximate Entailment for Relations"></a>Approximate Entailment for Relations</h3><p>​    approximate entailment constraints over relation representations </p><p>​    关系表示上的近似蕴含约束</p><p>​    <strong>approximate entailment：</strong></p><p>​    近似蕴含（近似继承）</p><p>​    an ordered pair of relations that the former <strong>approximately</strong> entails the latter</p><p>​    一个有序关系对，前者<strong>近似地</strong>引起后者</p><p>​    e.g. <strong>BornInCountry</strong> and <strong>Nationality</strong></p><p>​    一个人出生在一个国家，很可能，但是不是必然拥有该国家的国籍</p><p>​    each such relation pair is associated with a weight to indicate the confidence level of entailment</p><p>​    每个这样的关系对都对应着一个权重来表示其蕴含的信任等级，更大的权重表示更高的信任等级</p><p>​    denote： $ r_p \overset{\lambda}{\rightarrow} r_q $  ,$r_p、r_q$ relations，$\lambda$ confidence level，set of all  $\tau$</p><p>​    通过上式描述关系p、q之间的近似蕴含，其信任等级为λ ，所有近似蕴含的集合表示为T</p><p>​    This kind of entailment can be derived automatically from a KG by modern rule mining systems</p><p>​    这种蕴含（继承）能够由现在的规则挖掘系统自动导出</p><p>​    <strong><em>Strict entailment</em>：</strong></p><p>​    在深入探索近似蕴含之前，先探索严格蕴含（严格继承），例如 λ=+∞ ，严格蕴含 $r_p\rightarrow r_q$ 表示如果关系rp成立，那么关系rq一定成立，可以近似建模：要求</p><script type="math/tex; mode=display">\phi(e_i,r_p,e_j) \leq \phi(e_i,r_q,e_j), \forall e_i,e_j \in \mathcal{E}</script><p>​    即根据该嵌入模型，如果 $(e_i,r_p,e_j)$ 是一个正确事实，拥有较高的得分 $ \phi(e_i,r_p,e_j)$ ，那么拥有更高的得分的三元组 $(e_i,r_q,e_j)$ 也应该被预测为一个正确事实</p><p>​    Note that given  the non-negativity constraints defined by Eq. (2), a  sufficient condition for Eq. (3) to hold, is to further  impose：</p><p>​    注意，给定Eq.(1)所定义的非负约束条件，则进一步施加Eq.(2)成立的充分条件：</p><script type="math/tex; mode=display">Re(\vec{r_p}) \leq Re(\vec{r_q}), \quad Im(\vec{r_q})=Im(\vec{r_p})</script><p>​    <strong><em>Modeling of approximate entailment:</em></strong></p><script type="math/tex; mode=display">\begin{align}\lambda(Re(\vec{r_p})-Re(\vec{r_q})) \le \vec{\alpha}    \\\lambda(Im(\vec{r_p})-Im(\vec{r_q}))^2 \le \vec{\beta}\end{align}</script><p>​    α、β ≥ 0 是松弛变量，$(·)^2$ 表示一个项(向量中的)对操作（entry-wise）</p><p>​    Entailments with higher confidence levels show less tolerance for violating the constraints</p><p>​    蕴含的信任等级越高，表现出对违反该约束的更低容忍，当λ=+∞时，Eqs.4、5退化为Eq.3</p><p>​    The above analysis indicates that our approach can model entailment simply by imposing constraints over relation representations, without traversing all possible (ei,ej) entity pairs (i.e., grounding) .In addition, different confidence levels are encoded in the constraints, making our approach moderately tolerant of uncertainty.</p><p>​    上述分析表明，该方法能够通过对关系表示施加约束，而不遍历所有可能的实体对。同时，约束中编码了不同的置信级别，使得我们的方法对不确定性具有一定的容忍度。</p><h3 id="The-Overall-Model"><a href="#The-Overall-Model" class="headerlink" title="The Overall Model"></a>The Overall Model</h3><p>​    整体模型：</p><script type="math/tex; mode=display">\min _{\Theta,\{\boldsymbol{\alpha}, \boldsymbol{\beta}\}} \sum_{\mathcal{D}+\cup \mathcal{D}^{-}} \log \left(1+\exp \left(-y_{i j k} \phi\left(e_{i}, r_{k}, e_{j}\right)\right)\right)+\mu \sum_{\mathcal{T}} \mathbf{1}^{\top}(\boldsymbol{\alpha}+\boldsymbol{\beta})+\eta\|\Theta\|_{2}^{2}\\\begin{array}{l}{\text { s.t. } \lambda\left(\operatorname{Re}\left(\mathbf{r}_{p}\right)-\operatorname{Re}\left(\mathbf{r}_{q}\right)\right) \leq \boldsymbol{\alpha}} \\ {\lambda\left(\operatorname{Im}\left(\mathbf{r}_{p}\right)-\operatorname{Im}\left(\mathbf{r}_{q}\right)\right)^{2} \leq \boldsymbol{\beta}} \\ {\boldsymbol{\alpha}, \boldsymbol{\beta} \geq \mathbf{0}, \quad \forall r_{p} \stackrel{\lambda}{\rightarrow} r_{q} \in \mathcal{T}} \\ {\mathbf{0} \leq \operatorname{Re}(\mathbf{e}), \operatorname{Im}(\mathbf{e}) \leq \mathbf{1}, \quad \forall e \in \mathcal{E}}\end{array}</script><p>​        其中，$ \Theta \triangleq\{\mathbf{e} : e \in \mathcal{E}\} \cup\{\mathbf{r} : r \in \mathcal{R}\} $ 是所有实体和关系表示的集合，</p><p>​                    $ \mathcal{D}^+ $ $\mathcal{D^-}$ 分别是训练三元组中的正例和负例集合，</p><p>​                    $y_{ijk}=\pm 1$ 是三元组$\left(e_{i}, r_{k}, e_{j}\right)$ 为正或负的标签。</p><p>​    在该式中，第一部分是一个典型的对率损失函数（logistic loss），令三元组的分数接近它们的标签（which enforces triples to have scores close to their labels）；第二部分是近似蕴含约束中松弛变量的和（the sum of slack variables in the approximate entailment constraints），补偿系数为 $\mu \geq 0$  ；最后一部分是L2正则项以防止过拟合</p><p>​    第二部分的 motivation 动机：虽然我们允许这些约束中的松弛，但我们希望总的松弛量较小，以便更好地满足约束</p><p>​    为了解决这一优化问题，将近似蕴涵约束(以及相应的松弛变量)转换为惩罚项并添加到目标函数中，而非负约束保持不变，则上式可以写成：</p><script type="math/tex; mode=display">\begin{aligned} \min _{\Theta} & \sum_{\mathcal{D}^{+} \cup \mathcal{D}^{-}} \log \left(1+\exp \left(-y_{i j k} \phi\left(e_{i}, r_{k}, e_{j}\right)\right)\right) \\ &+\mu \sum_{\mathcal{T}} \lambda \mathbf{1}^{\top}\left[\operatorname{Re}\left(\mathbf{r}_{p}\right)-\operatorname{Re}\left(\mathbf{r}_{q}\right)\right]_{+} \\ &+\mu \sum_{\mathcal{T}} \lambda \mathbf{1}^{\top}\left(\operatorname{Im}\left(\mathbf{r}_{p}\right)-\operatorname{Im}\left(\mathbf{r}_{q}\right)\right)^{2}+\eta\|\Theta\|_{2}^{2} \end{aligned}\\s.t.  0 \leq \operatorname{Re}(\mathbf{e}), \operatorname{Im}(\mathbf{e}) \leq \mathbf{1}, \quad \forall e \in \mathcal{E}</script><p>​    其中 $ [\mathbf{x}]_{+}=\max (\mathbf{0}, \mathbf{x}) $  </p><p>​    使用mini-batch的SGD方法优化，使用AdaGrad调整学习率。</p><h2 id="Experiments-and-Results"><a href="#Experiments-and-Results" class="headerlink" title="Experiments and Results"></a>Experiments and Results</h2><h3 id="Datasets"><a href="#Datasets" class="headerlink" title="Datasets"></a>Datasets</h3><p><img src="/2019/05/12/paper-ComplEx-NNE-AER/1556852029458.png" alt="1556852029458"></p><p>​    使用AMIE+ 自动提取近似蕴含约束，考虑信任度高于0.8的</p><p><img src="/2019/05/12/paper-ComplEx-NNE-AER/1556851994080.png" alt="1556851994080"></p><h3 id="Link-Prediction"><a href="#Link-Prediction" class="headerlink" title="Link Prediction"></a>Link Prediction</h3><p><strong>链接预测</strong></p><p>​    <strong>task:</strong></p><p>​        predict a triple with one e missisng</p><p>​        预测有一个实体丢失的三元组 </p><p>​    <strong>Evaluation Protocol：评价方案：</strong></p><p>​        MRR mean reciprocal rank 平均倒序排名</p><p>​        HITS@N </p><p>​    <strong>Comparison Setting： 对照设置：</strong></p><ul><li><p>仅仅利用三元组而不利用附加信息    e.g. TransE、DistMult、HolE、<strong>ComplEx</strong>、ANALOGY</p></li><li><p>集成了逻辑背景知识的对ComplEx的扩展    e.g. RUGE 、ComplExR</p></li><li><p>在WN18和FB15K上的其他方法    e.g. R-GCN、ConvE、Single DistMult</p><p><strong>Our Approach：</strong></p></li></ul><ul><li><p>ComplEx-NNE：</p><p>Only the Non-Negativity constraints on Entity representations</p><p>仅仅使用实体表示的非负性约束，即 μ=0</p></li><li><p>ComplEx-NNE+AER：</p><p>further imposes the Approximate Entailment constraints over Relatio representations</p><p>在上面的基础上添加了关系的近似蕴含约束 ，即 μ&gt;0 </p><p><strong>Implementation Details: 实现细节：</strong></p><p><strong>Experimental Results：实验结果：</strong></p><p>​    <img src="/2019/05/12/paper-ComplEx-NNE-AER/1556889205329.png" alt="1556889205329"></p><p><img src="/2019/05/12/paper-ComplEx-NNE-AER/1556889226631.png" alt="1556889226631"></p></li></ul><h3 id="Analysis-on-Entity-Representations"><a href="#Analysis-on-Entity-Representations" class="headerlink" title="Analysis on Entity Representations"></a>Analysis on Entity Representations</h3><p><strong>实体表示分析</strong></p><p>​    Visualization of real components of entity representations learned</p><p>​    对学习到的实体表示的可视化结果表明，ComplEx-NNE+AER 能够使学到的向量更加稀疏，同类实体的表示也相对更加集中</p><p><img src="/2019/05/12/paper-ComplEx-NNE-AER/1557151470381.png" alt="1557151470381"></p><p>​    Values range from 0 (white) via 0.5 (orange) to 1 (black). </p><p>​    Semantic purity 语义纯净度</p><p>​    语义纯净度是指对于一个维度，该维度取值较大的实体都应该尽可能的属于同一类别，我们用熵来衡量语义纯    净度，熵越低，语义纯净度越高。</p><p><img src="/2019/05/12/paper-ComplEx-NNE-AER/1557151582038.png" alt="1557151582038"></p><p>​    Average entropy over all dimensions of real components of entity representations learned </p><p>​    不同K值下学习到的实体表示的平均熵</p><h3 id="Analysis-on-Relation-Representations"><a href="#Analysis-on-Relation-Representations" class="headerlink" title="Analysis on Relation Representations"></a>Analysis on Relation Representations</h3><p><strong>关系表示分析</strong></p><p>​    group relation pairs involved in the DB100K entailment constraints into 3 classes</p><p>​    <strong>equivalence</strong>, <strong>inversion</strong>, and <strong>others</strong>.</p><p>​    从每个类中选择两对关系，并将这些关系表示形象化，对于每个关系随机从它的实分量和虚分量中选取5个维</p><p>​    度。通过施加近似蕴含约束，这些关系表示能够更好的编码逻辑规则</p><pre><code>- 第一类（等价 equivalence）的关系对倾向于相同的表示  $\mathbf{r}_{p} \approx \mathbf{r}_{q}$ - 第二类（相反 inversion）的关系对 复共轭表示 $\mathbf{r}_{p} \approx \overline{\mathbf{r}}_{q}$- 第三类（其他 others）$ \operatorname{Re}\left(\mathbf{r}_{p}\right) \leq \operatorname{Re}\left(\mathbf{r}_{q}\right) $   $ \operatorname{Im}\left(\mathbf{r}_{p}\right) \approx \operatorname{Im}\left(\mathbf{r}_{q}\right) $</code></pre><p><img src="/2019/05/12/paper-ComplEx-NNE-AER/1557217726241.png" alt="1557217726241"></p><p>​    </p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>Using <strong>very simple constraints</strong> to improve KG embedding</p><p>Studied：</p><ul><li><p>non-negativity constraints to learn compact,interpretable entity representations</p><p>非负性约束，学习紧凑的、可解释的实体表示</p></li><li><p>approximate entailment constraints to further encode logical regularities into relation representations</p><p>近似蕴含约束，将逻辑规则进一步编码的关系表示</p></li></ul><p>Impose <strong>prior beliefs</strong> upon the structure of the embedding space </p><p>将<strong>先验信念</strong>强加于嵌入空间的结构上，不会显著增加空间或时间复杂度。</p>]]></content>
      
      
      <categories>
          
          <category> 论文阅读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> KG embedding </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2019网络行为分析研讨会</title>
      <link href="/2019/04/20/2019%E7%BD%91%E7%BB%9C%E8%A1%8C%E4%B8%BA%E5%88%86%E6%9E%90%E7%A0%94%E8%AE%A8%E4%BC%9A/"/>
      <url>/2019/04/20/2019%E7%BD%91%E7%BB%9C%E8%A1%8C%E4%B8%BA%E5%88%86%E6%9E%90%E7%A0%94%E8%AE%A8%E4%BC%9A/</url>
      
        <content type="html"><![CDATA[<h1 id="2019网络行为分析研讨会-合肥"><a href="#2019网络行为分析研讨会-合肥" class="headerlink" title="2019网络行为分析研讨会(合肥)"></a>2019网络行为分析研讨会(合肥)</h1><h2 id="网络表示学习-宋国杰-北大"><a href="#网络表示学习-宋国杰-北大" class="headerlink" title="网络表示学习-宋国杰-北大"></a>网络表示学习-宋国杰-北大</h2><p>​    ML = Representation + Objective + Optimization</p><ul><li><p>Representation Learning:</p><ul><li><p>linear:</p><p>PCA、LDA</p></li><li><p>non-linear</p><p>LLE、Isomap</p></li></ul></li><li><p>feature extract</p><ul><li><p>Traditional</p><p>stand-by-stand</p></li><li><p>Deep</p><p>End-to-End 、 ICLR</p></li></ul></li><li><p>Network Representation Learning</p><p>general language -&gt; describing、modeling complex system</p><ul><li>Hard:<ul><li>complexity (literative)</li><li>panallelizability(couple)</li><li>?</li></ul></li></ul><p>network space -&gt; vector space</p></li><li><p>Method </p><ul><li><p>shallow model</p><ul><li><p>Adjacency based Similarity</p><p>=&gt;    Multi-hop Similarity </p></li><li><p>Random-walk Embedding</p><p>=&gt;    Negative Sampling</p><p>=&gt;    node2vec:Binsed Walks</p><p>​    local/global (深度/广度优先)</p></li><li><p>LINE</p></li></ul></li><li><p>deep model</p><ul><li>Graph Neural Network (GNN)</li><li>GCNs</li><li>GraphSAGE</li><li>Gated GNNs</li></ul></li></ul></li><li><p>Works</p><ul><li><p>Hierarchical Network Embedding</p><p>=&gt;    Subspace Embedding Model</p><p>=&gt;    SepNE : Bringing Separability to Network Embedding</p><p>​    Graph Partitioning、landwork selecting</p></li><li><p>Scalable Network Embedding</p><p>=&gt;    Memory Adaptive Network Embedding</p><p>​    图划分、对部分表示的结果进行保存，下次计算时同步更新</p></li><li><p>Tag Embedding</p><p>Tag2Gauss</p></li><li><p>Domain Adaptation Network Embedding(DANE)</p><p>两套数据通过一套参数嵌入，对齐分布后使用GAN网络</p></li></ul></li></ul><h2 id="异质网络表示学习-石川-北邮"><a href="#异质网络表示学习-石川-北邮" class="headerlink" title="异质网络表示学习-石川-北邮"></a>异质网络表示学习-石川-北邮</h2><ul><li>Basic Concept</li></ul><p>​    Network Schema、Meta Path(元路径)</p><ul><li>Challenge</li></ul><p>​    complex、main semantic</p><ul><li><p>Work</p><ul><li><p>Shallow model</p><ul><li><p>HERec Random Walk based</p><p>=&gt;    HIN Embedding for Recommondation</p></li><li><p>meta-path based content for Recommendation MCRec</p></li><li><p>Relational-Structure aware: 关系结构</p><p>隶属关系 -&gt; 欧氏距离</p><p>交互关系 -&gt; Trans方法</p></li><li><p>Hyperbolize space 双曲空间</p><ul><li>power-low分布</li><li>hierarchy特性</li></ul></li></ul></li><li><p>Deep model</p><ul><li><p>Deep Neural Network based embedding ： NeuACF</p><p>meta-path -&gt; extract feature of different aspect</p><p>Neural network based Aspect-level Collaborative Filtering (NeuACF)</p></li><li><p>Attention based</p><p>GNN</p><p>Heterogeneous Graph Attention Network (HAN)</p></li></ul></li></ul></li></ul><h2 id="从网络到文本分析-胡琳梅-北邮"><a href="#从网络到文本分析-胡琳梅-北邮" class="headerlink" title="从网络到文本分析-胡琳梅-北邮"></a>从网络到文本分析-胡琳梅-北邮</h2><p>​    已有：BOW、次序列</p><p>​    问题：语义关系、融合外部数据(e.g. 关系数据)</p><h3 id="基于异质图神经网络的短文本分类"><a href="#基于异质图神经网络的短文本分类" class="headerlink" title="基于异质图神经网络的短文本分类"></a>基于异质图神经网络的短文本分类</h3><p>​    挑战：</p><ul><li><p>Semantically Sparseness</p></li><li><p>Ambiguity</p></li><li><p>limit labeled data</p><p>解决：</p></li><li><p>HIN、KGs</p></li><li><p>？</p></li></ul><h3 id="基于图神经网络的实体链接"><a href="#基于图神经网络的实体链接" class="headerlink" title="基于图神经网络的实体链接"></a>基于图神经网络的实体链接</h3><p>​    消歧：</p><ul><li><p>局部信息 - mention</p></li><li><p>全局信息 - 文档层面的实体连贯性</p><p>方法：</p><p>​    增强全局信息</p><p>​    文本当中Entity-Word -&gt; 异质信息网络 -&gt; 卷积 -&gt; CRF</p></li></ul><h3 id="基于图卷积神经网络的知识图谱表示学习"><a href="#基于图卷积神经网络的知识图谱表示学习" class="headerlink" title="基于图卷积神经网络的知识图谱表示学习"></a>基于图卷积神经网络的知识图谱表示学习</h3><ul><li>GCN for entity-word relation</li><li>TransE for KG relations</li><li>GKRL</li></ul><h2 id="Graph-Mining：from-Structure-to-Knowledge-周川-中科院"><a href="#Graph-Mining：from-Structure-to-Knowledge-周川-中科院" class="headerlink" title="Graph Mining：from Structure to Knowledge-周川-中科院"></a>Graph Mining：from Structure to Knowledge-周川-中科院</h2><p>Application：</p><p>​    Node Classification、Node Importance、Community Detection、Link Prediction</p><p>Topic(Work):</p><ul><li><p>Social network Analysis</p><ul><li>?</li><li>subgraph</li><li>network coarsening</li></ul></li><li><p>Social Recommendation System <strong>SoRec</strong> </p></li><li><p>Graph Neural Network Fraud Detection(欺诈检测)</p><ul><li><p>How to detect：</p><p>Interaction Between Users and Items：Bipartite graph</p><p>​    Users -&gt; Source nodes</p><p>​    Items -&gt; Sink nodes</p><p>​    User behaviors -&gt; Edges</p><p>Fraudulent Behaviors</p><p>​    Lockstep feature</p><p>Problems Transformation</p><p>​    Fraud detection</p><p>​    Find dense blocks in bipartite graph</p></li><li><p>Traditional：</p><p>Dense Block Detection Methods</p><p>​    Attribute-based Fraud Detection Methods</p><p>​        Not adversarially robust</p><p>​    Camouflage Actions - Cases</p><p><strong>topology structure</strong></p><p>Structural Information Based Fraud Detection Methods</p><p>​    Limitations、Shallow、Not robust</p></li><li><p>Work </p><ul><li><p>Motivation</p><p>Make deep structure learning of the bipartite graph for fraud detection</p></li><li><p>Idea</p><ul><li>Deep network embedded fraud Detection</li><li>No attribute information，only topology structure</li></ul></li><li><p>Solution - DeepFD Model</p><ul><li><p>Deep structure learning framework (Two components)</p><p>learn vector representation in latent space</p><ul><li><p>Global graph structure information </p><p>Deep auto-encoder</p></li><li><p>User behavior preservation</p><p>Fraudsters - similar</p><p>Normal users - independent</p><p>Based on similarity metric</p></li></ul></li><li><p>Detect dense blocks</p><p>Classical DBSCAN algorithm</p></li></ul></li></ul></li></ul></li></ul><h2 id="行为大数据分析与决策-熊赟-复旦"><a href="#行为大数据分析与决策-熊赟-复旦" class="headerlink" title="行为大数据分析与决策-熊赟-复旦"></a>行为大数据分析与决策-熊赟-复旦</h2><h2 id="基于深度学习的推荐算法研究-庄福振-中科院"><a href="#基于深度学习的推荐算法研究-庄福振-中科院" class="headerlink" title="基于深度学习的推荐算法研究-庄福振-中科院"></a>基于深度学习的推荐算法研究-庄福振-中科院</h2>]]></content>
      
      
      <categories>
          
          <category> 会议报告 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> representation learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Paper-TextTruth</title>
      <link href="/2019/04/15/Paper-TextTruth/"/>
      <url>/2019/04/15/Paper-TextTruth/</url>
      
        <content type="html"><![CDATA[<h1 id="TextTruth"><a href="#TextTruth" class="headerlink" title="TextTruth:"></a>TextTruth:</h1><p><strong>An Unsupervised Approach to Discover Trustworthy Information from Multi-Sourced Text Data</strong></p><p>一种无监督方法，从多源文本数据中发现可信信息</p><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p><strong>Truth discovery   真实信息发现/真值发现</strong></p><p><strong>含义：</strong></p><p>​    能够在不受任何监督的情况下从嘈杂的多源数据中提取可信的信息。</p><p><strong>问题：</strong></p><p>​    现有方法多针对结构化数据；</p><p>​    文本数据有其独特的特点，无法满足从原始文本数据中提取可信信息的强烈需求</p><p><strong>主要挑战：</strong></p><ul><li>文本答案的多因素属性(答案可能包含多个关键要素)</li><li>单词用法的多样性（不同的单词可能有相同的语义）</li></ul><p><strong>本文方法：</strong></p><ul><li>将从特定问题的答案中提取的关键词组合成多个可解释的因素；</li><li>从而推断出答案因素和答案提供者的可信度</li><li>之后，每个问题的答案都可以根据因素的可信度进行排序</li><li>非监督方法，能够应用于涉及文本数据的各种应用场景</li><li>能够准确选择可信答案，即使答案涉及多因素</li></ul><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p><strong>海量数据在线平台：</strong></p><p>​    <em>Amazon Mechanical Turk ； Stack Exchange ； Yahoo Answers</em></p><p><strong>问题：</strong></p><p>​    此类多源数据通常由非专家在线用户提供，因此数据中可能存在错误甚至冲突</p><p><strong>两个基本原则：</strong>（真值发现的）</p><ul><li>如果用户提供了很多值得信赖的信息或真实答案，其可靠性就很高</li><li>如果一个答案得到了很多可靠用户的支持，该答案更可能是正确的</li></ul><p><strong>文本数据的两个特性：</strong></p><ul><li>答案可能是多要素的，给定的文本答案很难覆盖所有要素</li><li>单词用法的多样性，用户可能通过不同的关键词传达非常相似的意思</li></ul><p><img src="/2019/04/15/Paper-TextTruth/1555292296700.png" alt="Figure 1"></p><p>​    左图：用户、答案、问题之间的关系</p><p>​    中图：例证展示关键词、答案要素【特性2】</p><p>​    右图：用户的答案中包含的要素【特性1】</p><p><strong>TextTruth：</strong></p><p>​    将每个答案中的关键词作为输入，输出其候选答案可信度的排名</p><ol><li>步骤：<ol><li>首先将文本答案中的关键词转换为经过训练的可计算向量表示（细粒度）    </li><li>通过对语义相似的关键词进行聚类，对答案的多样性进行建模（从每个答案要素的角度分析可信性）</li></ol></li><li>优点：<ol><li>通过对每个答案要素的可信度进行评估，可以处理文本答案部分正确现象</li><li>通过将答案关键词建模为向量表示形式，使得答案中的要素可计算，从而解决文本数据中用词多样性问题</li></ol></li></ol><h2 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h2><h3 id="Truth-Discovery"><a href="#Truth-Discovery" class="headerlink" title="Truth Discovery"></a>Truth Discovery</h3><ul><li><p>概念</p><p>从相互冲突的多源数据中识别出可信信息</p></li><li><p>处理场景</p><ul><li>不同数据类型</li><li>数据源依赖关系</li><li>细粒度数据源可靠性</li><li>实体/对象依赖关系</li><li>长尾数据</li></ul></li></ul><h3 id="Community-Question-Answering"><a href="#Community-Question-Answering" class="headerlink" title="Community Question Answering"></a>Community Question Answering</h3><ul><li><p>概念</p><p>CQA，社区问答系统</p></li><li><p>现有工作分类</p><ul><li>从众包答案中提取特征，将答案质量评估任务转化为分类或排序问题<ul><li>通常需要高质量训练集和各种有用的特性来训练模型，实际应用较为困难</li></ul></li><li>转化为专家寻找问题（expert finding problem），基于答案提供者来推断答案的质量<ul><li>需要外部信息， 例如询问者-回答者交互、投票信息等</li></ul></li></ul></li></ul><h3 id="Answer-Selection"><a href="#Answer-Selection" class="headerlink" title="Answer Selection"></a>Answer Selection</h3><ul><li><p>概念</p><p>答案选择，从一组候选句子中选择最合适的答案</p></li><li><p>不同方法</p><ul><li>基于词汇特征</li><li>基于神经网络的模型表示一个句子在向量空间中的意义，并借此比问题和候选答案</li><li>引入注意力机制，增强句子表征学习</li></ul></li></ul><h2 id="Problem-Formulation"><a href="#Problem-Formulation" class="headerlink" title="Problem Formulation"></a>Problem Formulation</h2><p><strong>术语：</strong></p><ul><li><p>Question 问题</p><p>一个问题 <strong><em>q</em></strong> 包含 <strong><em>N<sub>q</sub></em></strong> 个单词，并且能够被用户回答 </p></li><li><p>Answer 回答</p><p>用户 <strong><em>u</em></strong> 对于问题 <strong><em>q</em></strong> 给出的一个回答表示为 <strong><em>a<sub>qu</sub></em></strong></p></li><li><p>Answer Keyword 答案关键词</p><p>答案关键词是答案中的领域转由内容单词/短语</p><p>用户 <strong><em>u</em></strong> 对问题 <strong><em>q</em></strong> 给出的答案中的第<strong>m</strong>个答案关键词表示为 <strong><em>x<sub>qum</sub></em></strong></p></li><li><p>Answer Factor 答案要素</p><p>答案要素是答案的关键点，用答案关键词的簇表示</p><p>问题 <strong><em>q</em></strong> 的答案中第 <strong>k</strong> 个答案要素表示为 <strong><em>c<sub>qk</sub></em></strong></p></li><li><p>Problem Definition 问题定义</p><p>对每个问题，不同用户可能提供不同答案；答案可能由包含多个要素的复合句组成；答案可能部分正确</p><p>用户集 <script type="math/tex">\{u\}_1^U</script>      问题集 <script type="math/tex">\{q\}_1^Q</script>     答案集 <script type="math/tex">\{ a_{qu} \}_{q,u=1,1}^{Q,U}</script>     <strong>U</strong>  用户数量    <strong>Q</strong>  问题数量    </p><p>目的：提取出每个问题的<strong>高可信度答案</strong>和答案中的<strong>高可信度关键要素</strong></p></li></ul><h2 id="Methodology"><a href="#Methodology" class="headerlink" title="Methodology"></a>Methodology</h2><h3 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h3><ul><li><p><strong>问题</strong></p><ul><li>在将真值发现方法应用于复杂自然语言问题寻找可信答案时，应该考虑到答案之间的语义相关性，从而准确估计用户的可信度。但是学习整个答案的准确表示向量很困难，特别是上下文语料库不丰富时</li><li>自然语言复杂性，答案的意义过于复杂，无法用一个向量表示</li></ul></li><li><p><strong>处理方式</strong></p><ul><li>使用更加细粒度的语义单元（semantic units），例如答案要素（answer factors）来确定答案的可信度</li></ul></li><li><p><strong>处理步骤</strong></p><ul><li><p>对每个问题，首先提取每个答案中的关键词，并学习其向量表示</p></li><li><p>然后将这些词/短语级关键词聚类到语义聚类中，即要素(factors)</p><p>这些要素代表了问题答案中所有可能的关键点，可以用来确定答案的可信度；</p><p>对于每个簇中的关键字，由于他们具有非常相似的语义含义，他们的可信度应该几乎相同。</p></li><li><p>用户可能有不同的可信度，能够通过他们提供的答案反映出来</p></li><li><p>提出一种两步方法估计每个答案的可信度</p><ul><li>指定一个考虑用户可靠性的概率模型来建模关键词的生成<ul><li>首先生成答案要素和其语义参数的混合</li><li>然后生成双重用户可信变量，对某个用户提供的答案要素的准确性和全面性进行建模</li><li>最后根据语义、答案要素的可信度、提供答案的用户的可信度来选择答案要素；并通过VMF分布生成关键词嵌入向量</li></ul></li></ul></li></ul></li><li><p><strong>优点</strong></p><ul><li>答案要素和用户可信度的设计考虑了答案的多因素特性</li><li>关键词嵌入向量的生成也捕捉到了词汇用法的多样性</li><li>因此能够捕获文本数据的独特特征</li></ul></li></ul><h3 id="Generative-Model"><a href="#Generative-Model" class="headerlink" title="Generative Model"></a>Generative Model</h3><p>​    建立一个概率模型共同学习每个问题的<strong>答案要素</strong>和每个答案要素的<strong>真值标签（truth labels）</strong></p><p>​    对一个答案 $a_{qu}$ ，提取领域特定的答案关键词（keywords），并得到它们的归一化（normalized）向量表示</p><p>​    所有向量表示的集合记作  $\{v_{qum}\}$  ，也作为概率模型的观察（observation）</p><p><img src="/2019/04/15/Paper-TextTruth/1555298663041.png" alt="Figure 2"></p><p>​    提出模型的盘子表示法(Plate notation)；白色圆圈表示潜在变量，灰色圆圈表示观察，其他表示超参数</p><hr><p><strong>模型组成：</strong></p><h4 id="Answer-Factor-Modeling"><a href="#Answer-Factor-Modeling" class="headerlink" title="Answer Factor Modeling"></a>Answer Factor Modeling</h4><p>答案要素建模</p><p>​    模型首先根据狄利克雷分布(Dirichlet distribution)生成要素的混合。</p><p>​    形式上，混合分布 $\pi_q$ 生成：(服从概率分布)</p><script type="math/tex; mode=display">\pi \sim Dirichlet(\beta)</script><p>​    其中，$\beta$ 是 $K_q$ 维向量，$K_q$ 表示问题 <strong><em>q</em></strong> 的答案中所含的要素个数</p><p>​    对于问题 <strong><em>q</em></strong> 下的第 <strong>k</strong> 个答案要素，通过一个二元真值标签  $t_{qk}$ 对其可信度进行建模。</p><p>​    模型首先生成先验事实概率 $ \gamma _{qk}$ ，它决定了每个要素为真值的先验分布，服从超参数为 $\alpha_1^{(a)}$ 和 $\alpha _0^{(a)}$ 的Beta分布</p><script type="math/tex; mode=display">\gamma_{qk} \sim Beta(\alpha_1^{(a)},\alpha_0^{(a)})</script><p>​    然后真值标签（truth label） $t_{qk} $ 由参数为 $\gamma_{qk} $ 的伯努利分布（Bernoulli distribution）生成</p><script type="math/tex; mode=display">t_{qk} \sim Bernoulli(\gamma_{qk})</script><p>​    最后，为了对每个答案要素的语义特征进行建模，通过其共轭先验分布<script type="math/tex">\Phi (\mu_{qk},\kappa_{qk};m_0,R_0,c)</script><br>   定义vMF分布的质心参数（centroid parameter） <script type="math/tex">\mu_{qk}</script> 、集中参数（concentrate parameter）<script type="math/tex">\kappa_{qk}</script> 。</p><script type="math/tex; mode=display">\mu_{qk},\kappa_{qk} \sim \Phi (\mu_{qk},\kappa_{qk};m_0,R_0,c)</script><p>​    其中，<script type="math/tex">\Phi (\mu_{qk},\kappa_{qk};m_0,R_0,c)</script> 定义：</p><p>​                            <script type="math/tex">\Phi (\mu_{qk},\kappa_{qk};m_0,R_0,c) \propto \{C_D(\kappa_{qk})\}^cexp(\kappa_{kq}R_0m_0^T\mu_{qk})</script> </p><p>​    <script type="math/tex">C_D(\kappa) = \frac{\kappa^{D/2-1}}{I_{D/2-1(\kappa)}}</script>    ;   <script type="math/tex">I_{D/2-1}(·)</script> 是第一类修正贝塞尔函数（modified Bessel function of the first kind）</p><h4 id="User-Reliability-Modeling"><a href="#User-Reliability-Modeling" class="headerlink" title="User Reliability Modeling"></a>User Reliability Modeling</h4><p>用户可信度建模</p><p>​    <strong>每个用户的可信度是由其提供的答案推断出来的。</strong></p><p>​    由于用户提供答案的策略不同（有些人可能只提供很有信心的答案，一些人的答案中可能含有不同可信度的    答案要素），需要使用二重评分（two-fold score）来为用户可信度建模</p><p>​    <strong>假设</strong>我们事先知道所有的答案要素（answer factors）和它们的真值标签（truth labels），对所有问题和答    案：</p><p>​        $TP_u $     用户 <strong><em>u</em></strong> 的答案所覆盖的可信要素     $FP_u $     用户 <strong><em>u</em></strong> 的答案所覆盖的不可信要素</p><p>​        $FN_u$     用户 <strong><em>u</em></strong> 的答案未覆盖的可信要素     $FN_u $     用户 <strong><em>u</em></strong> 的答案未覆盖的不可信要素</p><div class="table-container"><table><thead><tr><th>真实情况</th><th>预测结果</th><th>预测结果</th></tr></thead><tbody><tr><td></td><td>正例</td><td>反例</td></tr><tr><td>正例</td><td>TP【真正例】</td><td>FN【假反例】</td></tr><tr><td>反例</td><td>FP【假正例】</td><td>TN【真反例】</td></tr></tbody></table></div><p>​        $false\space positive\space  rate = \frac{FP_u}{FP_u+TN_u} $     $true\space positive\space rate = \frac{TP_u}{TP_u+FN_u}$</p><p>​    即可利用假正例率 <strong><em>FPR</em></strong> 、真正例率 <strong><em>TPR</em></strong> 来充分表征用户 <strong><em>u</em></strong> 的可信度</p><p>​    <strong>但是在生成过程中，答案要素及其真值标签是事先不知道的</strong></p><p>​    定义二重用户可信度变量 $\phi_u^0$  和  $\phi_u^1$  对用户 <strong><em>u</em></strong> 的答案所覆盖的 <strong><em>FPR</em></strong> 、 <strong><em>TPR</em></strong>  进行建模：</p><p>​    对于每个用户 <strong><em>u</em></strong> ，我们通过两个Beta分布生成 $\phi_u^0$  和  $\phi_u^1$  </p><script type="math/tex; mode=display">\phi_u^0 \sim Beta(\alpha_{0,1},\alpha_{0,0})    \qquad\qquad\qquad     (False\space Positive\space Rate)\\ \phi_u^1 \sim Beta(\alpha_{1,1},\alpha_{1,0})    \qquad\qquad\qquad (True\space Positive\space Rate)</script><p>​    其中 $\alpha_{0,1}$  假正例计数    $\alpha_{0,0}$ 真反例计数    $\alpha_{1,1}$ 真正例计数    $\alpha_{1,0}$ 假反例计数</p><h4 id="Observation-Modeling"><a href="#Observation-Modeling" class="headerlink" title="Observation Modeling"></a>Observation Modeling</h4><p>观察建模</p><p>​    使用关键词的向量表示作为观察（observations）</p><p>​    对用户 <strong><em>u</em></strong> 对问题 <strong><em>q</em></strong> 的第 <strong>m</strong> 个单词表示，指定了如下的生成过程：</p><ul><li><p>首先，定义一个二元指标（binary indicator） $y_{u,qk}$  ，表示基于用户 <strong><em>u</em></strong> 的可信度时，问题 <strong><em>q</em></strong> 的答案中第 <strong>k</strong> 个答案要素是否应该被用户 <strong><em>u</em></strong> 所覆盖。</p><p>对于问题 <strong><em>q</em></strong> ，如果其真值标签 $t_{qk} = 1$ ，用户 <strong><em>u</em></strong> 的回答覆盖第 <strong>k</strong> 个答案要素的概率遵循可信度参数为 $\phi_u^0 $ 的伯努利分布（Bernoulli distribution）</p><script type="math/tex; mode=display">y_{u,qk} \sim Bernoulli(\phi_u^0) \qquad\qquad\qquad If t_{qk}=0,\\y_{u,qk} \sim Bernoulli(\phi_u^1) \qquad\qquad\qquad If t_{qk}=1,</script><p>至此，已经确定了答案 $a_{qu}$ 应该覆盖的答案要素集，并且考虑了用户 <strong><em>u</em></strong> 的可信度</p></li><li><p>然后，对于答案 <script type="math/tex">a_{qu}</script> 中的第 <strong>m</strong> 个关键词，其要素标签（factor label） <script type="math/tex">z_{qum}</script> 由概率密度函数表示为：</p></li></ul><script type="math/tex; mode=display">P(z_{qum}=k|\pi_q,y_{u,qk}) \propto \begin{cases}\pi_{qk} \qquad & if\space y_{u,qk}=1, \\0 & if\space y_{u,qk}=0.\end{cases}</script><p>​    概率密度函数综合考虑了答案要素混合分布（answer factor mixture distribution）和二元指标 $y_{u,q·}$ ，这意    味着在确定一个答案关键词时考虑到了语义和用户可信度</p><ul><li><p>确定要素标签后，模型对描述其对应的要素的语义含义的关键词向量进行采样</p><p>该过程中不应该涉及用户可信度</p><p>关键词的向量表示（<script type="math/tex">v_{qum}</script>）是从参数为 <script type="math/tex">\mu_{qk}$ , $\kappa_{qk}</script> 的<strong>vMF</strong>分布中随机抽样：</p><script type="math/tex; mode=display">v_{qum} \sim vMF(\mu_{qk},\kappa_{qk}).</script><p>对于一个遵循<strong>vMF</strong>分布的 <strong>D</strong> 维单元语义向量（unit semantic vector） <strong><em>v</em></strong> ，其概率密度函数为：</p><script type="math/tex; mode=display">p(v_{qum}|\mu_{qk},\kappa_{qk})=C_D(\kappa_{qk})exp(\kappa_{qk}\mu_{qk}^Tv_{qum})</script><p>对<strong>vMF</strong>分布，有两个参数：</p><p>​    平均方向（mean direction） <script type="math/tex">\mu_{qk}</script> ,  集中参数（concentration parameter） <script type="math/tex">\kappa_{qk}</script>   (<script type="math/tex">\kappa_{qk} >0</script>)</p><p><script type="math/tex">v_{qum}</script> 在单位球（the unit sphere）上的分布集中在平均方向 <script type="math/tex">\mu_{qk}</script> 的方向上，当 <script type="math/tex">\kappa_{qk}</script> 越大时越集中</p><p>在本文场景中，均值向量 $\mu$ 作为一个单位球上的语义关注（semantic focus on the unit sphere），并围绕其产生相关的语义映射（produces relevant semantic embeddings around it）</p></li></ul><h4 id="Overall-generative-process"><a href="#Overall-generative-process" class="headerlink" title="Overall generative process"></a>Overall generative process</h4><p>​    <img src="/2019/04/15/Paper-TextTruth/1555321921375.png" alt="Figure 3"></p><h3 id="Trustworthy-Aware-Answer-Scoring"><a href="#Trustworthy-Aware-Answer-Scoring" class="headerlink" title="Trustworthy-Aware Answer Scoring"></a>Trustworthy-Aware Answer Scoring</h3><p>信任感知的答案评分</p><p>​    答案的可信度应该由<strong>其提供的正确信息的量来评估</strong></p><p>​    给定问题 <strong><em>q</em></strong> 的每个答案要素的推断真值标签（inferred truth labels），根据与真值标签（truth label）$t_{qk}$</p><p>​    <script type="math/tex">t_{qk}=1</script> 的答案要素相关的答案 <script type="math/tex">a_{qu}</script> 中答案关键词的数量对答案进行评分：</p><script type="math/tex; mode=display">score_{qu} = \sum_{k=1}^{K_q}N_{u,qk}\Bbb I(t_{qk}=1),</script><p>​    其中    </p><p>​            <script type="math/tex">K_q</script> 是问题 <strong><em>q</em></strong> 的答案要素的数量    <script type="math/tex">N_{u,qk}</script> 表示用户 <strong><em>u</em></strong> 提供的，且被聚簇到要素 <strong><em>k</em></strong> 中的关键词数量</p><p>​            <script type="math/tex">\Bbb I(t_{qk}=1) =1 \quad if\space t_{qk}=1 \\</script>         <script type="math/tex">\Bbb I(t_{qk}=1) =0 \quad if\space t_{qk}=0</script></p><h3 id="Model-Fitting"><a href="#Model-Fitting" class="headerlink" title="Model Fitting"></a>Model Fitting</h3><p>模型拟合</p><p>​    估计潜在变量和用户可信度参数的方法</p><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><h3 id="Datasets"><a href="#Datasets" class="headerlink" title="Datasets"></a>Datasets</h3><p>数据集</p><ul><li><p>SuperUser Dataset  &amp;  ServerFault Dataset</p><ul><li><p>介绍：</p><p>来自社区问答CQA网站 <em>SuperUser.com</em> 和 <em>ServerFault.com</em> ,主要分别关注日常电脑使用和服务器管理方面的问题。</p></li><li><p>任务：</p><p>为每个问题提取最可信的答案</p></li><li><p>使用来自网站的答案投票作为评估的基本事实（groundtruths）</p></li></ul></li><li><p>Student Exam Dataset</p><ul><li><p>介绍：</p><p>北得克萨斯大学一个本科生班级提供的计算机科学导论作业的答案，由30名学生提交</p></li><li><p>任务：</p><p>为每个问题提取Top-K可信的学生答案</p></li><li><p>groundtruth由教师给出，所有的答案由两个评委独立评分，使用从0(完全错误)-5(完美答案)的整数范围</p></li></ul></li></ul><p><strong>Data Statistics.</strong></p><div class="table-container"><table><thead><tr><th>Item</th><th>SuperUser</th><th>ServerFault</th><th>Student Exam</th></tr></thead><tbody><tr><td>of Questions</td><td>3379</td><td>7621</td><td>80</td></tr><tr><td>of Users</td><td>1036</td><td>1920</td><td>30</td></tr><tr><td>of Answers</td><td>16014</td><td>40373</td><td>2273</td></tr></tbody></table></div><p><strong>Pre-Processing 预处理</strong></p><p>​    对所有的数据集，抛弃所有代码块、HTML标记、文本中的停止符；</p><p>​    使用实体字典（entity dictionary）和Stanford POS-Tagger 提取答案关键词</p><p>​    为训练词向量表示，使用所有抓取的文本作为语料库</p><p>​    使用 <em>gensim</em> 包中的 Skip-gram 结构学习每个答案关键词的向量表示</p><p>​    词向量维度为100，上下文窗口大小为5，最小出现次数为20</p><h3 id="Comparison-Methods"><a href="#Comparison-Methods" class="headerlink" title="Comparison Methods"></a>Comparison Methods</h3><ul><li><p>Bag-of-Word（BOW） Similarity</p><p>利用词袋模型，问题向量与其对应的答案向量之间的相似性对答案进行排序</p></li><li><p>Topic Similarity</p><p>利用LDA模型对每个问题及对应的答案提取一个100维的主题表示，类似BOW，根据问题的余弦相似度进行排序</p></li><li><p>CRH + Topic Dist.</p></li><li><p>CRH + Word Vec.</p></li><li><p>CATD + Topic Dist.</p></li><li><p>CATD + Word Vec.</p></li></ul><p><strong>Evaluation Metrics 评价指标</strong></p><ul><li>在CQA数据集上，使用每个问题返回最佳答案的精确度（precision）</li><li>在学生测试数据集上，使用每个问题返回的Top-K的答案的平均得分</li></ul><h3 id="Performance-and-Analysis"><a href="#Performance-and-Analysis" class="headerlink" title="Performance and Analysis"></a>Performance and Analysis</h3><div class="table-container"><table><thead><tr><th>Method</th><th>ServerFault</th><th>SuperUser</th></tr></thead><tbody><tr><td>BOW Similarity</td><td>0.2077</td><td>0.1944</td></tr><tr><td>Topic Similarity</td><td>0.2462</td><td>0.2462</td></tr><tr><td>CATD + Topic Dist.</td><td>0.2311</td><td>0.2308</td></tr><tr><td>CATD + Word Vec.</td><td>0.1821</td><td>0.2234</td></tr><tr><td>CRH + Topic Dist.</td><td>0.2453</td><td>0.2453</td></tr><tr><td>CRH + Word Vec.</td><td>0.1847</td><td>0.2231</td></tr><tr><td>TextTruth</td><td><strong>0.3985</strong></td><td><strong>0.4019</strong></td></tr></tbody></table></div><p><img src="/2019/04/15/Paper-TextTruth/1555334285637.png" alt="Figure 4"></p><p><strong>Analysis</strong></p><ul><li><p>基于检索的方法（BOW Similarity、Topic Similarity）</p><p>仅根据问题和答案之间的语义相似性对答案进行排序。</p><p>问题本身不一定会包含理想答案中应该包含的所有语义。</p><p>因此只能发现相关的答案，不能发现可信的答案</p></li><li><p>现有的真值发现方法能够捕获答案排序的用户可信度，但并不理想</p><p>因为将答案视为一个完整的语义单元，忽略了每个答案的语义可能是复杂的，单个向量无法捕获这些答案之间的内在关联</p><p>CRH和CATD将这些单个向量表示的加权聚合作为“真实”语义表示来评估用户可靠性，只会产生不准确的表示，进一步导致不正确的汇总结果</p></li></ul><h3 id="Case-Study"><a href="#Case-Study" class="headerlink" title="Case Study"></a>Case Study</h3><p><img src="/2019/04/15/Paper-TextTruth/1555334682417.png" alt="Figure 5"></p><p>​    蓝色的是估计的可信的关键词，红色的是估计不可信的或不相关的关键词</p><p><strong>表现分析：</strong></p><p>​    可以看到能够自动选择对问题有意义的关键词，例如 <strong><em>node</em></strong> 、<strong><em>tree</em></strong> 、<strong><em>root</em></strong></p><p>​    可以看到排名最高的答案中可信关键词比排名靠后的不可信答案中的多</p><p>​    如果是使用现有的方法，问题本身只包括一个关键词 <strong><em>tree</em></strong> ，基于检索的方法会将不可信答案排在Top2之前，    因为包含的关键词与问题中的完全相同；其次看到正确的关键词涉及多个要素，这在自然语言问答中也十分    常见，现有的方法也无法成功处理。</p><h3 id="User-Reliability-Validation"><a href="#User-Reliability-Validation" class="headerlink" title="User Reliability Validation"></a>User Reliability Validation</h3><p>​    CQA数据集中没有直接的用户可信度值，研究学生考试数据集中估计的用户可信度。 </p><p><img src="/2019/04/15/Paper-TextTruth/1555335156210.png" alt="Figure 6"></p><p>​    蓝点表示用户，y轴为用户可信度的groundtruth，x轴为估计的用户可信度</p><p>​    可以看到，groundtruth用户可信度Y增加时，估计的用户可靠性得分X通常会增加</p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>由于自然语言的语义模糊性和文本答案的复杂性，现有的真值发现方法都存在对非结构化文本数据的处理问题。</p><p>本文提出了一个概率模型 TextTruth，该模型将从答案中提取的关键要素向量表示作为输入，并基于每个答案中关键要素的可信度进行排序输出。</p><p>具体来说，模型通过对答案要素嵌入表示的生成过程建模，共同学习每个答案要素聚类的聚类标签和真值标签。</p>]]></content>
      
      
      <categories>
          
          <category> 论文阅读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Data Mining </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Python-NLP-Chapter3</title>
      <link href="/2019/04/02/Python-NLP-Chapter3/"/>
      <url>/2019/04/02/Python-NLP-Chapter3/</url>
      
        <content type="html"><![CDATA[<h1 id="Chapter3-中文分词技术"><a href="#Chapter3-中文分词技术" class="headerlink" title="Chapter3 中文分词技术"></a>Chapter3 中文分词技术</h1><h2 id="List"><a href="#List" class="headerlink" title="List"></a>List</h2><ul><li>中文分词的概念与种类</li><li>常用分词（规则分词、统计分词、混合分词等）的技术介绍</li><li>开源中文分词工具——Jieba简介</li><li>实战分词之高频词提取</li></ul><h2 id="中文分词简介"><a href="#中文分词简介" class="headerlink" title="中文分词简介"></a>中文分词简介</h2><ul><li><p>主要流派</p><ul><li>规则分词<ul><li>人工设立词库，按照一定方式进行匹配切分</li><li>难以处理新词</li></ul></li><li>统计分词<ul><li>过于依赖语料质量</li></ul></li><li>混合分词（规则+统计）</li></ul></li><li><p>主要困难：<strong>分词歧义</strong></p></li></ul><h2 id="规则分词"><a href="#规则分词" class="headerlink" title="规则分词"></a>规则分词</h2><p>基于规则的分词是一种机械分词方法，通过<strong>维护词典</strong>，在切分语句时，将语句的每个字符串与词表中的词进行逐一匹配，找到则切分，否则不予切分。</p><p>按照匹配切分的方式划分：</p><ul><li>正向最大匹配法 MM</li><li>逆向最大匹配法 RMM</li><li>双向最大匹配法</li></ul><h3 id="正向最大匹配法"><a href="#正向最大匹配法" class="headerlink" title="正向最大匹配法"></a>正向最大匹配法</h3><p>Maximum Match Method ，MM法。</p><ul><li><p>基本思想：</p><p>已知分词词典中<strong>最长词</strong>有<code>i</code>个汉子字符，则用被处理文档的当前字串中的前<code>i</code>个字作为匹配字段，查找字典。</p><p>如果字典中存在这样的<code>i</code>字的词，匹配成功，将其作为一个词切分出去。</p><p>如果不存在，将字段中最后一个字去掉，重新进行匹配；如此进行，直到匹配成功。然后取下一个<code>i</code>字字串进行匹配处理，直到文档被扫描完</p></li></ul><figure class="hljs highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MM</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment">#设置词典中最长字串为3</span></span><br><span class="line">        self.window_size = <span class="number">3</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">cut</span><span class="params">(self,text)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        对文本进行切片并匹配</span></span><br><span class="line"><span class="string">        :param text:要进行匹配的文本</span></span><br><span class="line"><span class="string">        :return: 匹配到的结果</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        result = []</span><br><span class="line">        index = <span class="number">0</span></span><br><span class="line">        text_length = len(text)</span><br><span class="line">        <span class="comment">#定义词典</span></span><br><span class="line">        dic = [<span class="string">'研究'</span>,<span class="string">'研究生'</span>,<span class="string">'生命'</span>,<span class="string">'命'</span>,<span class="string">'的'</span>,<span class="string">'起源'</span>]</span><br><span class="line">        <span class="keyword">while</span> text_length &gt; index:</span><br><span class="line">            piece = <span class="string">''</span></span><br><span class="line">            <span class="comment">#range(start,stop,step) =&gt; range(4,0,-1) =&gt; size in (4,3,2,1)</span></span><br><span class="line">            <span class="keyword">for</span> size <span class="keyword">in</span> range(self.window_size + index,index,<span class="number">-1</span>):    <span class="comment">#4,0,-1</span></span><br><span class="line">                <span class="comment">#对text进行切片，切size大小</span></span><br><span class="line">                piece = text[index:size]</span><br><span class="line">                <span class="keyword">if</span> piece <span class="keyword">in</span> dic:</span><br><span class="line">                    <span class="comment">#在词典中匹配到之后，后移继续切片</span></span><br><span class="line">                    index = size - <span class="number">1</span></span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">            <span class="comment">#没有匹配到，则往后移动</span></span><br><span class="line">            index = index + <span class="number">1</span></span><br><span class="line">            result.append(piece+<span class="string">'----'</span>)</span><br><span class="line">        print(result)</span><br><span class="line">        <span class="keyword">return</span> result</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    text = <span class="string">'研究生命的起源'</span></span><br><span class="line">    <span class="comment"># text = '研究你生命的起源'</span></span><br><span class="line">    tokenizer = MM()</span><br><span class="line">    tokenizer.cut(text)</span><br></pre></td></tr></table></figure><p>输出结果：</p><figure class="hljs highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[<span class="string">'研究生----'</span>, <span class="string">'命----'</span>, <span class="string">'的----'</span>, <span class="string">'起源----'</span>]</span><br></pre></td></tr></table></figure><p>可以看出，当词典中有长词汇覆盖短词汇时表现较差。</p><h3 id="逆向最大匹配法"><a href="#逆向最大匹配法" class="headerlink" title="逆向最大匹配法"></a>逆向最大匹配法</h3><p>Reverse Maximum Match Method RMM法</p><p>基本原理与MM法相同，不同的是分词切分的方向与MM法<strong>相反</strong></p><ul><li><p>基本思想</p><p>从文档末端开始匹配扫描，每次取末端<code>i</code>个字符做匹配</p><p>相应的，使用的分词词典是<strong>逆序词典</strong></p></li><li><p>实际操作</p><p>先将文档进行<strong>倒排处理</strong>，生成<strong>逆序文档</strong>。然后根据<strong>逆序词典</strong>，对<strong>逆序文档</strong>使用<strong>正向最大匹配法</strong>处理即可</p></li></ul><p>由于汉语中偏正结构较多，逆向最大匹配法比正向匹配的误差要小。</p><figure class="hljs highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RMM</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.window_size = <span class="number">3</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">cut</span><span class="params">(self,text)</span>:</span></span><br><span class="line">        result = []</span><br><span class="line">        index = len(text)</span><br><span class="line">        dic = [<span class="string">'研究'</span>,<span class="string">'研究生'</span>,<span class="string">'生命'</span>,<span class="string">'命'</span>,<span class="string">'的'</span>,<span class="string">'起源'</span>]</span><br><span class="line">        <span class="keyword">while</span> index &gt; <span class="number">0</span>:</span><br><span class="line">            piece = <span class="string">''</span></span><br><span class="line">            <span class="keyword">for</span> size <span class="keyword">in</span> range(index - self.window_size,index):</span><br><span class="line">                piece = text[size:index]</span><br><span class="line">                <span class="keyword">if</span> piece <span class="keyword">in</span> dic:</span><br><span class="line">                    index = size + <span class="number">1</span></span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">            index = index - <span class="number">1</span></span><br><span class="line">            result.append(piece + <span class="string">'----'</span>)</span><br><span class="line">        <span class="comment">#倒置result里面元素的位置</span></span><br><span class="line">        result.reverse()</span><br><span class="line">        print(result)</span><br><span class="line">        <span class="keyword">return</span> result</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    text = <span class="string">'研究生命的起源'</span></span><br><span class="line">    tokenizer = RMM()</span><br><span class="line">    tokenizer.cut(text)</span><br></pre></td></tr></table></figure><p>输出结果：</p><figure class="hljs highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[<span class="string">'研究----'</span>, <span class="string">'生命----'</span>, <span class="string">'的----'</span>, <span class="string">'起源----'</span>]</span><br></pre></td></tr></table></figure><h3 id="双向最大匹配"><a href="#双向最大匹配" class="headerlink" title="双向最大匹配"></a>双向最大匹配</h3><p>Bi-directction Matching Method;</p><p>将正向最大匹配得到的分词结果和逆向最大匹配得到的结果进行比较，然后按照最大匹配原则，选取词数切分最少的作为结果。</p><ul><li>规则<ul><li>如果正反向分词结果<strong>词数不同</strong>，取<strong>分词数量较少</strong>的那个</li><li>如果分词结果<strong>词数相同</strong>：<ul><li>分词结果相同，没有歧义，可以返回任一个</li><li>分词结果不同，返回其中<strong>单字较少</strong>的那个</li></ul></li></ul></li></ul><figure class="hljs highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MM</span><span class="params">(object)</span>:</span></span><br><span class="line">    ...</span><br><span class="line">   </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RMM</span><span class="params">(object)</span>:</span></span><br><span class="line">    ...</span><br><span class="line">    </span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    text = <span class="string">'研究生命的起源'</span></span><br><span class="line">    tokenizer1 = MM()</span><br><span class="line">    tokenizer2 = RMM()</span><br><span class="line">    MMresult = tokenizer1.cut(text)</span><br><span class="line">    RMMresult = tokenizer2.cut(text)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> len(MMresult) == len(RMMresult):</span><br><span class="line">        print(<span class="string">"分词结果词数相同"</span>)</span><br><span class="line">        <span class="comment">#判定是否结果相同，否则返回单字较少的</span></span><br><span class="line">        <span class="keyword">if</span> MMresult == RMMresult:</span><br><span class="line">            print(<span class="string">"分词结果相同"</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment">#判断单字数量</span></span><br><span class="line">            print(<span class="string">"返回单字数量少的"</span>)</span><br><span class="line">    <span class="keyword">elif</span> len(MMresult) &lt; len(RMMresult):</span><br><span class="line">        print(<span class="string">"MMresult:"</span>)</span><br><span class="line">        print(MMresult)</span><br><span class="line">    <span class="keyword">elif</span> len(MMresult) &gt; len(RMMresult):</span><br><span class="line">        print(<span class="string">"RMMresult:"</span>)</span><br><span class="line">        print(RMMresult)</span><br></pre></td></tr></table></figure><p>输出结果：</p><figure class="hljs highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[<span class="string">'研究生----'</span>, <span class="string">'命----'</span>, <span class="string">'的----'</span>, <span class="string">'起源----'</span>]</span><br><span class="line">[<span class="string">'研究----'</span>, <span class="string">'生命----'</span>, <span class="string">'的----'</span>, <span class="string">'起源----'</span>]</span><br><span class="line">分词结果词数相同</span><br><span class="line">返回单字数量少的</span><br></pre></td></tr></table></figure><h2 id="统计分词"><a href="#统计分词" class="headerlink" title="统计分词"></a>统计分词</h2><ul><li><p>主要思想</p><p>把每个词看做是由词的最小单位的各个字组成的，如果相连的字在不同的文本中出现的词数越多，就证明这相连的字很可能就是一个词。</p><p>因此利用字与字相邻出现的频率来反应成词的可靠度。</p></li><li><p>操作步骤</p><ul><li><p>建立统计语言模型</p></li><li><p>对句子进行单词划分，然后对划分结果进行概率计算，获得概率最大的分词方式。</p><p>其中使用统计学习算法，如隐<strong>马尔科夫HMM</strong>，<strong>条件随机场CRF</strong>等</p></li></ul></li></ul><h3 id="语言模型"><a href="#语言模型" class="headerlink" title="语言模型"></a>语言模型</h3><p>为长度为 <strong><em>m</em></strong> 的字符串确定其概率分布<strong><em>P(ω~1~ ，ω~2~，…，ω~m~)</em></strong> ，其中 <strong><em>ω~1~</em></strong> 到  <strong><em>ω~m~</em></strong> 依次表示文本中的各个词语。</p><p>一般采用<strong>链式法则</strong>计算其概率值：</p><script type="math/tex; mode=display">P(\omega_1,\omega_2,···,\omega_m) = P(\omega_1)P(\omega_2|\omega_1)P(\omega_3|\omega_1,\omega_2)\\···P(\omega_i|\omega_1,\omega2,···,\omega_{i-1})···P(\omega_m|\omega_1,\omega_2,···,\omega_{m-1})</script><p>观察可知，当<strong>文本过长</strong>时，公式右侧从第三项起每一项计算难度都很大。</p><p>为解决该问题，提出<strong><em>n</em></strong>元模型（n-gram model）降低该计算难度：</p><p>​    即在估算条件概率时，忽略距离大于等于n的上文词的影响，因此$P(\omega_i|\omega_1,\omega2,…,\omega_{i-1})$ 的计算可以简化：</p><script type="math/tex; mode=display">P(\omega_i|\omega_1,\omega2,···,\omega_{i-1}) \approx P(\omega_i|\omega_{i-(n-1)},···,\omega_{i-1})</script><p>可以看出，当n=1时称为一元模型 <em>unigram model</em>  ，效果并不理想。当 $ n\geqslant 2 $ 时，该模型可以保留一定的词序信息，且 n 越大，保留的信息越丰富，但计算成本也呈指数级增加。</p><p>一般使用频率计数的比例来计算 <strong><em>n</em></strong> 元条件概率：</p><script type="math/tex; mode=display">P(\omega_i|\omega_{i-(n-1)},···,\omega_{i-1},\omega_i) = \frac{count(\omega_{i-(n-1)},···,\omega_{i-1},\omega_i)}{count(\omega_{i-(n-1)},···,\omega_{i-1})}</script><p>​    式中 <script type="math/tex">count(\omega_{i-(n-1)},···,\omega_{i-1})</script> 表示词语 <script type="math/tex">\omega_{i-(n-1)},···,\omega_{i-1}</script> 在语料库中出现的总次数</p><p>​    可见，当<strong><em>n</em></strong>越大，模型包含的词序信息越丰富，同时计算量随之增大。与此同时，长度越长的文本序列出现的    词数也会减少，可能出现分子分母为0的情况。</p><p>​    因此，一般需要配合相应的<strong>平滑算法</strong>解决该问题，如<strong>拉普拉斯平滑算法</strong>等</p><h3 id="HMM模型"><a href="#HMM模型" class="headerlink" title="HMM模型"></a>HMM模型</h3><p>隐含马尔可夫模型（HMM），将分词作为字在字串中的序列标注任务来实现的。</p><ul><li><p>基本思路</p><p>每个字在构造一个特定的词语时都占据着一个确定的构词位置（即<strong>词位</strong>），现规定每个字最多只有四个构词位置，即 <strong>B（词首）、M（词中）、E（词尾）、S（单独成词）</strong>。</p></li></ul><figure class="hljs highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">中文 / 分词 / 是 / 文本处理 / 不可或缺 / 的 / 一步！</span><br></pre></td></tr></table></figure><figure class="hljs highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">中/B 文/E 分/B 词/E 是/S 文/B 本/M 处/M 理/E 不/B 可/M 或/M 缺/E 的/S 一/B 步/E !/S</span><br></pre></td></tr></table></figure><p>用数学抽象表示：</p><p>​    用<script type="math/tex">\lambda = \lambda_1\lambda_2···\lambda_n</script>代表输入的句子，<strong><em>n</em></strong>为句子长度，<script type="math/tex">\lambda_i</script>表示字，<script type="math/tex">o=o_1o_2···o_n</script>代表输出的标签，那么理想的输出为：</p><script type="math/tex; mode=display">max = maxP(o_1o_2···o_n|\lambda_1\lambda_2···\lambda_n)</script><p>在分词任务上，<strong><em>o</em></strong>即为B、M、E、S这四种标记，λ为注入“中” “文”等句子中的每一个字（包括标点等非中文字符）</p><p>引入<strong>观测独立性假设</strong>：每个字的输出仅仅与当前字有关</p><p>可以得到：</p><script type="math/tex; mode=display">P(o_1o_2···o_n|\lambda_1\lambda_2···\lambda_n) = P(o_1|\lambda_1)P(o_2|\lambda_2)···P(o_n|\lambda_n)</script><p>但是该方法完全没有考虑上下文，会出现不合理的情况：</p><p>​    例如B后面只能是<strong><em>M</em></strong>或者<strong><em>E</em></strong>，基于观测独立假设，可能得到<strong><em>BBB</em></strong>、<strong><em>BEM</em></strong>等输出。</p><p>HMM即用来解决该问题的一种方法。通过<strong>贝叶斯公式</strong> <script type="math/tex">P(A|B)=\frac{P(B|A)P(A)}{P(B)}</script>：</p><script type="math/tex; mode=display">P(o|\lambda)=\frac{P(o,\lambda)}{P(\lambda)}=\frac{P(\lambda|o)P(o)}{P(\lambda)}</script><p><strong><em>λ</em></strong>为给定的输入，因此<strong>P(λ)</strong>计算为常数，可以忽略。即最大化<strong>P(o|λ)</strong>等价于最大化<strong>P(λ|o)P(o)</strong></p><p>针对 <strong>P(λ|o)P(o)</strong> 作 <a href="&quot;在某一时刻所处的状态已知的情况下，过程在该时刻之后的状态只与该时刻的状态有关，而与该时刻之前的状态无关&quot;">马尔科夫假设</a>，得到：</p><script type="math/tex; mode=display">P(\lambda|o) = P(\lambda_1|o_1)P(\lambda_2|o_2)···P(\lambda_n|o_n)</script><p>同时，对 <strong>P(o)</strong> 有：</p><script type="math/tex; mode=display">P(o) = P(o_1)P(o_2|o_1)P(o_3|o_1,O_2)···P(o_n|o_1,o_2,···,o_{n-1})</script><p>此时再做<a href="&quot;在马尔科夫假设的情况下，每个输出仅仅与上一个输出有关&quot;">齐次马尔科夫假设</a> ,那么有：</p><script type="math/tex; mode=display">P(o) = P(o_1)P(o_2|o_1)P(o_3|O_2)···P(o_n|o_{n-1})</script><p>于是有：</p><script type="math/tex; mode=display">P(\lambda|o)P(o)\sim P(\lambda_1|o_1)P(o_2|o_1)P(\lambda_2|o_2)P(o_3|o_2)···P(o_n|o_{n-1})P(\lambda_n|o_n)</script><p>在<strong><em>HMM</em></strong>中，将<script type="math/tex">P(\lambda_k|o_k)</script>称为<strong>发射概率</strong>，<script type="math/tex">P(o_k|o_{k-1})</script>称为<strong>转移概率</strong>。通过设置某些<script type="math/tex">P(o_k|o_{k-1})=0</script>，可以排除类似BBB、EM等不合理的组合。</p><p>在<strong><em>HMM</em></strong>中，求解<strong><em>max</em>P(λ|o)P(o)</strong>的常用方法是<strong>Veterbi算法</strong>，它是一种动态规划方法，核心思想是：</p><p>​    如果最终的最优路径经过某个<script type="math/tex">o_i</script> ,那么从初始结点到<script type="math/tex">o_{i-1}</script>点的路径必然也是一个最优路径，因为每个节点<script type="math/tex">o_i</script>只    会影响前后两个<script type="math/tex">P(o_{i-1}|o_i)</script> 和 <script type="math/tex">P(o_i|o_{i+1})</script></p><p>根据该思想，可通过地推的方法，在考虑每个<script type="math/tex">o_i</script>时只需要求出所有经过各<script type="math/tex">o_{i-1}</script>的候选点的最优路径，再与当前的<script type="math/tex">o_i</script>结合比较。每步只需要算不超过<script type="math/tex">l^2</script>次。Viterbi算法的效率是<script type="math/tex">O(n·l^2)</script>，<script type="math/tex">l</script> 是候选数目最多的结点<script type="math/tex">o_i</script>的候选数目。</p><figure class="hljs highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">HMM</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">try_load_model</span><span class="params">(self,trained)</span>:</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(self,path)</span>:</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">viterbi</span><span class="params">(self,text,states,strat_p,trans_p,emit_p)</span>:</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">cut</span><span class="params">(self,text)</span>:</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">HMM</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line">        <span class="comment">#存取算法中间结果，不用每次都训练模型</span></span><br><span class="line">        self.model_file = <span class="string">'./data/hmm_model.pkl'</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">#状态值集合</span></span><br><span class="line">        self.state_list = [<span class="string">'B'</span>,<span class="string">'M'</span>,<span class="string">'E'</span>,<span class="string">'S'</span>]</span><br><span class="line">        <span class="comment">#参数加载，用于判断是否需要重新加载model_file</span></span><br><span class="line">        self.load_para = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">#加载已计算的中间结果，当需要重新训练时，需要初始化清空结果</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">try_load_model</span><span class="params">(self,trained)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> trained:</span><br><span class="line">            <span class="keyword">import</span> pickle</span><br><span class="line">            <span class="keyword">with</span> open(self.model_file,<span class="string">'rb'</span>) <span class="keyword">as</span> f:</span><br><span class="line">                self.A_dic = pickle.load(f)</span><br><span class="line">                self.B_dic = pickle.load(f)</span><br><span class="line">                self.Pi_dic = pickle.load(f)</span><br><span class="line">                self.load_para = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment">#状态转移概率 (状态-&gt;状态的条件概率)</span></span><br><span class="line">            self.A_dic = &#123;&#125;</span><br><span class="line">            <span class="comment">#发射概率 (状态-&gt;词语的条件概率)</span></span><br><span class="line">            self.B_dic = &#123;&#125;</span><br><span class="line">            <span class="comment">#状态的初始概率 (一句话第一个字被标记成S B E M的概率)</span></span><br><span class="line">            self.Pi_dic = &#123;&#125;</span><br><span class="line">            self.load_para = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">#计算转移概率、发射概率、初始概率</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(self,path)</span>:</span></span><br><span class="line">        <span class="comment">#重置几个概率矩阵</span></span><br><span class="line">        self.try_load_model(<span class="literal">False</span>)</span><br><span class="line">        <span class="comment">#统计状态出现次数，求P(o)</span></span><br><span class="line">        Count_dic = &#123;&#125;</span><br><span class="line">        <span class="comment">#初始化参数</span></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">init_parameters</span><span class="params">()</span>:</span></span><br><span class="line">            <span class="keyword">for</span> state <span class="keyword">in</span> self.state_list:</span><br><span class="line">                self.A_dic[state] = &#123;s:<span class="number">0.0</span> <span class="keyword">for</span> s <span class="keyword">in</span> self.state_list&#125;</span><br><span class="line">                self.Pi_dic[state] = <span class="number">0.0</span></span><br><span class="line">                self.B_dic[state] = &#123;&#125;</span><br><span class="line"></span><br><span class="line">                Count_dic[state] = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">makeLabel</span><span class="params">(text)</span>:</span></span><br><span class="line">            out_text = []</span><br><span class="line">            <span class="keyword">if</span> len(text) == <span class="number">1</span>:</span><br><span class="line">                <span class="comment">#单独成词</span></span><br><span class="line">                out_text.append(<span class="string">'S'</span>)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="comment">#词首、词中、词尾</span></span><br><span class="line">                out_text += [<span class="string">'B'</span>]+[<span class="string">'M'</span>]*(len(text)<span class="number">-2</span>)+[<span class="string">'E'</span>]</span><br><span class="line"></span><br><span class="line">            <span class="keyword">return</span> out_text</span><br><span class="line"></span><br><span class="line">        init_parameters()</span><br><span class="line">        line_num = <span class="number">-1</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">#观察者集合，主要是字以及标点等</span></span><br><span class="line">        words = set()</span><br><span class="line">        <span class="keyword">with</span> open(path,encoding=<span class="string">'utf8'</span>) <span class="keyword">as</span> f:</span><br><span class="line">            <span class="keyword">for</span> line <span class="keyword">in</span> f:</span><br><span class="line">                line_num += <span class="number">1</span></span><br><span class="line">                <span class="comment">#去除首位空格</span></span><br><span class="line">                line = line.strip()</span><br><span class="line">                <span class="keyword">if</span> <span class="keyword">not</span> line:</span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">                word_list = [i <span class="keyword">for</span> i <span class="keyword">in</span> line <span class="keyword">if</span> i != <span class="string">' '</span>]</span><br><span class="line">                words |= set(word_list) <span class="comment">#更新字的集合  | 位或运算符，按照二进制做或操作</span></span><br><span class="line"></span><br><span class="line">                linelist = line.split()</span><br><span class="line"></span><br><span class="line">                line_state = []</span><br><span class="line">                <span class="keyword">for</span> w <span class="keyword">in</span> linelist:</span><br><span class="line">                    line_state.extend(makeLabel(w))</span><br><span class="line"></span><br><span class="line">                <span class="comment">#assert断言，声明布尔值必须为真的判定，如果发生异常表明表达式为假</span></span><br><span class="line">                <span class="keyword">assert</span> len(word_list) == len(line_state)</span><br><span class="line"></span><br><span class="line">                <span class="keyword">for</span> k,v <span class="keyword">in</span> enumerate(line_state):</span><br><span class="line">                    Count_dic[v] += <span class="number">1</span></span><br><span class="line">                    <span class="keyword">if</span> k==<span class="number">0</span>:</span><br><span class="line">                        self.Pi_dic[v] += <span class="number">1</span>    <span class="comment">#每个句子的第一个字的状态，用于计算初始状态概率</span></span><br><span class="line">                    <span class="keyword">else</span>:</span><br><span class="line">                        self.A_dic[line_state[k<span class="number">-1</span>]][v] += <span class="number">1</span> <span class="comment">#计算转移概率</span></span><br><span class="line">                        self.B_dic[line_state[k]][word_list[k]] = \</span><br><span class="line">                            self.B_dic[line_state[k]].get(word_list[k],<span class="number">0</span>) + <span class="number">1.0</span>     <span class="comment">#计算发射概率</span></span><br><span class="line"></span><br><span class="line">        self.Pi_dic = &#123;k:v *<span class="number">1.0</span>/line_num <span class="keyword">for</span> k,v <span class="keyword">in</span> self.Pi_dic.items()&#125;</span><br><span class="line">        self.A_dic = &#123;k:&#123;k1:v1 / Count_dic[k] <span class="keyword">for</span> k1,v1 <span class="keyword">in</span> v.items()&#125; <span class="keyword">for</span> k,v <span class="keyword">in</span> self.A_dic.items()&#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">#加1平滑</span></span><br><span class="line">        self.B_dic = &#123;k:&#123;k1:(v1+<span class="number">1</span>)/Count_dic[k] <span class="keyword">for</span> k1,v1 <span class="keyword">in</span> v.items()&#125; <span class="keyword">for</span> k,v <span class="keyword">in</span> self.B_dic.items()&#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">#序列化</span></span><br><span class="line">        <span class="keyword">import</span> pickle</span><br><span class="line">        <span class="keyword">with</span> open(self.model_file,<span class="string">'wb'</span>) <span class="keyword">as</span> f :</span><br><span class="line">            <span class="comment">#使用pickle.dump序列化对象，并将结果数据流写到文件对象中</span></span><br><span class="line">            pickle.dump(self.A_dic,f)</span><br><span class="line">            pickle.dump(self.B_dic,f)</span><br><span class="line">            pickle.dump(self.Pi_dic,f)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> self</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">viterbi</span><span class="params">(self,text,states,start_p,trans_p,emit_p)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        Veterbi算法的实现，求最大概率的路径</span></span><br><span class="line"><span class="string">        :param text:</span></span><br><span class="line"><span class="string">        :param states:</span></span><br><span class="line"><span class="string">        :param start_p: 初始概率</span></span><br><span class="line"><span class="string">        :param trans_p: 转移概率</span></span><br><span class="line"><span class="string">        :param emit_p: 发射概率</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        V = [&#123;&#125;]</span><br><span class="line">        path = &#123;&#125;</span><br><span class="line">        <span class="keyword">for</span> y <span class="keyword">in</span> states:</span><br><span class="line">            V[<span class="number">0</span>][y] = start_p[y] * emit_p[y].get(text[<span class="number">0</span>],<span class="number">0</span>)</span><br><span class="line">            path[y] = [y]</span><br><span class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">1</span>,len(text)):</span><br><span class="line">            V.append(&#123;&#125;)</span><br><span class="line">            newpath = &#123;&#125;</span><br><span class="line"></span><br><span class="line">            <span class="comment">#检验训练的发射概率矩阵中是否有该字</span></span><br><span class="line">            <span class="comment">#python在一行后加 \ 作为换行标志符</span></span><br><span class="line">            neverSeen = text[t] <span class="keyword">not</span> <span class="keyword">in</span> emit_p[<span class="string">'S'</span>].keys() <span class="keyword">and</span> \</span><br><span class="line">                text[t] <span class="keyword">not</span> <span class="keyword">in</span> emit_p[<span class="string">'M'</span>].keys() <span class="keyword">and</span> \</span><br><span class="line">                text[t] <span class="keyword">not</span> <span class="keyword">in</span> emit_p[<span class="string">'E'</span>].keys() <span class="keyword">and</span> \</span><br><span class="line">                text[t] <span class="keyword">not</span> <span class="keyword">in</span> emit_p[<span class="string">'B'</span>].keys()</span><br><span class="line">            <span class="keyword">for</span> y <span class="keyword">in</span> states:</span><br><span class="line">                emitP = emit_p[y].get(text[t],<span class="number">0</span>) <span class="keyword">if</span> <span class="keyword">not</span> neverSeen <span class="keyword">else</span> <span class="number">1.0</span> <span class="comment">#设置未知字单独成词</span></span><br><span class="line">                (prob, state) = max(</span><br><span class="line">                    [(V[t - <span class="number">1</span>][y0] * trans_p[y0].get(y, <span class="number">0</span>) *</span><br><span class="line">                      emitP, y0)</span><br><span class="line">                     <span class="keyword">for</span> y0 <span class="keyword">in</span> states <span class="keyword">if</span> V[t - <span class="number">1</span>][y0] &gt; <span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">                V[t][y] = prob</span><br><span class="line">                newpath[y] = path[state] + [y]</span><br><span class="line">            path = newpath</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> emit_p[<span class="string">'M'</span>].get(text[<span class="number">-1</span>],<span class="number">0</span>)&gt;emit_p[<span class="string">'S'</span>].get(text[<span class="number">-1</span>],<span class="number">0</span>):</span><br><span class="line">            (prob,state) = max([(V[len(text) - <span class="number">1</span>][y],y) <span class="keyword">for</span> y <span class="keyword">in</span> (<span class="string">'E'</span>,<span class="string">'M'</span>)])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            (prob,state) = max([(V[len(text) - <span class="number">1</span>][y],y) <span class="keyword">for</span> y <span class="keyword">in</span> states])</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> (prob,path[state])</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">cut</span><span class="params">(self,text)</span>:</span></span><br><span class="line">        <span class="keyword">import</span> os</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self.load_para:</span><br><span class="line">            self.try_load_model(os.path.exists(self.model_file))</span><br><span class="line">        prob,pos_list = self.viterbi(text,self.state_list,self.Pi_dic,self.A_dic,self.B_dic)</span><br><span class="line">        begin,next = <span class="number">0</span>,<span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i,char <span class="keyword">in</span> enumerate(text):</span><br><span class="line">            pos = pos_list[i]</span><br><span class="line">            <span class="keyword">if</span> pos == <span class="string">'B'</span>:</span><br><span class="line">                begin = i</span><br><span class="line">            <span class="keyword">elif</span> pos == <span class="string">'E'</span>:</span><br><span class="line">                <span class="comment">#python yield 生成器</span></span><br><span class="line">                <span class="keyword">yield</span> text[begin:i+<span class="number">1</span>]</span><br><span class="line">                next = i+<span class="number">1</span></span><br><span class="line">            <span class="keyword">elif</span> pos == <span class="string">'S'</span>:</span><br><span class="line">                <span class="keyword">yield</span> char</span><br><span class="line">                next = i+<span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> next&lt;len(text):</span><br><span class="line">            <span class="keyword">yield</span> text[next:]</span><br><span class="line"></span><br><span class="line">hmm = HMM()</span><br><span class="line">hmm.train(<span class="string">'./data/trainCorpus.txt_utf8'</span>)</span><br><span class="line"></span><br><span class="line">text=<span class="string">'这是一个非常棒的方案!'</span></span><br><span class="line">res = hmm.cut(text)</span><br><span class="line">print(text)</span><br><span class="line">print(str(list(res)))</span><br></pre></td></tr></table></figure><p>输出结果：</p><figure class="hljs highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">这是一个非常棒的方案!</span><br><span class="line">[<span class="string">'这是'</span>, <span class="string">'一个'</span>, <span class="string">'非常'</span>, <span class="string">'棒'</span>, <span class="string">'的'</span>, <span class="string">'方案'</span>, <span class="string">'!'</span>]</span><br></pre></td></tr></table></figure><h3 id="其他统计分词算法"><a href="#其他统计分词算法" class="headerlink" title="其他统计分词算法"></a>其他统计分词算法</h3><p>条件随机场 CRF 也是一种基于马尔科夫思想的统计模型</p><p>​    CRF使得每个状态不止与它前面的状态有关，还与它后面的状态有关</p><p>神经网络分词算法，将深度学习方法应用</p><p>​    通常采用CNN、LSTM等自动发现一些模式和特征，然后结合CRF、softmax等分类算法进行分词预测</p><h2 id="混合分词"><a href="#混合分词" class="headerlink" title="混合分词"></a>混合分词</h2><p>​    在实际工程应用中，多是基于一种分词算法，然后用其他分词算法加以辅助。</p><h2 id="中文分词工具——Jieba"><a href="#中文分词工具——Jieba" class="headerlink" title="中文分词工具——Jieba"></a>中文分词工具——Jieba</h2><p><a href="https://github.com/fxsjy/jieba" target="_blank" rel="noopener">Jieba官方地址</a></p><p>Jieba分词集合了基于规则和基于统计两类方法</p><p>首先基于前缀词典进行词图扫描；对于未登陆词，Jieba使用了基于汉字成词的HMM模型，采用Viterbi算法推导</p><p>前缀词典：</p><p>​    词典中的词按照前缀包含的顺序排列；如词典中出现“上”，之后以“上“开头的词都会出现在这一部分，如”上    海“，进而出现”上海市“，从而形成一种<strong>层级包含结构</strong>。</p><p>​    因此可以快速构建包含全部可能分词结果的<strong>有向无环</strong>图；</p><p>​        有向：指全部的路径都始于第一个字，止于最后一个字</p><p>​        无环：节点之间不构成闭环</p><h3 id="Jieba的三种分词模式"><a href="#Jieba的三种分词模式" class="headerlink" title="Jieba的三种分词模式"></a>Jieba的三种分词模式</h3><ul><li>精确模式：将句子最精确的切开，适合文本分析</li><li>全模式：把句子中所有可以成词的词语都扫描出来，速度快；但是不能解决歧义</li><li>搜索引擎模式：在精确模式的基础上，对长词再次切分，提高召回率；适用于搜索引擎分词</li></ul><figure class="hljs highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> jieba</span><br><span class="line">sent = <span class="string">'中文分词是文本处理不可或缺的一步！'</span></span><br><span class="line">seg_list1 = jieba.cut(sent,cut_all=<span class="literal">True</span>)</span><br><span class="line">print(<span class="string">'全模式：'</span>,<span class="string">'/'</span>.join(seg_list1))</span><br><span class="line">seg_list2 = jieba.cut(sent,cut_all=<span class="literal">False</span>)</span><br><span class="line">print(<span class="string">'精确模式：'</span>,<span class="string">'/'</span>.join(seg_list2))</span><br><span class="line">seg_list3 = jieba.cut(sent)</span><br><span class="line">print(<span class="string">'默认精确模式：'</span>,<span class="string">'/'</span>.join(seg_list3))</span><br><span class="line">seg_list4 = jieba.cut_for_search(sent)</span><br><span class="line">print(<span class="string">'搜索引擎模式：'</span>,<span class="string">'/ '</span>.join(seg_list4))</span><br></pre></td></tr></table></figure><p>输出结果：</p><figure class="hljs highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">全模式： 中文/分词/是/文本/文本处理/本处/处理/不可/不可或缺/或缺/的/一步//</span><br><span class="line">精确模式： 中文/分词/是/文本处理/不可或缺/的/一步/！</span><br><span class="line">默认精确模式： 中文/分词/是/文本处理/不可或缺/的/一步/！</span><br><span class="line">搜索引擎模式： 中文/ 分词/ 是/ 文本/ 本处/ 处理/ 文本处理/ 不可/ 或缺/ 不可或缺/ 的/ 一步/ ！</span><br></pre></td></tr></table></figure><h3 id="实战：高频词提取"><a href="#实战：高频词提取" class="headerlink" title="实战：高频词提取"></a>实战：高频词提取</h3><p>高频词：指文档中出现频率较高且非无用的词语，在一定程度上代表了文档的焦点所在。 </p><p>高频词提取：自然语言处理中的TF（Term Frequency）策略。</p><p>​    干扰项：标点符号、停用词（“的”，“是”，“了”等无意义的常用词）</p><figure class="hljs highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_content</span><span class="params">(path)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    加载指定路径下的数据</span></span><br><span class="line"><span class="string">    :param path:</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="keyword">with</span> open(path,<span class="string">'r'</span>,encoding=<span class="string">'gbk'</span>,errors=<span class="string">'ignore'</span>) <span class="keyword">as</span> f:</span><br><span class="line">        content = <span class="string">''</span></span><br><span class="line">        <span class="keyword">for</span> l <span class="keyword">in</span> f :</span><br><span class="line">            l = l.strip()</span><br><span class="line">            content += l</span><br><span class="line">        <span class="keyword">return</span> content</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_TF</span> <span class="params">(words,topK=<span class="number">10</span>)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    高频词统计</span></span><br><span class="line"><span class="string">    :param words:</span></span><br><span class="line"><span class="string">    :param topK: 前N个高频词</span></span><br><span class="line"><span class="string">    :return: 高频词数组</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    tf_dic = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> w <span class="keyword">in</span> words:</span><br><span class="line">        tf_dic[w] = tf_dic.get(w,<span class="number">0</span>) + <span class="number">1</span></span><br><span class="line">    <span class="comment">#lambda 快速构建一个匿名函数</span></span><br><span class="line">    <span class="comment">#python sorted函数进行排序，返回一个新的list</span></span><br><span class="line">    <span class="comment">#key参数传入一个自定义lambda函数，x:x[1]即根据列表中每个元组的第二个元素进行排序</span></span><br><span class="line">    <span class="comment">#reverse设置为True进行倒序排列，从大到小</span></span><br><span class="line">    <span class="keyword">return</span> sorted(tf_dic.items(),key=<span class="keyword">lambda</span> x:x[<span class="number">1</span>],reverse=<span class="literal">True</span>)[:topK]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">import</span> glob</span><br><span class="line">    <span class="keyword">import</span> random</span><br><span class="line">    <span class="keyword">import</span> jieba</span><br><span class="line"></span><br><span class="line">    <span class="comment">#glob 通配符模块，对目录内容进行匹配</span></span><br><span class="line">    files = glob.glob(<span class="string">'./data/news/C000013/*.txt'</span>)</span><br><span class="line">    corpus = [get_content(x) <span class="keyword">for</span> x <span class="keyword">in</span> files]</span><br><span class="line"></span><br><span class="line">    <span class="comment">#randint生成一个范围内的指定随机整数</span></span><br><span class="line">    sample_inx = random.randint(<span class="number">0</span>,len(corpus))</span><br><span class="line">    split_words = list(jieba.cut(corpus[sample_inx]))</span><br><span class="line">    print(<span class="string">'样本之一：'</span>+corpus[sample_inx])</span><br><span class="line">    print(<span class="string">'样本分词效果：'</span>+<span class="string">'/'</span>.join(split_words))</span><br><span class="line">    print(<span class="string">'样本的TopK(10)词：'</span>+str(get_TF(split_words)))</span><br><span class="line"></span><br><span class="line">main()</span><br></pre></td></tr></table></figure><p>展示结果：</p><figure class="hljs highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">样本之一：风油精含有薄荷脑、樟脑、桉叶油、丁香酚、水杨酸甲酯等成分，有消炎止痛、清凉止痒、杀菌、抗真菌等功效，所以常用于蚊虫叮咬及伤风感冒引起的头痛、头晕、晕车等症状。其实，它还可以用于轻度烫伤的治疗。烫伤最危险的是损伤部位由于细菌侵入而引起感染，而风油精中的薄荷脑、樟脑、桉叶油成分恰好对细菌有较好的杀灭作用，所以当受到小范围烫伤时，不妨试试风油精。不过，使用风油精前必须先分清烫伤的轻重。一般而言，只要是由火焰、热水、蒸汽、电流、放射线、激光、强酸、强碱等作用于人体所引起的损伤都可统称为烧烫伤。在临床上，烫伤被分为三级。Ⅰ度烫伤：为表皮烫伤，局部皮肤发红疼痛；Ⅱ度烫伤：浅Ⅱ度烫伤是真皮浅层烫伤，局部可出现水泡，泡壁薄，极度水肿，有剧痛；深Ⅱ度烫伤可达真皮深层，疼痛较轻，水泡较小，泡壁较厚，治愈后可留下瘢痕；Ⅲ度烫伤：烫伤深度达皮肤全层或更深，皮色苍白或形成焦痂，无痛感，经治疗后焦痂可脱落，但会形成肉芽创面，或引起局部畸形。可使用风油精治疗的烫伤主要指小范围浅Ⅱ度烫伤或Ⅰ度烫伤。其余类型烫伤必须送医院救治。使用方法：将风油精直接滴敷在烫伤部位，每隔<span class="number">3</span>—<span class="number">4</span>小时滴敷一次，不仅止痛效果明显，且不易发生感染，无结痂，愈后一般不会留下瘢痕。</span><br><span class="line">样本分词效果：风油精/含有/薄荷脑/、/樟脑/、/桉叶油/、/丁香酚/、/水杨酸甲酯/等/成分/，/有/消炎/止痛/、/清凉/止痒/、/杀菌/、/抗真菌/等/功效/，/所以/常用/于/蚊虫/叮咬/及/伤风感冒/引起/的/头痛/、/头晕/、/晕车/等/症状/。/其实/，/它/还/可以/用于/轻度/烫伤/的/治疗/。/烫伤/最/危险/的/是/损伤/部位/由于/细菌/侵入/而/引起/感染/，/而/风油精/中/的/薄荷脑/、/樟脑/、/桉叶油/成分/恰好/对/细菌/有/较/好/的/杀灭/作用/，/所以/当/受到/小/范围/烫伤/时/，/不妨/试试/风油精/。/不过/，/使用/风油精/前/必须/先/分清/烫伤/的/轻重/。/一般而言/，/只要/是/由/火焰/、/热水/、/蒸汽/、/电流/、/放射线/、/激光/、/强酸/、/强碱/等/作用/于/人体/所/引起/的/损伤/都/可/统称/为/烧烫伤/。/在/临床/上/，/烫伤/被/分为/三级/。/Ⅰ/度/烫伤/：/为/表皮/烫伤/，/局部/皮肤/发红/疼痛/；/Ⅱ/度/烫伤/：/浅/Ⅱ/度/烫伤/是/真皮/浅层/烫伤/，/局部/可/出现/水泡/，/泡壁/薄/，/极度/水肿/，/有/剧痛/；/深/Ⅱ/度/烫伤/可/达/真皮/深层/，/疼痛/较/轻/，/水泡/较/小/，/泡壁/较厚/，/治愈/后/可/留下/瘢痕/；/Ⅲ/度/烫伤/：/烫伤/深度/达/皮肤/全层/或/更深/，/皮色/苍白/或/形成/焦痂/，/无/痛感/，/经/治疗/后/焦痂/可/脱落/，/但会/形成/肉芽/创面/，/或/引起/局部/畸形/。/可/使用/风油精/治疗/的/烫伤/主要/指小/范围/浅/Ⅱ/度/烫伤/或/Ⅰ/度/烫伤/。/其余/类型/烫伤/必须/送/医院/救治/。/使用/方法/：/将/风油精/直接/滴/敷/在/烫伤/部位/，/每隔/<span class="number">3</span>/—/<span class="number">4</span>/小时/滴/敷/一次/，/不仅/止痛/效果/明显/，/且/不易/发生/感染/，/无/结痂/，/愈后/一般/不会/留下/瘢痕/。</span><br><span class="line">样本的TopK(<span class="number">10</span>)词：[(<span class="string">'，'</span>, <span class="number">28</span>), (<span class="string">'、'</span>, <span class="number">18</span>), (<span class="string">'烫伤'</span>, <span class="number">18</span>), (<span class="string">'。'</span>, <span class="number">10</span>), (<span class="string">'的'</span>, <span class="number">8</span>), (<span class="string">'度'</span>, <span class="number">7</span>), (<span class="string">'风油精'</span>, <span class="number">6</span>), (<span class="string">'可'</span>, <span class="number">6</span>), (<span class="string">'等'</span>, <span class="number">4</span>), (<span class="string">'引起'</span>, <span class="number">4</span>)]</span><br></pre></td></tr></table></figure><p><strong>自定义词典优化：</strong></p><p>​    将常用的停用词（包括标点符号）写入到文件中，并进行过滤</p><figure class="hljs highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">stop_words</span><span class="params">(path)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> open(path，encoding=<span class="string">'utf8'</span>) <span class="keyword">as</span> f :</span><br><span class="line">        <span class="keyword">return</span> [l.strip() <span class="keyword">for</span> l <span class="keyword">in</span> f]</span><br><span class="line"></span><br><span class="line"><span class="comment">#过滤停用词</span></span><br><span class="line">split_words = [x <span class="keyword">for</span> x <span class="keyword">in</span> jieba.cut(corpus[sample_inx]) <span class="keyword">if</span> x <span class="keyword">not</span> <span class="keyword">in</span> stop_words(<span class="string">'./data/stop_words.utf8'</span>)]</span><br></pre></td></tr></table></figure><p>​    输出结果：</p><figure class="hljs highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">样本之一：肺燥型咳嗽，干咳无痰或少痰者，宜吃以下之品。百合能润肺止咳。《上海常用中草药》中说：“百合治肺热咳嗽，干咳久咳。”可用鲜百合<span class="number">100</span>~<span class="number">150</span>克，加冰糖适量煎汤喝。也可用新百合<span class="number">120</span>克，蜂蜜<span class="number">50</span>克，拌和蒸熟后食用。甘蔗有生津、润肺、止咳的功效。肺燥咳嗽之人尤为适宜。可单饮甘蔗汁，或用蔗浆和粳米或糯米煮成稀薄粥食用。豆腐浆《随息居饮食谱》云：“豆浆清肺补胃，润燥化痰。”所以，燥热型干咳少痰者宜食。蜂蜜善于润燥，可治肺燥咳嗽。凡干咳无痰、燥咳不愈者，单用蜂蜜<span class="number">15</span>~<span class="number">30</span>克，开水冲服，早晚各<span class="number">1</span>次。也可用蜂蜜<span class="number">50</span>克，熟猪油<span class="number">50</span>克，同熬匀后备用，每日早晚开水冲服<span class="number">1</span>调羹。饴糖据《日华子本草》记载：“饴糖消痰止嗽，并润五脏。”凡燥咳无痰或少痰之人，宜用饴糖<span class="number">30</span>克，同白萝卜汁<span class="number">100</span>克，搅匀后蒸熟，分<span class="number">2</span>次服食，连吃<span class="number">3</span>~<span class="number">5</span>天，有很好的润肺止咳效果。白木耳能滋阴、润肺、生津、止咳，适宜肺燥干咳无痰之人食用。《增订伪药条辨》就有记载：“白木耳治肺热肺燥，干咳痰嗽。”柿霜有润燥、化痰、止咳的作用，能治肺热燥咳。《医学衷中参西录》中说得好：“柿霜入肺，而甘凉滑润。其甘也，能益肺气；其凉也，能清肺热；其滑也，能利肺痰；其润也，能滋肺燥。”可以每次用柿霜<span class="number">10</span>克，开水冲服，每日<span class="number">2</span>~<span class="number">3</span>次。北沙参能养阴清肺、祛痰止咳，适宜肺热燥咳者煎水喝。若用北沙参、麦门冬各<span class="number">10</span>克，川贝母<span class="number">6</span>克，同煎服则更妙。海松子能润肺燥，故肺燥于咳者宜食。《玄感传尸方》中有“风髓汤”一法：“治肺燥咳嗽：松子仁一两(<span class="number">30</span>克)，胡桃仁二两(<span class="number">100</span>克)。研膏，和熟蜜半两(<span class="number">15</span>克)收之。每服二钱(<span class="number">6</span>克)，食后沸汤点服。”或采用《士材三书》中的办法，用海松子仁<span class="number">30</span>克，同粳米<span class="number">50</span>克煮成稀薄粥食用。花生有润肺止咳作用，适宜燥咳之人服食。《药性考》中记载：“落花生，干咳者宜餐，滋燥润火。”民间常用花生仁、大枣、蜂蜜或冰糖各<span class="number">30</span>克，水煎，吃花生和枣并饮汤，日服<span class="number">2</span>次，对干咳无痰或少痰者颇宜。白砂糖能润肺生津，肺燥咳嗽之人宜服食。《本草纲目》云：“白砂糖润心肺燥热，治嗽消痰。”由白砂糖煎炼而成的冰块状结晶，也同样有润肺止咳作用，或含化，或煎水饮，对燥热者均宜。橄榄有清肺、生津、止咳的作用。《本草再新》中指出：“润肺滋阴，消痰理气，止咳嗽。”中国药科大学叶橘泉教授也认为橄榄“有清肺解毒化痰之功”。由于橄榄能润肺滋阴生津液，故燥咳之人宜食之，橄榄又有清肺消痰作用，所以肺热咳嗽者亦宜。榧子有润肺燥，止咳嗽的作用。《本草备要》记载：“榧子润肺杀虫”。《生生编》言其“治咳嗽”。《本草再新》中还说：“治肺火，止咳嗽”。所以，对燥热型咳嗽者，食之最宜。燕窝性味甘平，有养阴润燥作用。《本草从新》中曾说：“大养肺阴，化痰止嗽，补而能清”。《食物宜忌》也认为它能“润肺，消痰涎”。《本草再新》还指出：“大补元气，润肺滋阴，治虚劳咳嗽。”所以，对燥热咳嗽，久咳无痰或少痰之人，食之尤宜。芝麻性味甘辛，有润五脏的作用，故肺燥干咳之人宜食之。民间有用黑芝麻<span class="number">125</span>克，冰糖<span class="number">30</span>克，共捣烂，每次开水冲服<span class="number">15</span>~<span class="number">30</span>克，早晚各服一次，专治干咳症。黄精性子味甘，能润肺养阴生津，故肺燥型咳嗽久咳无痰者宜食之。《四川中药志》中就曾指出：“补肾润肺，益气滋阴，治肺虚咳嗽”，福建省民间常以此同冰糖炖食，以治肺燥干咳。石斛有生津养阴的作用。《药品化义》中说得好：“石斛气味轻清，合肺之性，性凉而清，得肺之宜。肺为娇脏，独此最为相配。主治肺气久虚，咳嗽不止。”凡肺虚肺燥久咳少痰之人，食之最宜。柿饼性味甘涩而凉。《本草通玄》中认为它能“润心肺，消痰”，古代医家常用以治疗肺燥干咳无痰或咳痰带血，民间也有用柿饼<span class="number">2</span>个，川贝母末<span class="number">9</span>克，柿饼挖开去核，纳入川贝末，饭上蒸熟，一次食尽，日服<span class="number">2</span>次。对肺燥型咳嗽者颇宜。猪肉有滋阴、润燥作用，尤其是肥猪肉，干咳燥咳者宜食之。正如《本草备要》所言：“猪肉生痰，惟风痰、湿痰、寒痰忌之，如老人燥痰干咳，更须肥浓以滋润之，不可拘泥于猪肉生痰之说也。”《随息居饮食谱》中还介绍：“治干嗽!猪肉煮汤，吹去油饮。”阿胶性子味甘，能滋阴，对肺燥干咳和肺阴虚所致的久咳不止者，食之尤宜。正如明·李时珍所言：“阿胶，大要只是补血与液，故能清肺益阴而治诸证。”《圣济总录》还有“阿胶饮”，就是以阿胶为主，“治久咳嗽”。甜杏仁性平味甘，能润肺止咳平喘，对燥咳虚喘者尤宜。《食物中药与便方》介绍：“肺病虚弱，老年咳嗽，干咳无痰：甜杏仁炒熟，每日早、晚嚼食<span class="number">7</span>～<span class="number">10</span>粒。或加砂糖一同捣烂细研，开水冲服，<span class="number">1</span>日<span class="number">2</span>次。”鸭肉性平，味甘咸，能滋阴止咳。《本草汇》中曾说它“滋阴除蒸，化虚痰，止咳嗽”。《随息居饮食谱》亦认为：“鸭肉滋五脏之阴，清虚劳之热，……止嗽。”故对肺燥型咳嗽，干咳无痰或少痰者，宜食之。鸭蛋亦有滋阴止咳作用，故肺燥咳嗽者亦宜。此外，风寒型咳嗽之人还宜吃些花生、赤砂糖、南瓜、大蒜、薤白、砂仁、桂皮、香醋、咖啡等。风热型咳嗽还宜服食苹果、草莓、菠萝、橄榄、椰子浆、菊花脑、瓠子、节瓜、苦瓜、地瓜、黄瓜、菜瓜、榧子、莴苣、茭白、薤菜、芹菜、绿豆芽等。肺燥型咳嗽者还宜吃牛奶、胖大海、青菜、无花果、梨子、桑椹、枇杷等。</span><br><span class="line">样本分词效果：肺燥/型/咳嗽/干咳/无痰/少/痰/宜吃/以下/之品/百合/润肺/止咳/上海/常用/中草药/中说/百合/治肺/热/咳嗽/干咳/久/可用/鲜/百合/<span class="number">100</span>/<span class="number">150</span>/克/加/冰糖/适量/煎/汤/喝/可用/新/百合/<span class="number">120</span>/克/蜂蜜/<span class="number">50</span>/克/拌和/蒸熟/食用/甘蔗/有生/津/润肺/止咳/功效/肺燥/咳嗽/尤为/适宜/可单/饮/甘蔗汁/或用/蔗浆/粳米/糯米/煮成/稀薄/粥/食用/豆腐/浆/随息/居/饮食/谱/云/豆浆/清肺/补胃/润燥/化痰/燥热/型/干咳/少/痰/宜食/蜂蜜/善于/润燥/可治/肺燥/咳嗽/干咳/无痰/燥/愈者/单/蜂蜜/<span class="number">15</span>/<span class="number">30</span>/克/开水/冲服/早晚/次/可用/蜂蜜/<span class="number">50</span>/克/熟猪油/<span class="number">50</span>/克/同熬/匀/后备/每日/早晚/开水/冲服/调羹/饴糖/日华子/本草/记载/饴糖/消痰/止嗽/并润/五脏/凡燥/痰/少/痰/宜用/饴糖/<span class="number">30</span>/克/白萝卜/汁/<span class="number">100</span>/克/搅匀/蒸熟/分/次/服食/吃/天/润肺/止咳/效果/白木耳/滋阴/润肺/生津/止咳/适宜/肺燥/干咳/无痰/食用/增订/伪药/条辨/记载/白木耳/治肺/热/肺燥/干/咳痰/嗽/柿霜/润燥/化痰/止咳/作用/能治肺/热燥/医学/衷中/参西录/中/说得好/柿霜/入/肺/甘凉/滑润/其甘/能益/肺气/其凉/清肺/热/其滑/能利/肺/痰/其润/能滋/肺燥/每次/柿霜/<span class="number">10</span>/克/开水/冲服/每日/次/北沙参/养阴清/肺/祛痰/止咳/适宜/肺热燥/咳者/煎/水/喝/若用/北沙参/麦门冬/<span class="number">10</span>/克/川贝母/克/煎/服则/更妙/海/松子/润肺/燥/肺燥/咳者/宜食/玄感传/尸方/中有/风髓/汤/一法/治/肺燥/咳嗽/松子/仁/一两/<span class="number">30</span>/克/胡桃/仁/二两/<span class="number">100</span>/克/研膏/熟蜜/半两/<span class="number">15</span>/克/收之/每服/二钱/克/食后/沸汤/点服/采用/士材/三书/中/办法/用海/松子/仁/<span class="number">30</span>/克/粳米/<span class="number">50</span>/克/煮成/稀薄/粥/食用/花生/润肺/止咳/作用/适宜/燥/咳之人/服食/药性/考/中/记载/落花生/干咳/宜餐/滋燥润火/民间/常用/花生仁/大枣/蜂蜜/冰糖/<span class="number">30</span>/克/水/煎/吃/花生/枣/饮汤/日服/次/干咳/无痰/少/痰/颇/宜/白砂糖/润肺/生津/肺燥/咳嗽/之人宜/服食/本草纲目/云/白砂糖/润/心肺/燥热/治嗽/消痰/白砂糖/煎/炼而成/冰块/状/结晶/同样/润肺/止咳/作用/含化/煎/水/饮/燥热/均/宜/橄榄/清肺/生津/止咳/作用/本草/新/中/指出/润肺/滋阴/消痰/理气/止/咳嗽/中国/药科/大学/叶橘/泉/教授/认为/橄榄/清肺/解毒/化痰/之功/橄榄/润肺/滋阴/生/津液/故燥/咳之人/宜食/橄榄/清肺/消痰/作用/肺热/咳嗽/宜/榧子/润肺/燥/止/咳嗽/作用/本草/备要/记载/榧子/润肺/杀虫/生/生编/言/治咳嗽/本草/新/中/治肺/火/止/咳嗽/燥热/型/咳嗽/食/最宜/燕窝/性味/甘平/养阴润/燥/作用/本草/新/中曾/大养/肺/阴/化痰/止嗽/补而能/清/食物/宜忌/认为/润肺/消痰/涎/本草/新/指出/大补/元气/润肺/滋阴/治/虚劳/咳嗽/燥热/咳嗽/久/痰/少/痰/食/尤宜/芝麻/性味/甘辛/有润/五脏/作用/肺燥/干咳/之人/宜食/民间/有用/黑芝麻/<span class="number">125</span>/克/冰糖/<span class="number">30</span>/克/共/捣烂/每次/开水/冲服/<span class="number">15</span>/<span class="number">30</span>/克/早晚/各服/一次/专治/干咳/症/黄精/性子/味甘/润肺/养阴生/津/肺燥/型/咳嗽/久/痰/宜食/四川/中药/志/中/指出/补肾/润肺/益气/滋阴/治肺/虚/咳嗽/福建省/民间/常/以此/冰糖/炖/食/以治/肺燥/干咳/石斛/有生/津养/阴/作用/药品/化义/中/说得好/石斛/气味/轻清/合肺/之性/性凉/清/肺之宜/肺为/娇脏/独此/最为/相配/主治/肺气久/虚/咳嗽/不止/肺/虚/肺燥/久/咳少/痰/食/最宜/柿饼/性味/甘涩/凉/本草/通玄/中/认为/润/心肺/消痰/古代/医家/常用/治疗/肺燥/干咳/无痰/咳痰/带血/民间/有用/柿饼/川贝母/末/克/柿饼/挖/开去/核/纳入/川贝/末/饭上/蒸熟/一次/食尽/日服/次/肺燥/型/咳嗽/颇/宜/猪肉/滋阴/润燥/作用/尤其/肥/猪肉/干咳/燥/咳者/宜食/本草/备要/言/猪肉/生痰/惟风/痰/湿痰/寒痰/忌/老人/燥/痰/干咳/更须/肥/浓以/滋润/不可/拘泥于/猪肉/生痰/随息/居/饮食/谱/中/介绍/治/干嗽/猪肉/煮汤/吹/油/饮/阿胶/性子/味甘/滋阴/肺燥/干咳/肺/阴虚/所致/久/不止/食/尤宜/明/·/李时珍/言/阿胶/补血/液/故能/清肺/益阴而治/诸证/圣济总录/阿胶/饮/阿胶/为主/治久/咳嗽/甜/杏仁/性/平味甘/润肺/止咳/平喘/对燥/咳虚喘者/尤宜/食物/中药/便方/介绍/肺病/虚弱/老年/咳嗽/干咳/无痰/甜/杏仁/炒熟/每日/早/晚嚼食/～/<span class="number">10</span>/粒/或加/砂糖/一同/捣烂/细研/开水/冲服/日/次/鸭肉/性平/味甘咸/滋阴/止咳/本草/汇/中曾/滋阴/除蒸/化虚/痰/止/咳嗽/随息/居/饮食/谱/认为/鸭肉/滋/五脏/之阴/清/虚劳/之热/止嗽/故对/肺燥/型/咳嗽/干咳/无痰/少/痰/宜食/鸭蛋/滋阴/止咳/作用/肺燥/咳嗽/宜/风寒/型/咳嗽/之人/宜吃些/花生/赤/砂糖/南瓜/大蒜/薤白/砂仁/桂皮/香醋/咖啡/风热型/咳嗽/还宜/服食/苹果/草莓/菠萝/橄榄/椰子/浆/菊花/脑/瓠子/节瓜/苦瓜/地瓜/黄瓜/菜瓜/榧子/莴苣/茭白/薤菜/芹菜/绿豆芽/肺燥/型/咳嗽/宜/吃/牛奶/胖大海/青菜/无花果/梨子/桑椹/枇杷</span><br><span class="line">样本的TopK(<span class="number">10</span>)词：[(<span class="string">'咳嗽'</span>, <span class="number">25</span>), (<span class="string">'克'</span>, <span class="number">22</span>), (<span class="string">'肺燥'</span>, <span class="number">19</span>), (<span class="string">'润肺'</span>, <span class="number">17</span>), (<span class="string">'干咳'</span>, <span class="number">16</span>), (<span class="string">'痰'</span>, <span class="number">14</span>), (<span class="string">'止咳'</span>, <span class="number">12</span>), (<span class="string">'作用'</span>, <span class="number">11</span>), (<span class="string">'滋阴'</span>, <span class="number">10</span>), (<span class="string">'本草'</span>, <span class="number">9</span>)]</span><br></pre></td></tr></table></figure><p>用户可以加载自己的自定义词典：</p><figure class="hljs highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#Jieba加载用户自定义词典</span></span><br><span class="line">jieba.load_userdict(<span class="string">'./data/user_dict.utf8'</span>)</span><br></pre></td></tr></table></figure><p>要求用户词典格式一般如下：</p><figure class="hljs highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">朝三暮四 <span class="number">3</span> i</span><br><span class="line">大数据 <span class="number">5</span></span><br><span class="line">汤姆 nz</span><br><span class="line">公主坟</span><br></pre></td></tr></table></figure><p>每一行分为三个部分：词语、词频（可省略）、词性（可省略）；用空格隔开，顺序不可以颠倒；</p><p>词典文件需要是 <strong><em>utf8</em></strong> 编码</p>]]></content>
      
      
      <categories>
          
          <category> Python自然语言处理实战 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
            <tag> Python </tag>
            
            <tag> Jieba </tag>
            
            <tag> 中文分词 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Python-NLP-Chapter2</title>
      <link href="/2019/03/31/Python-NLP-Chapter2/"/>
      <url>/2019/03/31/Python-NLP-Chapter2/</url>
      
        <content type="html"><![CDATA[<h1 id="Chapter-2-NLP前置技术解析"><a href="#Chapter-2-NLP前置技术解析" class="headerlink" title="Chapter 2 NLP前置技术解析"></a>Chapter 2 NLP前置技术解析</h1><h2 id="list"><a href="#list" class="headerlink" title="list"></a>list</h2><ul><li>选择Python作为NLP开发语言</li><li>安装与使用Anaconda</li><li>正则表达式</li><li>Numpy</li></ul><h2 id="Python语言"><a href="#Python语言" class="headerlink" title="Python语言"></a>Python语言</h2><ul><li><a href="https://docs.python.org/zh-cn/3.7/" target="_blank" rel="noopener">官方中文文档</a></li></ul><h2 id="正则表达式在NLP的基本应用"><a href="#正则表达式在NLP的基本应用" class="headerlink" title="正则表达式在NLP的基本应用"></a>正则表达式在NLP的基本应用</h2><p>作用：</p><p>​    将非结构化、半结构化文本转为结构化以方便后续的文本挖掘；去除“噪声”</p><h3 id="正则表达式中一些特殊符号可以处理常用逻辑"><a href="#正则表达式中一些特殊符号可以处理常用逻辑" class="headerlink" title="正则表达式中一些特殊符号可以处理常用逻辑"></a>正则表达式中一些特殊符号可以处理常用逻辑</h3><div class="table-container"><table><thead><tr><th>符号</th><th>含义</th></tr></thead><tbody><tr><td>.</td><td>匹配任意一个字符<br>“.” 代替任何单个字符，换行除外</td></tr><tr><td>^</td><td>匹配<strong>开始</strong>的字符串</td></tr><tr><td>$</td><td>匹配<strong>结尾</strong>的字符串</td></tr><tr><td>[]</td><td>匹配多个字符串</td></tr></tbody></table></div><h3 id="匹配字符串"><a href="#匹配字符串" class="headerlink" title="匹配字符串"></a>匹配字符串</h3><p>在Python中使用re模块实现正则表达式。</p><p>通过使用 <figure class="hljs highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">**例 获取包含“爬虫”关键字的句子**</span><br><span class="line"></span><br><span class="line">```python</span><br><span class="line">import re</span><br><span class="line">text_string = &apos;文本最重要的来源无疑是网络。我们要把网络中的文本获取形成一个文本数据库。利用一个爬虫抓取到网络中的信息。爬取的策略有广度爬取和深度爬取。根据用户的需求，爬虫可以有主题爬虫和通用爬虫之分。&apos;</span><br><span class="line">regex = &apos;爬虫&apos;</span><br><span class="line">#以句号为分隔符通过split切分段落为句子</span><br><span class="line">p_string = text_string.split(&apos;。&apos;) </span><br><span class="line">for line in p_string:</span><br><span class="line">    if re.search(regex,line) is not None:</span><br><span class="line">        print(line)</span><br></pre></td></tr></table></figure></p><p>输出结果：</p><figure class="hljs highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">利用一个爬虫抓取到网络中的信息</span><br><span class="line">根据用户的需求，爬虫可以有主题爬虫和通用爬虫之分</span><br></pre></td></tr></table></figure><h4 id="例-匹配任意一个字符"><a href="#例-匹配任意一个字符" class="headerlink" title="例 匹配任意一个字符"></a>例 匹配任意一个字符</h4><div class="table-container"><table><thead><tr><th>正则表达式</th><th>可以匹配的例子</th><th>不能匹配的例子</th></tr></thead><tbody><tr><td>a.c</td><td>abc  ,  branch</td><td>add ,  crash</td></tr><tr><td>..t</td><td>bat  ,  oat</td><td>it  ,  table</td></tr></tbody></table></div><figure class="hljs highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line">text_string = <span class="string">'文本最重要的来源无疑是网络。我们要把网络中的文本获取形成一个文本数据库。利用一个爬虫抓取到网络中的信息。爬取的策略有广度爬取和深度爬取。根据用户的需求，爬虫可以有主题爬虫和通用爬虫之分。'</span></span><br><span class="line"><span class="comment">#查找包含“爬”+任意一个字的句子</span></span><br><span class="line">regex = <span class="string">'爬.'</span></span><br><span class="line"><span class="comment">#以句号为分隔符通过split切分段落为句子</span></span><br><span class="line">p_string = text_string.split(<span class="string">'。'</span>)</span><br><span class="line"><span class="keyword">for</span> line <span class="keyword">in</span> p_string:</span><br><span class="line">    <span class="keyword">if</span> re.search(regex,line) <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        print(line)</span><br></pre></td></tr></table></figure><p>输出结果：</p><figure class="hljs highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">利用一个爬虫抓取到网络中的信息</span><br><span class="line">爬取的策略有广度爬取和深度爬取</span><br><span class="line">根据用户的需求，爬虫可以有主题爬虫和通用爬虫之分</span><br></pre></td></tr></table></figure><h4 id="例-匹配起始和结尾字符串"><a href="#例-匹配起始和结尾字符串" class="headerlink" title="例 匹配起始和结尾字符串"></a>例 匹配起始和结尾字符串</h4><ul><li>“^a” 表示的是匹配所有以字母a<strong>开头</strong>的字符串</li><li>“a$” 表示的是所有以字母a<strong>结尾</strong>的字符串</li></ul><figure class="hljs highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line">text_string = <span class="string">'文本最重要的来源无疑是网络。我们要把网络中的文本获取形成一个文本数据库。利用一个爬虫抓取到网络中的信息。爬取的策略有广度爬取和深度爬取。根据用户的需求，爬虫可以有主题爬虫和通用爬虫之分。'</span></span><br><span class="line"><span class="comment">#查找以“文本”为起始的句子</span></span><br><span class="line">regex = <span class="string">'^文本'</span></span><br><span class="line"><span class="comment">#以句号为分隔符通过split切分段落为句子</span></span><br><span class="line">p_string = text_string.split(<span class="string">'。'</span>)</span><br><span class="line"><span class="keyword">for</span> line <span class="keyword">in</span> p_string:</span><br><span class="line">    <span class="keyword">if</span> re.search(regex,line) <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        print(line)</span><br></pre></td></tr></table></figure><p>输出结果：</p><figure class="hljs highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">文本最重要的来源无疑是网络</span><br></pre></td></tr></table></figure><figure class="hljs highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line">text_string = <span class="string">'文本最重要的来源无疑是网络。我们要把网络中的文本获取形成一个文本数据库。利用一个爬虫抓取到网络中的信息。爬取的策略有广度爬取和深度爬取。根据用户的需求，爬虫可以有主题爬虫和通用爬虫之分。'</span></span><br><span class="line"><span class="comment">#查找以“信息”为结尾的句子</span></span><br><span class="line">regex = <span class="string">'信息$'</span></span><br><span class="line"><span class="comment">#以句号为分隔符通过split切分段落为句子</span></span><br><span class="line">p_string = text_string.split(<span class="string">'。'</span>)</span><br><span class="line"><span class="keyword">for</span> line <span class="keyword">in</span> p_string:</span><br><span class="line">    <span class="keyword">if</span> re.search(regex,line) <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        print(line)</span><br></pre></td></tr></table></figure><p>输出结果：</p><figure class="hljs highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">利用一个爬虫抓取到网络中的信息</span><br></pre></td></tr></table></figure><h4 id="例-匹配多个字符"><a href="#例-匹配多个字符" class="headerlink" title="例 匹配多个字符"></a>例 匹配多个字符</h4><ul><li>[bcr]at代表的是匹配 bat 、cat 、 rat</li></ul><figure class="hljs highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line">text_string = [<span class="string">'[重要的]今年第七号台风23日登录广州东部沿海地区'</span>,<span class="string">'上海发布车库销售管理通知：违规者暂停网签资格'</span>,<span class="string">'[紧要的]中国对印连发强硬信息，印度急切需要结束对峙'</span>]</span><br><span class="line"><span class="comment">#使用^表示起始</span></span><br><span class="line"><span class="comment">#存在“重要”或“紧要”，使用[]匹配多个字符</span></span><br><span class="line"><span class="comment">#以“..”代表之后的两个字符</span></span><br><span class="line">regex = <span class="string">'^\[[重紧]..\]'</span></span><br><span class="line"><span class="keyword">for</span> line <span class="keyword">in</span> text_string:</span><br><span class="line">    <span class="keyword">if</span> re.search(regex,line) <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        print(line)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        print(<span class="string">"not match"</span>)</span><br></pre></td></tr></table></figure><p>输出结果：</p><figure class="hljs highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[重要的]今年第七号台风23日登录广州东部沿海地区</span><br><span class="line">not match</span><br><span class="line">[紧要的]中国对印连发强硬信息，印度急切需要结束对峙</span><br></pre></td></tr></table></figure><h3 id="转义字符”-”匹配"><a href="#转义字符”-”匹配" class="headerlink" title="转义字符”\”匹配"></a>转义字符”\”匹配</h3><p>正则表达式中使用”\”作为转义字符，因此如果要匹配文本中的字符”\”,需要四个反斜杠<figure class="hljs highlight plain"><figcaption><span>：</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">前两个和后两个分别在编程语言里转义成反斜杠，然后再在正则表达式里转义成一个反斜杠</span><br><span class="line"></span><br><span class="line">```python</span><br><span class="line">import  re</span><br><span class="line">if re.search(&quot;\\\\&quot;,&quot;I have one nee\dle&quot;) is not None:</span><br><span class="line">    print(&quot;match it&quot;)</span><br><span class="line">else:</span><br><span class="line">    print(&quot;not match&quot;)</span><br></pre></td></tr></table></figure></p><p>输出结果：</p><figure class="hljs highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">match it</span><br></pre></td></tr></table></figure><p>Python中可以通过<figure class="hljs highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">````python</span><br><span class="line">import  re</span><br><span class="line">if re.search(r&quot;\\&quot;,&quot;I have one nee\dle&quot;) is not None:</span><br><span class="line">    print(&quot;match it&quot;)</span><br><span class="line">else:</span><br><span class="line">    print(&quot;not match&quot;)</span><br><span class="line">`</span><br></pre></td></tr></table></figure></p><p>输出结果：</p><figure class="hljs highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">match it</span><br></pre></td></tr></table></figure><h3 id="抽取文本中的数字"><a href="#抽取文本中的数字" class="headerlink" title="抽取文本中的数字"></a>抽取文本中的数字</h3><ul><li><p>通过正则表达式匹配年份</p><p>[0-9]代表从0-9的所有数字，[a-z]代表从a到z的所有小写字母</p><figure class="hljs highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">import re</span><br><span class="line">strings = [&apos;War of 1812&apos;,&apos;There are 5280 feet to a mile&apos;,&apos;Happy New Year 2019!&apos;]</span><br><span class="line">#先声明year_strings为list变量，不然下面直接用会报错</span><br><span class="line">year_strings=[]</span><br><span class="line">for string in strings:</span><br><span class="line">    #匹配含有1000~2999数字的字符串</span><br><span class="line">    #[0-9]&#123;3&#125;表示重复[0-9]三次，相当于[0-9][0-9][0-9]</span><br><span class="line">    if re.search(&apos;[1-2][0-9]&#123;3&#125;&apos;,string):</span><br><span class="line">        year_strings.append(string)</span><br><span class="line">print(year_strings)</span><br></pre></td></tr></table></figure><p>输出结果：</p><figure class="hljs highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[&apos;War of 1812&apos;, &apos;Happy New Year 2019!&apos;]</span><br></pre></td></tr></table></figure></li><li><p>抽取所有的年份</p><p>使用re模块的另一个方法<code>findall()</code>来返回匹配带正则表达式的那部分字符串</p><p><code>re.findall(&quot;[a-z]&quot;,&#39;abc1234&#39;)</code>得到的结果是<code>[&quot;a&quot;,&quot;b&quot;,&quot;c&quot;]</code></p><figure class="hljs highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line">years_string = <span class="string">'2016 was a good year,but 2017 will be better!'</span></span><br><span class="line"><span class="comment">#抽取所有2000~2999的年份</span></span><br><span class="line">years = re.findall(<span class="string">'[2][0-9]&#123;3&#125;'</span>,years_string)</span><br><span class="line">print(years)</span><br></pre></td></tr></table></figure><p>输出结果:</p><figure class="hljs highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[<span class="string">'2016'</span>, <span class="string">'2017'</span>]</span><br></pre></td></tr></table></figure></li></ul><h2 id="Numpy使用详解"><a href="#Numpy使用详解" class="headerlink" title="Numpy使用详解"></a>Numpy使用详解</h2><p>Numpy(Numerical Python) 是高性能科学计算和数据分析的基础包，提供矩阵运算的功能。主要功能：</p><ul><li>ndarray  一个具有向量算术运算和复杂广播能力的多维数组对象</li><li>用于对数组数据进行快速运算的标准数学函数</li><li>用于读写磁盘数据的工具以及用于操作内存映射文件的工具</li><li>非常有用的线性代数，傅里叶变换和随机数操作</li><li>用于集成C/C++ 和 Fortran 代码的工具</li></ul><p>也可以用作通用数据的高效多维容器，可以定义任意的数据类型。</p><p><strong>广播</strong> ： 当有两个维度不同的数组(array)运算的时候，可以用低维的数组复制成高维数组参与运算(Numpy运算时需要结构相同)</p><h3 id="创建Numpy数组"><a href="#创建Numpy数组" class="headerlink" title="创建Numpy数组"></a>创建Numpy数组</h3><p>在Numpy中，最核心的数据结构是 <strong>ndarray</strong> ，代表的是多维数组。</p><p>借用线性代数的说法，一维数组通常称为向量(vector)，二维数组通常称为矩阵(matrix)</p><figure class="hljs highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="comment">#通过array可以将向量直接导入</span></span><br><span class="line">vector = np.array([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>])</span><br><span class="line"><span class="comment">#通过array也可以将矩阵导入</span></span><br><span class="line">matrix = np.array([[<span class="number">1</span>,<span class="string">'Tim'</span>],[<span class="number">2</span>,<span class="string">'Joey'</span>],[<span class="number">3</span>,<span class="string">'Johnny'</span>],[<span class="number">4</span>,<span class="string">'Frank'</span>]])</span><br><span class="line">print(vector)</span><br><span class="line">print(matrix)</span><br></pre></td></tr></table></figure><p>输出结果：</p><figure class="hljs highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[<span class="number">1</span> <span class="number">2</span> <span class="number">3</span> <span class="number">4</span>]</span><br><span class="line">[[<span class="string">'1'</span> <span class="string">'Tim'</span>]</span><br><span class="line"> [<span class="string">'2'</span> <span class="string">'Joey'</span>]</span><br><span class="line"> [<span class="string">'3'</span> <span class="string">'Johnny'</span>]</span><br><span class="line"> [<span class="string">'4'</span> <span class="string">'Frank'</span>]]</span><br></pre></td></tr></table></figure><h3 id="获取Numpy数组的维度"><a href="#获取Numpy数组的维度" class="headerlink" title="获取Numpy数组的维度"></a>获取Numpy数组的维度</h3><ul><li>通过<code>arange(n)</code> 方法生成0到n-1的数组</li><li>通过<code>reshape(row,column)</code>自动架构一个多行多列的array对象</li><li>通过<code>shape</code>属性获取Numpy数组的维度</li></ul><figure class="hljs highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">array = np.arange(<span class="number">12</span>)</span><br><span class="line">print(array)</span><br><span class="line"><span class="comment">#将array构造成三行4列的array对象</span></span><br><span class="line">matrix = array.reshape(<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line">print(matrix)</span><br><span class="line"><span class="comment">#shape返回一个元组tuple，第一个代表行，第二个代表列</span></span><br><span class="line">print(matrix.shape)</span><br></pre></td></tr></table></figure><p>输出结果：</p><figure class="hljs highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[ <span class="number">0</span>  <span class="number">1</span>  <span class="number">2</span>  <span class="number">3</span>  <span class="number">4</span>  <span class="number">5</span>  <span class="number">6</span>  <span class="number">7</span>  <span class="number">8</span>  <span class="number">9</span> <span class="number">10</span> <span class="number">11</span>]</span><br><span class="line">[[ <span class="number">0</span>  <span class="number">1</span>  <span class="number">2</span>  <span class="number">3</span>]</span><br><span class="line"> [ <span class="number">4</span>  <span class="number">5</span>  <span class="number">6</span>  <span class="number">7</span>]</span><br><span class="line"> [ <span class="number">8</span>  <span class="number">9</span> <span class="number">10</span> <span class="number">11</span>]]</span><br><span class="line">(<span class="number">3</span>, <span class="number">4</span>)</span><br></pre></td></tr></table></figure><h3 id="获取本地数据"><a href="#获取本地数据" class="headerlink" title="获取本地数据"></a>获取本地数据</h3><ul><li>通过<code>genfromtxt()</code>读取本地的数据集，<code>delimiter</code>参数定义数据由什么分割</li></ul><figure class="hljs highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="comment">#genfromtxt默认comments为#，即数据中由#标注的数据将被注释掉</span></span><br><span class="line"><span class="comment">#由于我的数据文件中使用#开头作为表头，设定注释用字符为##</span></span><br><span class="line">nfl = np.genfromtxt(<span class="string">"D:/Code/Python-NLP-Code/source/price.csv"</span>,delimiter=<span class="string">"\t"</span>,comments=<span class="string">"##"</span>)</span><br><span class="line">print(nfl)</span><br></pre></td></tr></table></figure><p>输出结果：</p><figure class="hljs highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">[[     nan      nan      nan      nan      nan      nan]</span><br><span class="line"> [<span class="number">1.00e+00</span> <span class="number">2.10e+03</span> <span class="number">3.00e+00</span> <span class="number">1.00e+00</span>      nan      nan]</span><br><span class="line"> [<span class="number">2.00e+00</span> <span class="number">3.20e+03</span> <span class="number">4.00e+00</span> <span class="number">2.00e+00</span>      nan      nan]</span><br><span class="line"> [<span class="number">3.00e+00</span> <span class="number">2.20e+03</span> <span class="number">2.00e+00</span> <span class="number">2.00e+00</span>      nan      nan]</span><br><span class="line"> [<span class="number">4.00e+00</span> <span class="number">1.90e+03</span> <span class="number">4.00e+00</span> <span class="number">1.00e+00</span>      nan      nan]</span><br><span class="line"> [<span class="number">5.00e+00</span> <span class="number">3.40e+03</span> <span class="number">5.00e+00</span> <span class="number">1.00e+00</span>      nan      nan]</span><br><span class="line"> [<span class="number">6.00e+00</span> <span class="number">6.70e+03</span> <span class="number">3.00e+00</span> <span class="number">1.00e+00</span>      nan      nan]</span><br><span class="line"> [<span class="number">7.00e+00</span> <span class="number">1.20e+03</span> <span class="number">2.00e+00</span> <span class="number">1.00e+00</span>      nan      nan]</span><br><span class="line"> [<span class="number">8.00e+00</span> <span class="number">3.50e+03</span> <span class="number">4.00e+00</span> <span class="number">1.00e+00</span>      nan      nan]</span><br><span class="line"> [<span class="number">9.00e+00</span> <span class="number">6.50e+03</span> <span class="number">1.00e+00</span> <span class="number">2.00e+00</span>      nan      nan]</span><br><span class="line"> [<span class="number">1.00e+01</span> <span class="number">9.80e+03</span> <span class="number">2.00e+00</span> <span class="number">1.00e+00</span>      nan      nan]</span><br><span class="line"> [<span class="number">1.10e+01</span> <span class="number">7.80e+03</span> <span class="number">3.00e+00</span> <span class="number">1.00e+00</span>      nan      nan]</span><br><span class="line"> [<span class="number">1.20e+01</span> <span class="number">6.70e+03</span> <span class="number">4.00e+00</span> <span class="number">3.00e+00</span>      nan      nan]</span><br><span class="line"> [<span class="number">1.30e+01</span> <span class="number">5.50e+03</span> <span class="number">6.00e+00</span> <span class="number">3.00e+00</span>      nan      nan]</span><br><span class="line"> [<span class="number">1.40e+01</span> <span class="number">6.60e+03</span> <span class="number">3.00e+00</span> <span class="number">1.00e+00</span>      nan      nan]</span><br><span class="line"> [<span class="number">1.50e+01</span> <span class="number">3.34e+03</span> <span class="number">2.00e+00</span> <span class="number">2.00e+00</span>      nan      nan]]</span><br></pre></td></tr></table></figure><p>自己手动编写的数据文件<code>price.csv</code>：</p><figure class="hljs highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">idprice#bedroom#bathroom#isBricklocal</span><br><span class="line">1210031tbos</span><br><span class="line">2320042rbos</span><br><span class="line">3220022tbos</span><br><span class="line">4190041tbos</span><br><span class="line">5340051tbos</span><br><span class="line">6670031tbos</span><br><span class="line">7120021rbos</span><br><span class="line">8350041rbos</span><br><span class="line">9650012rlink</span><br><span class="line">10980021tlink</span><br><span class="line">11780031rlink</span><br><span class="line">12670043rlink</span><br><span class="line">13550063tlink</span><br><span class="line">14660031tnew york</span><br><span class="line">15334022tnew york</span><br></pre></td></tr></table></figure><h3 id="正确读取数据"><a href="#正确读取数据" class="headerlink" title="正确读取数据"></a>正确读取数据</h3><ul><li><p><code>genfromtxt</code>函数默认其数据类型<code>dtype</code>为<code>float</code>，因此不是该种类型的数据会读出 nan(not a number) 数据类型转换出错 或 na(not available) 数值为空、不存在。</p></li><li><p>将<code>dtype</code>关键字设置为<code>&#39;U75&#39;</code>，即表示每个值都是75byte的unicode。</p></li><li><code>skip_header</code>关键字可以设置为整数，该参数可以跳过文件开头的对应的行数，然后再执行任何其他操作</li></ul><figure class="hljs highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="comment"># genfromtxt默认comments为#，即数据中由#标注的数据将被注释掉</span></span><br><span class="line"><span class="comment"># dtype设置为U75,即每个值都是75byte的Unicode。</span></span><br><span class="line"><span class="comment"># skip_header设置为整数，意思是跳过文件开头的X行。</span></span><br><span class="line">nfl = np.genfromtxt(<span class="string">"D:/Code/Python-NLP-Code/source/price.csv"</span>,delimiter=<span class="string">"\t"</span>,comments=<span class="string">"##"</span>,dtype=<span class="string">'U75'</span>,skip_header=<span class="number">1</span>)</span><br><span class="line">print(nfl)</span><br></pre></td></tr></table></figure><p>输出结果：</p><figure class="hljs highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">[[<span class="string">'id'</span> <span class="string">'price'</span> <span class="string">'#bedroom'</span> <span class="string">'#bathroom'</span> <span class="string">'#isBrick'</span> <span class="string">'local'</span>] <span class="comment">#添加skip_header=1后本行消失</span></span><br><span class="line"> [<span class="string">'1'</span> <span class="string">'2100'</span> <span class="string">'3'</span> <span class="string">'1'</span> <span class="string">'t'</span> <span class="string">'bos'</span>]</span><br><span class="line"> [<span class="string">'2'</span> <span class="string">'3200'</span> <span class="string">'4'</span> <span class="string">'2'</span> <span class="string">'r'</span> <span class="string">'bos'</span>]</span><br><span class="line"> [<span class="string">'3'</span> <span class="string">'2200'</span> <span class="string">'2'</span> <span class="string">'2'</span> <span class="string">'t'</span> <span class="string">'bos'</span>]</span><br><span class="line"> [<span class="string">'4'</span> <span class="string">'1900'</span> <span class="string">'4'</span> <span class="string">'1'</span> <span class="string">'t'</span> <span class="string">'bos'</span>]</span><br><span class="line"> [<span class="string">'5'</span> <span class="string">'3400'</span> <span class="string">'5'</span> <span class="string">'1'</span> <span class="string">'t'</span> <span class="string">'bos'</span>]</span><br><span class="line"> [<span class="string">'6'</span> <span class="string">'6700'</span> <span class="string">'3'</span> <span class="string">'1'</span> <span class="string">'t'</span> <span class="string">'bos'</span>]</span><br><span class="line"> [<span class="string">'7'</span> <span class="string">'1200'</span> <span class="string">'2'</span> <span class="string">'1'</span> <span class="string">'r'</span> <span class="string">'bos'</span>]</span><br><span class="line"> [<span class="string">'8'</span> <span class="string">'3500'</span> <span class="string">'4'</span> <span class="string">'1'</span> <span class="string">'r'</span> <span class="string">'bos'</span>]</span><br><span class="line"> [<span class="string">'9'</span> <span class="string">'6500'</span> <span class="string">'1'</span> <span class="string">'2'</span> <span class="string">'r'</span> <span class="string">'link'</span>]</span><br><span class="line"> [<span class="string">'10'</span> <span class="string">'9800'</span> <span class="string">'2'</span> <span class="string">'1'</span> <span class="string">'t'</span> <span class="string">'link'</span>]</span><br><span class="line"> [<span class="string">'11'</span> <span class="string">'7800'</span> <span class="string">'3'</span> <span class="string">'1'</span> <span class="string">'r'</span> <span class="string">'link'</span>]</span><br><span class="line"> [<span class="string">'12'</span> <span class="string">'6700'</span> <span class="string">'4'</span> <span class="string">'3'</span> <span class="string">'r'</span> <span class="string">'link'</span>]</span><br><span class="line"> [<span class="string">'13'</span> <span class="string">'5500'</span> <span class="string">'6'</span> <span class="string">'3'</span> <span class="string">'t'</span> <span class="string">'link'</span>]</span><br><span class="line"> [<span class="string">'14'</span> <span class="string">'6600'</span> <span class="string">'3'</span> <span class="string">'1'</span> <span class="string">'t'</span> <span class="string">'new york'</span>]</span><br><span class="line"> [<span class="string">'15'</span> <span class="string">'3340'</span> <span class="string">'2'</span> <span class="string">'2'</span> <span class="string">'t'</span> <span class="string">'new york'</span>]]</span><br></pre></td></tr></table></figure><h3 id="Numpy数组索引"><a href="#Numpy数组索引" class="headerlink" title="Numpy数组索引"></a>Numpy数组索引</h3><ul><li>Numpy支持list一样的定位操作</li><li>使用[x,y]来提取第x行，第y列的数据</li></ul><figure class="hljs highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">matrix = np.array([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">20</span>,<span class="number">30</span>,<span class="number">40</span>]])</span><br><span class="line">print(matrix[<span class="number">0</span>,<span class="number">1</span>])</span><br></pre></td></tr></table></figure><p>输出结果：</p><figure class="hljs highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">2</span></span><br></pre></td></tr></table></figure><h3 id="切片"><a href="#切片" class="headerlink" title="切片"></a>切片</h3><ul><li>numpy支持list一样的切片操作</li></ul><figure class="hljs highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">matrix = np.array([</span><br><span class="line">    [<span class="number">5</span>,<span class="number">10</span>,<span class="number">15</span>],</span><br><span class="line">    [<span class="number">20</span>,<span class="number">25</span>,<span class="number">30</span>],</span><br><span class="line">    [<span class="number">35</span>,<span class="number">40</span>,<span class="number">45</span>]</span><br><span class="line">])</span><br><span class="line"><span class="comment">#选择所有行且列的索引是1的数据</span></span><br><span class="line">print(matrix[:,<span class="number">1</span>])</span><br><span class="line">print()</span><br><span class="line"><span class="comment">#选择所有行且列的索引是0和1的数据</span></span><br><span class="line">print(matrix[:,<span class="number">0</span>:<span class="number">2</span>])</span><br><span class="line">print()</span><br><span class="line"><span class="comment">#选择行的索引是1和2且所有列的数据</span></span><br><span class="line">print(matrix[<span class="number">1</span>:<span class="number">3</span>,:])</span><br><span class="line">print()</span><br><span class="line"><span class="comment">#选择行的所以是1和2且列的索引是0和1的数据</span></span><br><span class="line">print(matrix[<span class="number">1</span>:<span class="number">3</span>,<span class="number">0</span>:<span class="number">2</span>])</span><br></pre></td></tr></table></figure><p>输出结果：</p><figure class="hljs highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[<span class="number">10</span> <span class="number">25</span> <span class="number">40</span>]</span><br><span class="line"></span><br><span class="line">[[ <span class="number">5</span> <span class="number">10</span>]</span><br><span class="line"> [<span class="number">20</span> <span class="number">25</span>]</span><br><span class="line"> [<span class="number">35</span> <span class="number">40</span>]]</span><br><span class="line"></span><br><span class="line">[[<span class="number">20</span> <span class="number">25</span> <span class="number">30</span>]</span><br><span class="line"> [<span class="number">35</span> <span class="number">40</span> <span class="number">45</span>]]</span><br><span class="line"></span><br><span class="line">[[<span class="number">20</span> <span class="number">25</span>]</span><br><span class="line"> [<span class="number">35</span> <span class="number">40</span>]]</span><br></pre></td></tr></table></figure><h3 id="数组比较"><a href="#数组比较" class="headerlink" title="数组比较"></a>数组比较</h3><ul><li>numpy可以进行数组或矩阵的比较，比较之后会产生<code>boolean</code>值</li><li>允许使用条件符来拼接条件，<code>&amp;</code>代表<strong>且</strong>，<code>|</code>代表<strong>或</strong></li></ul><figure class="hljs highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">matrix = np.array([</span><br><span class="line">    [<span class="number">5</span>,<span class="number">10</span>,<span class="number">15</span>],</span><br><span class="line">    [<span class="number">20</span>,<span class="number">25</span>,<span class="number">30</span>],</span><br><span class="line">    [<span class="number">35</span>,<span class="number">40</span>,<span class="number">45</span>]</span><br><span class="line">])</span><br><span class="line"><span class="comment">#判断matrix矩阵中的每个值是否等于25</span></span><br><span class="line">m = (matrix == <span class="number">25</span>)</span><br><span class="line">print(m)</span><br></pre></td></tr></table></figure><p>输出结果：</p><figure class="hljs highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[[<span class="literal">False</span> <span class="literal">False</span> <span class="literal">False</span>]</span><br><span class="line"> [<span class="literal">False</span>  <span class="literal">True</span> <span class="literal">False</span>]</span><br><span class="line"> [<span class="literal">False</span> <span class="literal">False</span> <span class="literal">False</span>]]</span><br></pre></td></tr></table></figure><p>较为复杂的一个例子：</p><figure class="hljs highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">matrix = np.array([</span><br><span class="line">    [<span class="number">5</span>,<span class="number">10</span>,<span class="number">15</span>],</span><br><span class="line">    [<span class="number">20</span>,<span class="number">25</span>,<span class="number">30</span>],</span><br><span class="line">    [<span class="number">35</span>,<span class="number">40</span>,<span class="number">45</span>]</span><br><span class="line">])</span><br><span class="line"><span class="comment">#matrix[:,1]选取所有行切索引为1的列的数据，然后判断是否等于25</span></span><br><span class="line">second_column_25 = (matrix[:,<span class="number">1</span>] == <span class="number">25</span>)</span><br><span class="line">print(second_column_25)</span><br><span class="line"><span class="comment">#展示返回true值的那一行数据</span></span><br><span class="line">print(matrix[second_column_25,:])</span><br><span class="line"><span class="comment">#更清晰的展示上面的作用，即选择True行的数据展示</span></span><br><span class="line">print(matrix[[<span class="literal">True</span>,<span class="literal">True</span>,<span class="literal">False</span>],:])</span><br></pre></td></tr></table></figure><p>输出结果：</p><figure class="hljs highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[<span class="literal">False</span>  <span class="literal">True</span> <span class="literal">False</span>]</span><br><span class="line"></span><br><span class="line">[[<span class="number">20</span> <span class="number">25</span> <span class="number">30</span>]]</span><br><span class="line"></span><br><span class="line">[[ <span class="number">5</span> <span class="number">10</span> <span class="number">15</span>]</span><br><span class="line"> [<span class="number">20</span> <span class="number">25</span> <span class="number">30</span>]]</span><br></pre></td></tr></table></figure><h3 id="替代值"><a href="#替代值" class="headerlink" title="替代值"></a>替代值</h3><ul><li>numpy可以运用布尔值来替换值</li></ul><figure class="hljs highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">vector = np.array([<span class="number">5</span>,<span class="number">10</span>,<span class="number">15</span>,<span class="number">20</span>])</span><br><span class="line"><span class="comment">#在数组中利用</span></span><br><span class="line"><span class="comment">#判断等于5或10，得到一个布尔值数组</span></span><br><span class="line">equal_to_ten_or_five = (vector == <span class="number">10</span>) | (vector == <span class="number">5</span>)</span><br><span class="line"><span class="comment">#利用布尔值数组将值替换为50</span></span><br><span class="line">vector[equal_to_ten_or_five] = <span class="number">50</span></span><br><span class="line">print(vector)</span><br><span class="line"><span class="comment">#在矩阵中利用</span></span><br><span class="line">matrix = np.array([</span><br><span class="line">    [<span class="number">5</span>,<span class="number">10</span>,<span class="number">15</span>],</span><br><span class="line">    [<span class="number">20</span>,<span class="number">25</span>,<span class="number">30</span>],</span><br><span class="line">    [<span class="number">35</span>,<span class="number">40</span>,<span class="number">45</span>]</span><br><span class="line">])</span><br><span class="line"><span class="comment">#将第二列中为25的值替换为10</span></span><br><span class="line">second_column_25 = matrix[:,<span class="number">1</span>] == <span class="number">25</span></span><br><span class="line">matrix[second_column_25,<span class="number">1</span>] = <span class="number">10</span></span><br><span class="line">print(matrix)</span><br></pre></td></tr></table></figure><p>输出结果：</p><figure class="hljs highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[<span class="number">50</span> <span class="number">50</span> <span class="number">15</span> <span class="number">20</span>]</span><br><span class="line"></span><br><span class="line">[[ <span class="number">5</span> <span class="number">10</span> <span class="number">15</span>]</span><br><span class="line"> [<span class="number">20</span> <span class="number">10</span> <span class="number">30</span>]</span><br><span class="line"> [<span class="number">35</span> <span class="number">40</span> <span class="number">45</span>]]</span><br></pre></td></tr></table></figure><ul><li>利用替换处理<strong>空值</strong></li></ul><figure class="hljs highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">matrix = np.array([</span><br><span class="line">    [<span class="string">''</span>,<span class="string">'10'</span>,<span class="string">'15'</span>],</span><br><span class="line">    [<span class="string">'20'</span>,<span class="string">'25'</span>,<span class="string">'30'</span>],</span><br><span class="line">    [<span class="string">'35'</span>,<span class="string">'40'</span>,<span class="string">''</span>]</span><br><span class="line">])</span><br><span class="line"><span class="comment">#将第3列中的空值替换为'0'</span></span><br><span class="line">second_column_25 = (matrix[:,<span class="number">2</span>] == <span class="string">''</span>)</span><br><span class="line">matrix[second_column_25,<span class="number">2</span>] = <span class="string">'0'</span></span><br><span class="line">print(matrix)</span><br></pre></td></tr></table></figure><p>输出结果：</p><figure class="hljs highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[[<span class="string">''</span> <span class="string">'10'</span> <span class="string">'15'</span>]</span><br><span class="line"> [<span class="string">'20'</span> <span class="string">'25'</span> <span class="string">'30'</span>]</span><br><span class="line"> [<span class="string">'35'</span> <span class="string">'40'</span> <span class="string">'0'</span>]]</span><br></pre></td></tr></table></figure><h3 id="数据类型转换"><a href="#数据类型转换" class="headerlink" title="数据类型转换"></a>数据类型转换</h3><ul><li>Numpy ndarray 数据类型可以通过参数 <code>dtype</code> 设定</li><li>可以使用 <code>astype</code> 转换类型，调用时会返回一个新的数组，也就是原始数据的一份复制</li></ul><figure class="hljs highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">vector = np.array([<span class="string">"1"</span>,<span class="string">"2"</span>,<span class="string">"3"</span>])</span><br><span class="line"><span class="comment">#从string转为float类</span></span><br><span class="line"><span class="comment">#如果含非数字类型，会报错</span></span><br><span class="line">vector = vector.astype(float)</span><br><span class="line">print(vector)</span><br></pre></td></tr></table></figure><p>输出结果：</p><figure class="hljs highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[<span class="number">1.</span> <span class="number">2.</span> <span class="number">3.</span>]</span><br></pre></td></tr></table></figure><h3 id="Numpy的统计计算方法"><a href="#Numpy的统计计算方法" class="headerlink" title="Numpy的统计计算方法"></a>Numpy的统计计算方法</h3><ul><li><code>sum()</code> 计算数组元素的和；</li><li><code>mean()</code> 计算数组元素的平均值</li><li><code>max()</code> 计算数组元素的最大值</li></ul><p>对矩阵计算结果为一个一维数组，需要指定行或列；注意数值类型必须是<code>int</code>或<code>float</code></p><figure class="hljs highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="comment">#数组例子</span></span><br><span class="line">vector = np.array([<span class="number">5</span>,<span class="number">10</span>,<span class="number">15</span>,<span class="number">20</span>])</span><br><span class="line">print(vector.sum())</span><br><span class="line"><span class="comment">#矩阵例子</span></span><br><span class="line">matrix = np.array([</span><br><span class="line">    [<span class="number">5</span>,<span class="number">10</span>,<span class="number">15</span>],</span><br><span class="line">    [<span class="number">20</span>,<span class="number">10</span>,<span class="number">30</span>],</span><br><span class="line">    [<span class="number">35</span>,<span class="number">40</span>,<span class="number">45</span>]</span><br><span class="line">])</span><br><span class="line"><span class="comment">#axis=1计算行的和</span></span><br><span class="line">print(matrix.sum(axis=<span class="number">1</span>))</span><br><span class="line"><span class="comment">#axis=0计算列的和</span></span><br><span class="line">print(matrix.sum(axis=<span class="number">0</span>))</span><br></pre></td></tr></table></figure><p>输出结果：</p><figure class="hljs highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">50</span></span><br><span class="line"></span><br><span class="line">[ <span class="number">30</span>  <span class="number">60</span> <span class="number">120</span>]</span><br><span class="line"></span><br><span class="line">[<span class="number">60</span> <span class="number">60</span> <span class="number">90</span>]</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Python自然语言处理实战 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
            <tag> Python </tag>
            
            <tag> Numpy </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Python-NLP-Chapter1</title>
      <link href="/2019/03/31/Python-NLP-Chapter1/"/>
      <url>/2019/03/31/Python-NLP-Chapter1/</url>
      
        <content type="html"><![CDATA[<h1 id="Chapter-1-NLP基础"><a href="#Chapter-1-NLP基础" class="headerlink" title="Chapter 1 NLP基础"></a>Chapter 1 NLP基础</h1><h2 id="List"><a href="#List" class="headerlink" title="List"></a>List</h2><ul><li>NLP基础概念</li><li>NLP的发展与应用</li><li>NLP常用术语以及扩展介绍</li></ul><h2 id="NLP的概念"><a href="#NLP的概念" class="headerlink" title="NLP的概念"></a>NLP的概念</h2><p>NLP（Natural Language Processing ，自然语言处理），研究用计算机来 <em>处理、理解以及运用人类语言</em> ，达到人与计算机之间进行有效通讯。</p><h2 id="NLP的基本分类"><a href="#NLP的基本分类" class="headerlink" title="NLP的基本分类"></a>NLP的基本分类</h2><ul><li>自然语言理解<ul><li>音系学</li><li>词态学</li><li>句法学</li><li>语义句法学</li><li>语用学</li></ul></li><li>自然语言生成<ul><li>自然语言文本</li></ul></li></ul><h2 id="NLP的研究任务"><a href="#NLP的研究任务" class="headerlink" title="NLP的研究任务"></a>NLP的研究任务</h2><ul><li><p>机器翻译</p><p>计算机具备将一种语言翻译成另一种语言的能力</p></li><li><p>情感分析</p><p>计算机能够判断用户评论是否积极</p></li><li><p>智能问答</p><p>计算机能够正确回答输入的问题</p></li><li><p>文摘生成</p><p>计算机能够准确归纳、总结并产生文本摘要</p></li><li><p>文本分类</p><p>计算机能够菜鸡各种文章，进行主题分析，进而进行自动分类</p></li><li><p>舆论分析</p><p>计算机能够判断目前舆论的导向</p></li><li><p>知识图谱</p><p>知识点相互连接而成的语义网络</p></li></ul><h2 id="NLP相关知识构成"><a href="#NLP相关知识构成" class="headerlink" title="NLP相关知识构成"></a>NLP相关知识构成</h2><h3 id="基本术语"><a href="#基本术语" class="headerlink" title="基本术语"></a>基本术语</h3><ol><li><p>分词 segment</p><p>词是<strong>最小的</strong>能够<strong>独立活动</strong>的<strong>有意义的</strong>语言成分，中文分词问题比较重要</p><p><strong>常用手段</strong>：基于字典的最长串匹配</p><p>问题：歧义分词</p></li><li><p>词性标注 part-of-speech tagging</p><p>词性一般指动词、名词、形容词等。</p><p>目的：表征词的一种<strong>隐藏状态</strong>，隐藏状态构成的转移就构成了<strong>状态转移序列</strong></p></li><li><p>命名实体识别 NER Named Entity Recognition</p><p>从文本中识别具有<strong>特定类别</strong>的实体，通常是名词</p></li><li><p>句法分析 syntax parsing</p><p>往往是一种<strong>基于规则</strong>的专家系统</p></li><li><p>指代消解 anaphora resolution</p><p>中文中代词出现的频率很高，指代消解即是辨明代词所代表的意思</p></li><li><p>情感识别 emotion recognition</p><p>本质上是<strong>分类问题</strong>，一般分为正面、负面，或加上中性。</p><p>方法：词袋模型+分类器、词向量模型+RNN等</p></li><li><p>纠错 correction</p><p>自动纠错多应用在搜索技术及输入法中</p><p>方法：基于N-Gram进行纠错、通过字典树、有限状态机</p></li><li><p>问答系统 QA system</p><p>较为复杂，需要语音识别、合成，自然语言理解、知识图谱等多项技术配合</p></li></ol><h3 id="知识结构"><a href="#知识结构" class="headerlink" title="知识结构"></a>知识结构</h3><ul><li><p>句法语义分析</p><p>针对目标句子，进行各种句法分析，如分词、词性标记、命名实体识别、链接，句法分析、语义角色识别、多义词消歧等</p></li><li><p>关键词抽取</p><p>抽取目标文本中的主要信息，如从一条新闻中抽取。</p><p>涉及实体识别、时间抽取、因果关系抽取等</p></li><li><p>文本挖掘</p><p>对文本的聚类、分类、信息抽取、摘要、情感分析以及对挖掘信息和知识的可视化、交互式的呈现界面</p></li><li><p>机器翻译</p><p>可分为文本翻译、语音翻译、手语翻译、图形翻译等</p></li><li><p>信息检索</p><p>对大规模的文档进行索引。</p></li><li><p>问答系统</p><p>对自然语言查询语句进行语义分析，包括实体链接、关系识别，形成逻辑表达式，然后到知识库中查找可能的候选答案并根据排序机制找出最佳答案</p></li><li><p>对话系统</p></li></ul><h2 id="语料库-中文"><a href="#语料库-中文" class="headerlink" title="语料库(中文)"></a>语料库(中文)</h2><ul><li><p><a href="https://dumps.wikimedia.org/zhwiki/" target="_blank" rel="noopener">中文维基百科</a></p></li><li><p><a href="http://download.labs./sougou.com/resource/ca.php" target="_blank" rel="noopener">搜狗新闻语料库</a></p><p>提供URL和正文信息</p></li><li><p><a href="https://www.kaggle.com/tmdb/tmdb-movie-metadata" target="_blank" rel="noopener">IMDB情感分析语料库</a></p><p>包括影片的众多信息、演员、片长、内容介绍、分级、评论等</p></li></ul><h2 id="探讨NLP的几个层面"><a href="#探讨NLP的几个层面" class="headerlink" title="探讨NLP的几个层面"></a>探讨NLP的几个层面</h2><ol><li><p>词法分析</p><p>分词、词性标注</p></li><li><p>句法分析</p></li><li><p>语义分析</p><p>语义角色标注(semantic role labeling)</p></li></ol>]]></content>
      
      
      <categories>
          
          <category> Python自然语言处理实战 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Python-NLP-Abstract</title>
      <link href="/2019/03/31/Python-NLP-Abstract/"/>
      <url>/2019/03/31/Python-NLP-Abstract/</url>
      
        <content type="html"><![CDATA[<h1 id="Abstract-内容简介"><a href="#Abstract-内容简介" class="headerlink" title="Abstract 内容简介"></a>Abstract 内容简介</h1><hr><h2 id="Part-1"><a href="#Part-1" class="headerlink" title="Part 1"></a>Part 1</h2><p>介绍NLP所需要了解的Python科学包、正则表达式、Solr检索</p><h3 id="Section-1"><a href="#Section-1" class="headerlink" title="Section 1"></a>Section 1</h3><h3 id="Section-2"><a href="#Section-2" class="headerlink" title="Section 2"></a>Section 2</h3><h3 id="Section-11"><a href="#Section-11" class="headerlink" title="Section 11"></a>Section 11</h3><hr><h2 id="Part2"><a href="#Part2" class="headerlink" title="Part2"></a>Part2</h2><p>介绍NLP相关的各个知识点</p><h3 id="Section3-5"><a href="#Section3-5" class="headerlink" title="Section3~5"></a>Section3~5</h3><p><em>词法分析</em> 层面的一些技术，是NLP技术的基础</p><h3 id="Section6"><a href="#Section6" class="headerlink" title="Section6"></a>Section6</h3><p><em>句法分析</em> 技术</p><h3 id="Section7"><a href="#Section7" class="headerlink" title="Section7"></a>Section7</h3><p>常用的 <em>向量化</em> 方法，常用于各种NLP任务的输入</p><h3 id="Section8"><a href="#Section8" class="headerlink" title="Section8"></a>Section8</h3><p><em>情感分析</em> 相关概念、场景以及一般做情感分析的流程</p><h3 id="Section9"><a href="#Section9" class="headerlink" title="Section9"></a>Section9</h3><p><em>机器学习</em> 的一些基本概念，重点突出NLP常用的 <em>分类算法</em> 、<em>聚类算法</em> </p><h3 id="Section10"><a href="#Section10" class="headerlink" title="Section10"></a>Section10</h3><p>介绍NLP中常用的 <em>深度学习算法</em>  </p>]]></content>
      
      
      <categories>
          
          <category> Python自然语言处理实战 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>AHU HPC培训</title>
      <link href="/2019/03/25/AHU%20HPC%E5%9F%B9%E8%AE%AD/"/>
      <url>/2019/03/25/AHU%20HPC%E5%9F%B9%E8%AE%AD/</url>
      
        <content type="html"><![CDATA[<h4 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h4><ul><li>域名<br>hpcc.ahu.edu.cn</li><li>配置情况</li></ul><div class="table-container"><table><thead><tr><th>CPU</th><th>26</th><th>*</th></tr></thead><tbody><tr><td>gpu1</td><td>1</td><td>*</td></tr><tr><td>gpu2</td><td>1</td><td>2卡</td></tr><tr><td>gpu8</td><td>3</td><td>8卡</td></tr></tbody></table></div><h4 id="Linux下操作命令"><a href="#Linux下操作命令" class="headerlink" title="Linux下操作命令"></a>Linux下操作命令</h4><ul><li><p>登录到结点</p><ul><li>ssh [gpu01]</li></ul></li><li><p>登陆后查看</p><ul><li>top </li><li>free </li></ul></li><li><p>查看历史命令</p><ul><li>.bash_history</li></ul></li><li><p>环境变量</p><ul><li>.bashrc</li><li>.bash_profile</li></ul></li><li><p>常用环境变量</p><ul><li><p>设置可执行程序的查找路径<br>  export PATH=/:$PATH</p><p>  e.g. export PATH=/Share/apps/matlab//R2016a/bin:$PATH<br>  # 直接在窗口输入时只在当前窗口有效<br>  # 可以写入到 ~/.bashrc文件中<br>  # 或写入到自定义.sh文件中，然后通过source加载</p></li></ul></li></ul><h4 id="slurm作业管理器"><a href="#slurm作业管理器" class="headerlink" title="slurm作业管理器"></a>slurm作业管理器</h4><ul><li>sinfo查看当前各节点资源<ul><li>timelimit 时间限制</li><li>State 状态<ul><li>mix 有人用但是没有满</li><li>alloc 被占满</li><li>idle 空闲</li></ul></li></ul></li><li>squeue查看任务队列<ul><li>-u 账号名</li></ul></li><li>srun交互式提交<ul><li>-p 节点名</li><li>-N 节点数</li><li>-n 核心数</li><li>-c 指定每个进程使用的cpu核数</li><li>-gres=gpu:1 指定使用GPU卡 ※必须</li><li>程序命令</li></ul></li><li>sbatch后台提交作业<ul><li>shell脚本<ul><li>固定格式<br>#!/bin/bash</li></ul></li><li>参数同srun</li></ul></li><li>默认输出<ul><li>slurm-作业号.out文件</li></ul></li><li>scancel取消作业<ul><li>参数：作业号</li><li>-u 用户名</li><li>-n 作业名NAME</li></ul></li><li>scontrol show job查看正在运行的作业信息<ul><li>参数：作业号</li></ul></li><li>sacct查看历史任务信息<ul><li>-u 用户名</li></ul></li><li>salloc申请结点资源</li></ul><h4 id="Q-amp-A"><a href="#Q-amp-A" class="headerlink" title="Q&amp;A"></a>Q&amp;A</h4><ul><li>安大集群软件<ul><li>/Share/apps</li></ul></li><li>常见问题<ul><li>节点、核数不匹配</li><li>缺库 .so.x </li></ul></li></ul><h4 id="提交AI作业"><a href="#提交AI作业" class="headerlink" title="提交AI作业"></a>提交AI作业</h4><ul><li>容器Singularity<ul><li>主程序路径<ul><li>/Share/apps/singularity/bin/singularity</li></ul></li><li>镜像文件<ul><li>/Share/imgs/ahu_ai.img</li></ul></li><li>GPU型号<ul><li>V100,显存16G，共28张卡</li></ul></li></ul></li><li>实例<ul><li>PyTorch脚本<ul><li><h1 id="SBATCH-—job-name-pytorch-作业名称"><a href="#SBATCH-—job-name-pytorch-作业名称" class="headerlink" title="SBATCH —job-name=pytorch    作业名称"></a>SBATCH —job-name=pytorch    作业名称</h1></li><li><h1 id="SBATCH-N-1-节点数"><a href="#SBATCH-N-1-节点数" class="headerlink" title="SBATCH -N 1    节点数"></a>SBATCH -N 1    节点数</h1></li><li><h1 id="SBATCH-—gres-gpu-1-GPU卡数-star"><a href="#SBATCH-—gres-gpu-1-GPU卡数-star" class="headerlink" title="SBATCH —gres=gpu:1      GPU卡数 :star:"></a>SBATCH —gres=gpu:1      GPU卡数 :star:</h1></li><li><h1 id="SBATCH-mem-20G-内存大小"><a href="#SBATCH-mem-20G-内存大小" class="headerlink" title="SBATCH -mem=20G     内存大小"></a>SBATCH -mem=20G     内存大小</h1></li><li><h1 id="SBATCH-p-GPU8-队列名（分区）"><a href="#SBATCH-p-GPU8-队列名（分区）" class="headerlink" title="SBATCH -p GPU8     队列名（分区）"></a>SBATCH -p GPU8     队列名（分区）</h1></li></ul></li><li>查看GPU使用情况<ul><li>nvidia-smi</li></ul></li><li>Tensorflow使用<ul><li>安装位置<ul><li>/usr/local/lib/python3.6/dist-packages/tensorlfow</li></ul></li><li>脚本<ul><li>类似PyTorch</li></ul></li></ul></li><li>安装软件<ul><li>./install</li><li>configure  -&gt;  make  -&gt;  make install</li><li>cmake  -&gt;  ccmake  -&gt;  make  —&gt;  make install </li><li>pip install </li><li>容器安装</li><li>没有root权限但是有安装常用软件的权限 :star:</li></ul></li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> 其他技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2019/03/25/hello-world/"/>
      <url>/2019/03/25/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="hljs highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="hljs highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="hljs highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="hljs highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
  
  
    
    
    <entry>
      <title>about</title>
      <link href="/about/index-1.html"/>
      <url>/about/index-1.html</url>
      
        <content type="html"><![CDATA[]]></content>
      
    </entry>
    
    
    
    <entry>
      <title>about</title>
      <link href="/about/index.html"/>
      <url>/about/index.html</url>
      
        <content type="html"><![CDATA[]]></content>
      
    </entry>
    
    
    
    <entry>
      <title>categories</title>
      <link href="/categories/index.html"/>
      <url>/categories/index.html</url>
      
        <content type="html"><![CDATA[]]></content>
      
    </entry>
    
    
    
    <entry>
      <title>tags</title>
      <link href="/tags/index.html"/>
      <url>/tags/index.html</url>
      
        <content type="html"><![CDATA[]]></content>
      
    </entry>
    
    
  
</search>
